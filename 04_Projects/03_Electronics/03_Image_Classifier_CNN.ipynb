{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "alien-murder",
   "metadata": {},
   "source": [
    "### Image Classifier using `CNN` in pytorch\n",
    "Right now we have `240` images of each class and we have `10` classes which are: \n",
    "```\n",
    "{\"camera\": 0, \"cellphone\": 1, \"charger\": 2, \"desktop\": 3, \"drone\": 4, \"headphone\": 5, \"laptop\": 6, \"mouse\": 7, \"remote\": 8, \"television\": 9}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lightweight-transmission",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "revolutionary-sewing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, cv2, json\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "arranged-raleigh",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Electronics:\n",
    "    CLASS_NAMES = \"class_names.json\"\n",
    "    ROOT = 'electronics'\n",
    "    \n",
    "    CAMERA = \"camera\"\n",
    "    CELLPHONE = 'cellphone'\n",
    "    CHARGER = 'charger'\n",
    "    DESKTOP = 'desktop'\n",
    "    DRONE = 'drone'\n",
    "    HEADPHONE = 'headphone'\n",
    "    LAPTOP = 'laptop'\n",
    "    MOUSE = 'mouse'\n",
    "    TELEVISION = 'television'\n",
    "    REMOTE = 'remote'\n",
    "    \n",
    "    GRAY = 'gray'\n",
    "    BGR = 'bgr'\n",
    "    RGB = 'rgb'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confirmed-essex",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "seasonal-lighter",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(Electronics.ROOT, Electronics.CLASS_NAMES), 'r') as f:\n",
    "    class_names = dict(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "whole-accounting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['camera', 'cellphone', 'charger', 'desktop', 'drone', 'headphone', 'laptop', 'mouse', 'remote', 'television'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "organized-journey",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "all_images = []\n",
    "all_labels = []\n",
    "for category in list(class_names.keys()): \n",
    "    all_images_path =  os.listdir(os.path.join(os.path.join(Electronics.ROOT, category), Electronics.BGR))\n",
    "    for image_path in all_images_path:\n",
    "        abs_image_path = os.path.join(os.path.join(os.path.join(Electronics.ROOT, \n",
    "                                                                category), Electronics.BGR), image_path)\n",
    "        image = cv2.imread(abs_image_path, cv2.IMREAD_UNCHANGED)\n",
    "        all_images.append(image)\n",
    "        all_labels.append(class_names[category])\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "judicial-export",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images_array = np.array(all_images)\n",
    "all_labels_array = np.array(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "second-rebate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2400,), (2400, 100, 100, 3))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_labels_array.shape, all_images_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "framed-memorial",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_labels = np.array([np.eye(10)[i] for i in all_labels_array])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corresponding-highway",
   "metadata": {},
   "source": [
    "> Creating an `Electronic` Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "mounted-mobility",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Electronic(Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        self.transform = transform\n",
    "        self.x = all_images_array\n",
    "        self.y = all_labels_array\n",
    "        \n",
    "        self.len = one_hot_labels.shape[0]\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        samples = self.x[index], self.y[index]\n",
    "        if self.transform:\n",
    "            samples = self.transform(samples)\n",
    "        return samples\n",
    "    \n",
    "electronic = Electronic()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-check",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constitutional-qatar",
   "metadata": {},
   "source": [
    "> `ToTensor` Transformer - This converts features and labels to pytorch `tensors`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "honey-florence",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor:\n",
    "    def __call__(self, sample):\n",
    "        x, y = sample\n",
    "        return torch.Tensor(x.astype('float32')), torch.from_numpy(np.array(y.astype('float32'))).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indian-local",
   "metadata": {},
   "source": [
    "> `Normalize` Transformer - This normalize features for each pixel to be between `0` and `1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "mature-johns",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize:\n",
    "    def __call__(self, sample):\n",
    "        x, y = sample\n",
    "        return x/255, np.array(y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "rural-dayton",
   "metadata": {},
   "outputs": [],
   "source": [
    "electronic = Electronic(transform = transforms.Compose([\n",
    "    Normalize(),\n",
    "    ToTensor()\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "social-headquarters",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.5373, 0.5373, 0.5373],\n",
       "          [0.5333, 0.5333, 0.5333],\n",
       "          [0.5765, 0.5765, 0.5765]],\n",
       " \n",
       "         [[0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.7647, 0.7647, 0.7647],\n",
       "          [0.7059, 0.7059, 0.7059],\n",
       "          [0.6431, 0.6431, 0.6431]],\n",
       " \n",
       "         [[0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.9176, 0.9176, 0.9176],\n",
       "          [0.7451, 0.7451, 0.7451],\n",
       "          [0.5882, 0.5882, 0.5882]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.2667, 0.2667, 0.2667],\n",
       "          [0.2588, 0.2588, 0.2588],\n",
       "          [0.2588, 0.2588, 0.2588],\n",
       "          ...,\n",
       "          [0.2392, 0.2392, 0.2392],\n",
       "          [0.2157, 0.2157, 0.2157],\n",
       "          [0.2275, 0.2275, 0.2275]],\n",
       " \n",
       "         [[0.3137, 0.3137, 0.3137],\n",
       "          [0.3647, 0.3647, 0.3647],\n",
       "          [0.3098, 0.3098, 0.3098],\n",
       "          ...,\n",
       "          [0.2314, 0.2314, 0.2314],\n",
       "          [0.2314, 0.2314, 0.2314],\n",
       "          [0.2078, 0.2078, 0.2078]],\n",
       " \n",
       "         [[0.3765, 0.3765, 0.3765],\n",
       "          [0.4275, 0.4275, 0.4275],\n",
       "          [0.3843, 0.3843, 0.3843],\n",
       "          ...,\n",
       "          [0.2196, 0.2196, 0.2196],\n",
       "          [0.2549, 0.2549, 0.2549],\n",
       "          [0.2784, 0.2784, 0.2784]]]),\n",
       " tensor(0))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "electronic[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stainless-greece",
   "metadata": {},
   "source": [
    "> Spliting data into `train` and `testing`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "electric-admission",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "western-western",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(electronic, random_state=42, test_size=.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident-development",
   "metadata": {},
   "source": [
    "> Data ``Loaders``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "chinese-ghana",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = DataLoader(train, \n",
    "                       batch_size=32,\n",
    "                       shuffle=True\n",
    "                      )\n",
    "test_set = DataLoader(train,\n",
    "                      batch_size=32,\n",
    "                      shuffle=False\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extended-lover",
   "metadata": {},
   "source": [
    "### Creating a `Image Classifier` model `CNN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "standard-needle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 256, kernel_size=(2, 2), stride=(1, 1))\n",
       "  (conv3): Conv2d(256, 64, kernel_size=(2, 2), stride=(1, 1))\n",
       "  (conv4): Conv2d(64, 16, kernel_size=(2, 2), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=400, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=128, bias=True)\n",
       "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels= 32, kernel_size=(3, 3))\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels= 256, kernel_size=(2, 2))\n",
    "        self.conv3 = nn.Conv2d(in_channels=256, out_channels= 64, kernel_size=(2, 2))\n",
    "        self.conv4 = nn.Conv2d(in_channels=64, out_channels= 16, kernel_size=(2, 2))\n",
    "        \n",
    "        self._to_linear = None\n",
    "        self.x = torch.randn(3, 100, 100).view(-1, 3, 100, 100)\n",
    "        self.conv(self.x)\n",
    "        \n",
    "        self.fc1 = nn.Linear(self._to_linear, 64)\n",
    "        self.fc2 = nn.Linear(64, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "                               \n",
    "    def conv(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv4(x)), (2, 2))\n",
    "        \n",
    "        if self._to_linear is None:\n",
    "            self._to_linear = x.shape[1] * x.shape[2] * x.shape[3]\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(-1, self._to_linear)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "    \n",
    "net = Net()\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ordered-clothing",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cross-grill",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_true):\n",
    "    print(y_pred, y_true)\n",
    "    with torch.no_grad():\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for i in range(len(y_true)):\n",
    "            if y_pred[i] == y_true[i]:\n",
    "                correct +=1\n",
    "            total +=1\n",
    "        return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "curious-isaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]) tensor([1, 4, 0, 0, 9, 9, 0, 3, 7, 3, 4, 7, 4, 0, 1, 2])\n",
      "Epoch 1/20\n",
      "loss: 2.309, accuracy: 0.000\n",
      "tensor([9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]) tensor([9, 2, 8, 6, 3, 6, 4, 6, 3, 4, 1, 9, 6, 3, 3, 9])\n",
      "Epoch 2/20\n",
      "loss: 2.282, accuracy: 0.188\n",
      "tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]) tensor([2, 2, 9, 2, 0, 4, 0, 5, 0, 0, 0, 8, 5, 6, 0, 4])\n",
      "Epoch 3/20\n",
      "loss: 2.315, accuracy: 0.062\n",
      "tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]) tensor([5, 2, 4, 7, 3, 0, 6, 1, 9, 9, 0, 1, 5, 6, 3, 1])\n",
      "Epoch 4/20\n",
      "loss: 2.311, accuracy: 0.125\n",
      "tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]) tensor([8, 4, 6, 2, 9, 5, 4, 8, 0, 6, 5, 0, 3, 0, 6, 0])\n",
      "Epoch 5/20\n",
      "loss: 2.292, accuracy: 0.188\n",
      "tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]) tensor([0, 2, 9, 4, 8, 1, 1, 4, 0, 4, 8, 3, 7, 9, 8, 8])\n",
      "Epoch 6/20\n",
      "loss: 2.313, accuracy: 0.000\n",
      "tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]) tensor([6, 7, 8, 6, 0, 1, 6, 8, 2, 0, 6, 2, 6, 5, 1, 8])\n",
      "Epoch 7/20\n",
      "loss: 2.304, accuracy: 0.312\n",
      "tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]) tensor([5, 8, 5, 6, 1, 1, 9, 4, 7, 1, 1, 9, 8, 7, 4, 0])\n",
      "Epoch 8/20\n",
      "loss: 2.317, accuracy: 0.062\n",
      "tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]) tensor([3, 5, 2, 0, 8, 4, 3, 7, 5, 3, 7, 4, 9, 2, 8, 3])\n",
      "Epoch 9/20\n",
      "loss: 2.297, accuracy: 0.000\n",
      "tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]) tensor([1, 6, 2, 9, 2, 5, 5, 1, 5, 2, 6, 4, 8, 0, 7, 1])\n",
      "Epoch 10/20\n",
      "loss: 2.317, accuracy: 0.125\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]) tensor([3, 2, 3, 1, 1, 8, 8, 1, 6, 5, 6, 3, 5, 8, 0, 3])\n",
      "Epoch 11/20\n",
      "loss: 2.316, accuracy: 0.000\n",
      "tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]) tensor([6, 1, 4, 6, 1, 0, 3, 6, 0, 7, 6, 7, 9, 6, 3, 8])\n",
      "Epoch 12/20\n",
      "loss: 2.285, accuracy: 0.312\n",
      "tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]) tensor([7, 8, 3, 1, 1, 4, 0, 5, 3, 4, 5, 1, 0, 5, 0, 5])\n",
      "Epoch 13/20\n",
      "loss: 2.330, accuracy: 0.000\n",
      "tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]) tensor([9, 3, 8, 0, 7, 3, 8, 7, 4, 8, 8, 9, 3, 3, 0, 5])\n",
      "Epoch 14/20\n",
      "loss: 2.302, accuracy: 0.000\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]) tensor([2, 2, 4, 6, 5, 1, 3, 8, 4, 2, 3, 2, 6, 2, 8, 6])\n",
      "Epoch 15/20\n",
      "loss: 2.310, accuracy: 0.125\n",
      "tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]) tensor([3, 0, 6, 6, 6, 5, 8, 4, 6, 6, 0, 0, 0, 8, 8, 7])\n",
      "Epoch 16/20\n",
      "loss: 2.292, accuracy: 0.312\n",
      "tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]) tensor([9, 0, 6, 1, 2, 0, 6, 8, 3, 9, 5, 5, 0, 9, 1, 1])\n",
      "Epoch 17/20\n",
      "loss: 2.310, accuracy: 0.125\n",
      "tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]) tensor([9, 0, 4, 2, 4, 1, 6, 2, 5, 6, 6, 5, 4, 9, 1, 4])\n",
      "Epoch 18/20\n",
      "loss: 2.297, accuracy: 0.188\n",
      "tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]) tensor([6, 5, 6, 9, 6, 7, 9, 3, 0, 9, 6, 7, 0, 1, 0, 5])\n",
      "Epoch 19/20\n",
      "loss: 2.288, accuracy: 0.250\n",
      "tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]) tensor([6, 1, 8, 7, 6, 3, 2, 2, 9, 7, 5, 7, 4, 8, 4, 3])\n",
      "Epoch 20/20\n",
      "loss: 2.301, accuracy: 0.125\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    y_true = None\n",
    "    y_pred = None\n",
    "    for X, y in train_set:\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass\n",
    "        outputs = net(X.view(-1, 3, 100, 100))\n",
    "#         print(outputs, y)\n",
    "        y_true = y\n",
    "        y_pred = torch.argmax(outputs.detach(), dim=1)\n",
    "        # loss \n",
    "        loss = criterion(outputs, y)\n",
    "        #backward pass\n",
    "        loss.backward()\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "    accu = accuracy(y_pred, y_true)\n",
    "    print(f'Epoch {epoch+1}/{EPOCHS}')\n",
    "    print(f\"loss: {loss.item():.3f}, accuracy: {accu:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ambient-burden",
   "metadata": {},
   "source": [
    "> **Conclusion** - The ``model`` is guessing, it's not learning anything from **features**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demanding-philippines",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
