{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQzYSXs0KHTL"
   },
   "source": [
    "### 2. Updated Sentiment Analysis using `IMDB` dataset.\n",
    "* In this notebook we are going to update the code from the previous example so that we can improve accuracy on our test dataset.\n",
    "\n",
    "We will walk through the following:\n",
    "* packed padded sequences\n",
    "* pre-trained word embeddings\n",
    "* different RNN architecture\n",
    "* bidirectional RNN\n",
    "* multi-layer RNN\n",
    "* regularization\n",
    "* a different optimizer typically `Adam`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "559vErX_KxbE"
   },
   "source": [
    "#### Preparing Data\n",
    "We'll be using ``packed padded sequences``, which will make our ``RNN`` only process the ``non-padded`` elements of our sequence, and for any padded element the output will be a ``zero`` tensor. To use packed padded sequences, we have to tell the RNN how long the actual sequences are. We do this by setting **``include_lengths = True``** for our ``TEXT`` field. This will cause ``batch.text`` to now be a tuple with the first element being our sentence (a numericalized tensor that has been padded) and the second element being the actual lengths of our sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JHdBFcEwKBTq"
   },
   "outputs": [],
   "source": [
    "import en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x5yuiuNoL_RT"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.legacy import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OL8qqi_PMnBb",
    "outputId": "92c89540-eb1b-4f51-cad0-6c102c59d83b"
   },
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n2QnkSgCMyMF",
    "outputId": "89f9989e-8134-4b4b-a653-80586b8c12d3"
   },
   "outputs": [],
   "source": [
    "TEXT = data.Field(tokenize = 'spacy',\n",
    "                  tokenizer_language = 'en_core_web_sm',\n",
    "              include_lengths=True)\n",
    "LABEL = data.LabelField(dtype = torch.float)\n",
    "TEXT, LABEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1YDry4njNspn"
   },
   "source": [
    "### Downloading the `IMDB` dataset.\n",
    "Another handy feature of ``TorchText`` is that it has support for common datasets used in natural language processing (NLP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "liLBrMzHNh4Y"
   },
   "outputs": [],
   "source": [
    "from torchtext.legacy import datasets\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BpPxuiwOOTwO"
   },
   "source": [
    "### Checking the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FKCdV1gnOJ3d",
    "outputId": "1cfa2ecf-4fc3-4adb-a135-9355a6eb9b99"
   },
   "outputs": [],
   "source": [
    "print(f\"TRAINING EXAMPLES: \\t {len(train_data)}\\nTEST EXAMPLES: \\t {len(test_data)}\\nTOTAL EXAMPLES: \\t {len(train_data) + len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bOAdJHLyPGqS"
   },
   "source": [
    "## Checking one example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uXBkmgNzOTG-",
    "outputId": "de2b12c7-ee7c-4e98-95d5-489329f4bf83"
   },
   "outputs": [],
   "source": [
    "vars(train_data.examples[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZeYM0qCPaWH"
   },
   "source": [
    "### Creating the validation data.\n",
    "By default the `IMDB` only have two sets which are the trainning and testing set, we also need the validation set in our sample. We are going to use the `.split()` method on the train data.\n",
    "\n",
    "1. `.split()` method.\n",
    "This method split the dataset into a ration of ``70% `` trainning and ``30%`` validation.\n",
    "* We can change this by specifying the keyword arg `split_ratio = 0.8` which means ``80%`` of the data will belong to the training and the rest to the testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "neUomSwCQnZ-"
   },
   "outputs": [],
   "source": [
    "from random import seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7usnXOytOTAu"
   },
   "outputs": [],
   "source": [
    "train_data, val_data = train_data.split(random_state=seed(SEED))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fLnQc9mdQrvD"
   },
   "source": [
    "Let's check how many example do we have now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AYYARSO-OS9m",
    "outputId": "64512945-2d7b-4a21-d4a6-f5f935542647"
   },
   "outputs": [],
   "source": [
    "print(f\"TRAINING EXAMPLES: \\t {len(train_data)}\\nVALIDATION EXAMPLES: \\t {len(val_data)}\\nTEST EXAMPLES: \\t {len(test_data)}\\nTOTAL EXAMPLES: \\t {len(train_data) + len(test_data) + len(val_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eq2bPWdLRE58"
   },
   "source": [
    "### $P$re-trained $W$ord $E$mbeddings.\n",
    "\n",
    "Now, instead of having our word embeddings initialized randomly, they are initialized with these pre-trained vectors. We get these vectors simply by specifying which vectors we want and passing it as an argument to ``build_vocab``. ``TorchText`` handles downloading the vectors and associating them with the correct words in our vocabulary.\n",
    "\n",
    "Here, we'll be using the ``\"glove.6B.100d\"`` vectors. Glove is the algorithm used to calculate the vectors, go here for more. ``6B`` indicates these vectors were trained on 6 billion tokens and ``100d`` indicates these vectors are 100-dimensional.\n",
    "\n",
    "The theory is that these pre-trained vectors already have words with similar semantic meaning close together in vector space, e.g. \"terrible\", \"awful\", \"dreadful\" are nearby. This gives our embedding layer a good initialization as it does not have to learn these relations from scratch.\n",
    "\n",
    "By default, ``TorchText`` will initialize words in your vocabulary but not in your pre-trained embeddings to zero. We don't want this, and instead initialize them randomly by setting ``unk_init to torch.Tensor.normal_``. This will now initialize those words via a Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "id": "gmOfq9dlREsZ",
    "outputId": "2b5b39fb-7da3-4707-da1c-7facfc2a65ba"
   },
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 25_000\n",
    "\n",
    "TEXT.build_vocab(train_data, \n",
    "                 max_size = MAX_VOCAB_SIZE,\n",
    "                 vectors = \"glove.6B.100d\",\n",
    "                 unk_init = torch.Tensor.normal_\n",
    "                 )\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iy2qkCZ1REin"
   },
   "source": [
    "### Creating Iterators - `BucketIterator`\n",
    "\n",
    "As before, we create the iterators, placing the tensors on the GPU if one is available.\n",
    "\n",
    "Another thing for packed padded sequences all of the tensors within a batch need to be sorted by their lengths. This is handled in the iterator by setting ``sort_within_batch = True``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XoR3rCyBREfv"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_iterator, test_iterator, validation_iterator = data.BucketIterator.splits(\n",
    "    (train_data, test_data, val_data),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    device=device,\n",
    "    sort_within_batch = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A2PNbHT0ympK"
   },
   "source": [
    "### Different `RNN` achitectures.\n",
    "We'll be using a different RNN architecture called a Long Short-Term Memory ``(LSTM)``. **Why is an LSTM better than a standard RNN?** Standard RNNs suffer from the vanishing gradient problem. LSTMs overcome this by having an extra recurrent state called a ``cell``, $c_0$ - which can be thought of as the \"memory\" of the LSTM - and the use use multiple gates which control the flow of information into and out of the memory.  We can simply think of the LSTM as a function of $x_t$, $h_t$ and $c_t$, instead of just $x_t$ and $h_t$.\n",
    "\n",
    "The `LSTM`\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://render.githubusercontent.com/render/math?math=%28h_t%2C%20c_t%29%20%3D%20%5Ctext%7BLSTM%7D%28x_t%2C%20h_t%2C%20c_t%29&mode=display\"/>\n",
    "</p>\n",
    "\n",
    "The `LSTM` looks like (without the embedding layer)\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/bentrevett/pytorch-sentiment-analysis/raw/2b666b3cba7d629a2f192c7d9c66fadcc9f0c363/assets/sentiment2.png\"/>\n",
    "</p>\n",
    "\n",
    "### Bidirectional RNN\n",
    "The concept behind a bidirectional RNN is simple. As well as having an RNN processing the words in the sentence from the first to the last (a forward RNN), we have a second RNN processing the words in the sentence from the last to the first (a backward RNN). At time step $t$, the forward RNN is processing word $x_t$, and the backward RNN is processing word $x_{T-t+1}$.\n",
    "\n",
    "In PyTorch, the hidden state (and cell state) tensors returned by the forward and backward RNNs are stacked on top of each other in a single tensor.\n",
    "\n",
    "We make our sentiment prediction using a concatenation of the last hidden state from the forward RNN (obtained from final word of the sentence), $h_T^\\rightarrow$, and the last hidden state from the backward RNN (obtained from the first word of the sentence), $h_T^\\leftarrow$, i.e. $\\hat{y}=f(h_T^\\rightarrow, h_T^\\leftarrow)$\n",
    "\n",
    "The image below shows a bi-directional RNN, with the forward RNN in orange, the backward RNN in green and the linear layer in silver.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/bentrevett/pytorch-sentiment-analysis/raw/2b666b3cba7d629a2f192c7d9c66fadcc9f0c363/assets/sentiment3.png\"/>\n",
    "</p>\n",
    "\n",
    "### Regularization\n",
    "Although we've added improvements to our model, each one adds additional parameters. Without going into overfitting into too much detail, the more parameters you have in in your model, the higher the probability that your model will overfit **(memorize the training data, causing a low training error but high validation/testing error, i.e. poor generalization to new, unseen examples).** To combat this, we use regularization. More specifically, we use a method of regularization called dropout. **Dropou**t works by randomly dropping out (setting to 0) neurons in a layer during a forward pass. The probability that each neuron is dropped out is set by a hyperparameter and each neuron with dropout applied is considered indepenently. One theory about why dropout works is that a model with parameters dropped out can be seen as a \"weaker\" (less parameters) model. The predictions from all these \"weaker\" models (one for each forward pass) get averaged together withinin the parameters of the model. Thus, your one model can be thought of as an ensemble of weaker models, none of which are over-parameterized and thus should not overfit.\n",
    "\n",
    "\n",
    "### Implementation Details\n",
    "Another addition to this model is that we are not going to learn the embedding for the ``<pad>`` token. This is because we want to explitictly tell our model that padding tokens are irrelevant to determining the sentiment of a sentence. This means the embedding for the pad token will remain at what it is initialized to (we initialize it to all zeros later). We do this by passing the index of our pad token as the ``padding_idx`` argument to the ``nn.Embedding`` layer.\n",
    "\n",
    "To use an ``LSTM`` instead of the standard RNN, we use ``nn.LSTM`` instead of ``nn.RNN``. Also, note that the ``LSTM `` returns the output and a tuple of the final hidden state and the final cell state, whereas the standard ``RNN`` only returned the output and final hidden state.\n",
    "\n",
    "As the final hidden state of our ``LSTM`` has both a forward and a backward component, which will be concatenated together, the size of the input to the ``nn.Linear`` layer is twice that of the hidden dimension size.\n",
    "\n",
    "Implementing bidirectionality and adding additional layers are done by passing values for the ``num_layers`` and bidirectional arguments for the RNN/LSTM.\n",
    "\n",
    "``Dropout`` is implemented by initializing an ``nn.Dropout`` layer (the argument is the probability of dropping out each neuron) and using it within the forward method after each layer we want to apply dropout to. \n",
    "\n",
    "**Note:** Never use dropout on the input or output layers (text or fc in this case), you only ever want to use dropout on intermediate layers. The ``LSTM`` has a dropout argument which adds dropout on the connections between hidden states in one layer to hidden states in the next layer.\n",
    "\n",
    "As we are passing the lengths of our sentences to be able to use packed padded sequences, we have to add a second argument, text_lengths, to forward.\n",
    "\n",
    "Before we pass our embeddings to the RNN, we need to pack them, which we do with ``nn.utils.rnn.packed_padded_sequence``. This will cause our ``RNN`` to only process the non-padded elements of our sequence. The ``RNN`` will then return packed_output (a packed sequence) as well as the hidden and cell states (both of which are tensors). Without packed padded sequences, hidden and cell are tensors from the last element in the sequence, which will most probably be a pad token, however when using packed padded sequences they are both from the last non-padded element in the sequence. Note that the lengths argument of ``packed_padded_sequence`` must be a CPU tensor so we explicitly make it one by using ``.to('cpu')``.\n",
    "\n",
    "We then unpack the output sequence, with ``nn.utils.rnn.pad_packed_sequence``, to transform it from a packed sequence to a tensor. The elements of output from padding tokens will be zero tensors (tensors where every element is zero). Usually, we only have to unpack output if we are going to use it later on in the model. Although we aren't in this case, we still unpack the sequence just to show how it is done.\n",
    "\n",
    "The final hidden state, hidden, has a shape of ``[num layers * num directions, batch size, hid dim]``. These are ordered: ``[forward_layer_0, backward_layer_0, forward_layer_1, backward_layer 1, ..., forward_layer_n, backward_layer n]``. As we want the final (top) layer forward and backward hidden states, we get the top two hidden layers from the first ``dimension``, ``hidden[-2,:,:]`` and ``hidden[-1,:,:]``, and concatenate them together before passing them to the linear layer (after applying dropout)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8JocSNu6REZ4",
    "outputId": "04d8f60a-c9d6-459f-f7c9-01a4cebae59f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vV6P86dfY_6N"
   },
   "source": [
    "### Creating a model.\n",
    "The next stage is building the model that we'll eventually train and evaluate.\n",
    "\n",
    "1. **The embedding layer.**\n",
    "\n",
    "The embedding layer is used to transform our sparse ``one-hot vector`` (sparse as most of the elements are 0) into a dense embedding vector (dense as the dimensionality is a lot smaller and all the elements are real numbers). This embedding layer is simply a single fully connected layer. As well as reducing the dimensionality of the input to the ``RNN``, there is the theory that words which have similar impact on the sentiment of the review are mapped close together in this dense vector space.\n",
    "\n",
    "The RNN layer is our RNN which takes in our dense vector and the previous hidden state $h_{t-1}$, which it uses to calculate the next hidden state, $h_t$.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/bentrevett/pytorch-sentiment-analysis/raw/2b666b3cba7d629a2f192c7d9c66fadcc9f0c363/assets/sentiment7.png\"/>\n",
    "</p\n",
    "\n",
    "Finally, the linear layer takes the final hidden state and feeds it through a fully connected layer, $f(h_T)$, transforming it to the correct output dimension.\n",
    "\n",
    "The forward method is called when we feed examples into our model.\n",
    "\n",
    "Each batch, text, is a tensor of size **[sentence length, batch size]**. That is a batch of sentences, each having each word converted into a one-hot vector.\n",
    "\n",
    "You may notice that this tensor should have another dimension due to the one-hot vectors, however PyTorch conveniently stores a one-hot vector as it's index value, i.e. the tensor representing a sentence is just a tensor of the indexes for each token in that sentence. The act of converting a list of tokens into a list of indexes is commonly called **numericalizing**.\n",
    "\n",
    "The input batch is then passed through the embedding layer to get embedded, which gives us a dense vector representation of our sentences. embedded is a tensor of size **[sentence length, batch size, embedding dim]**.\n",
    "\n",
    "embedded is then fed into the **RNN**. In some frameworks you must feed the initial hidden state, $h_0$, into the RNN, however in PyTorch, if no initial hidden state is passed as an argument it defaults to a tensor of all zeros.\n",
    "\n",
    "The **RNN** returns 2 tensors, output of size **[sentence length, batch size, hidden dim]** and hidden of size **[1, batch size, hidden dim]**. output is the concatenation of the hidden state from every time step, whereas hidden is simply the final hidden state. We verify this using the assert statement. Note the squeeze method, which is used to remove a dimension of size 1.\n",
    "\n",
    "Finally, we feed the last hidden state, hidden, through the linear layer, fc, to produce a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JdFAeHfcbDGd"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7GVEvwmzREU-"
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "  def __init__(self,input_size, hidden_size, embedding_size, num_layers, output_size):\n",
    "    super().__init__()\n",
    "    self.emb = nn.Embedding(input_size, embedding_dim=embedding_size)\n",
    "    self.rnn = nn.RNN(embedding_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "    self.fc = nn.Linear(hidden_size, out_features=output_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # x = [sent len, batch size]\n",
    "    embedded = self.emb(x)\n",
    "    #embedded = [sent len, batch size, emb dim]\n",
    "\n",
    "    output, hidden = self.rnn(embedded)\n",
    "    #output = [sent len, batch size, hid dim]\n",
    "    #hidden = [1, batch size, hid dim] \n",
    "    assert torch.equal(output[-1,:,:], hidden.squeeze(0))\n",
    "\n",
    "    return self.fc(output[-1,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0HjSPujodlm7"
   },
   "source": [
    "We now create an instance of our RNN class.\n",
    "\n",
    "The input size is the dimension of the one-hot vectors, which is equal to the vocabulary size.\n",
    "\n",
    "The embedding size is the size of the dense word vectors. This is usually around ``50-250`` dimensions, but depends on the size of the vocabulary.\n",
    "\n",
    "The ``hidden size`` is the size of the hidden states. This is usually around ``100-500`` dimensions, but also depends on factors such as on the vocabulary size, the size of the dense vectors and the complexity of the task.\n",
    "\n",
    "The ``output size`` is usually the number of classes, however in the case of only 2 classes the output value is between 0 and 1 and thus can be 1-dimensional, i.e. a single scalar real number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "obkgKjjURESR",
    "outputId": "35fa62a2-d224-44d2-d89b-6382b29953cb"
   },
   "outputs": [],
   "source": [
    "INPUT_SIZE = len(TEXT.vocab)\n",
    "EMBEDDING_SIZE = 100\n",
    "HIDDEN_SIZE = 256\n",
    "OUTPUT_SIZE = 1\n",
    "NUM_LAYERS = 1\n",
    "\n",
    "model = RNN(INPUT_SIZE, HIDDEN_SIZE, EMBEDDING_SIZE, NUM_LAYERS, OUTPUT_SIZE)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIg1ZcDueVR-"
   },
   "source": [
    "### A function that tells us how many trainable parameters do we have in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mwqdkPIFREPN",
    "outputId": "7687c9a3-dd99-49c8-ba52-254d5369df5d"
   },
   "outputs": [],
   "source": [
    "def count_trainable_params(model):\n",
    "  return sum(p.numel() for p in model.parameters() if p.requires_grad == True)\n",
    "\n",
    "print(f'The model has {count_trainable_params(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uX6YUTkYfKzm"
   },
   "source": [
    "### Trainning the model.\n",
    "We are going to use th `SGD` as our optimizer and `BCEWithLogitsLoss` as our loss.\n",
    "\n",
    "* The reason we are using this loss is because we don't have the activation function on our last layer [more](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html).\n",
    "\n",
    "* This loss combines a Sigmoid layer and the BCELoss in one single class. This version is more numerically stable than using a plain Sigmoid followed by a BCELoss as, by combining the operations into one layer, we take advantage of the log-sum-exp trick for numerical stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y6xR6X4pREIo"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ce2JxFIgUMW"
   },
   "source": [
    "### Pushing the model and loss function to the devics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mrNXBDvigTMd"
   },
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fBeD9BTfgmea"
   },
   "source": [
    "$L$oss and $A$ccuracy.\n",
    "\n",
    "Our criterion function calculates the loss, however we have to write our function to calculate the accuracy.\n",
    "\n",
    "This function first feeds the predictions through a sigmoid layer, squashing the values between 0 and 1, we then round them to the nearest integer. This rounds any value greater than 0.5 to 1 (a positive sentiment) and the rest to 0 (a negative sentiment).\n",
    "\n",
    "We then calculate how many rounded predictions equal the actual labels and average it across the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EFPWjrS7REFh"
   },
   "outputs": [],
   "source": [
    "def accuracy(y_preds, y_true):\n",
    "  #round predictions to the closest integer\n",
    "  rounded_preds = torch.round(torch.sigmoid(y_preds))\n",
    "  correct = (rounded_preds == y_true).float() #convert into float for division \n",
    "  acc = correct.sum() / len(correct)\n",
    "  return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jSxxiNYjhMTR"
   },
   "source": [
    "The train function iterates over all examples, one batch at a time.\n",
    "\n",
    "**model.train()** is used to put the model in \"training mode\", which turns on ``dropout`` and ``batch normalization``. Although we aren't using them in this model, it's good practice to include it.\n",
    "\n",
    "For each batch, we first ``zero the gradients``. Each parameter in a model has a grad attribute which stores the gradient calculated by the criterion. PyTorch does not automatically remove (or \"zero\") the gradients calculated from the last gradient calculation, so they must be manually zeroed.\n",
    "\n",
    "We then feed the batch of sentences, ``batch.text``, into the model. Note, you do not need to do ``model.forward(batch.text)``, simply calling the model works. The squeeze is needed as the predictions are initially size ``[batch size, 1]``, and we need to remove the dimension of size ``1`` as PyTorch expects the predictions input to our criterion function to be of size ``[batch size]``.\n",
    "\n",
    "The loss and accuracy are then calculated using our predictions and the labels, batch.label, with the loss being averaged over all examples in the batch.\n",
    "\n",
    "We calculate the gradient of each parameter with loss.``backward()``, and then update the parameters using the gradients and optimizer algorithm with ``optimizer.step().``\n",
    "\n",
    "The loss and accuracy is accumulated across the epoch, the ``.item()`` method is used to extract a scalar from a tensor which only contains a single value.\n",
    "\n",
    "Finally, we return the loss and accuracy, averaged across the epoch. The len of an iterator is the number of batches in the iterator.\n",
    "\n",
    "You may recall when initializing the LABEL field, we set ``dtype=torch.float``. This is because TorchText sets tensors to be LongTensors by default, however our criterion expects both inputs to be ``FloatTensors``. Setting the dtype to be torch.float, did this for us. The alternative method of doing this would be to do the conversion inside the train function by passing ``batch.label.float()`` instad of ``batch.label`` to the criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_t4zDse6hHZa"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        acc = accuracy(predictions, batch.label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YbfcmRVai4GZ"
   },
   "source": [
    "\n",
    "``evaluate`` is similar to train, with a few modifications as you don't want to update the parameters when evaluating.\n",
    "\n",
    "``model.eval()`` puts the model in \"evaluation mode\", this turns off ``dropout`` and ``batch normalization``. Again, we are not using them in this model, but it is good practice to include them.\n",
    "\n",
    "No gradients are calculated on PyTorch operations inside the ``with no_grad()`` block. This causes less memory to be used and speeds up computation.\n",
    "\n",
    "The rest of the function is the same as train, with the removal of ``optimizer.zero_grad(),`` ``loss.backward()`` and ``optimizer.step()``, as we do not update the model's parameters when evaluating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AcJ_HDXTi3my"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            loss = criterion(predictions, batch.label)\n",
    "            acc = accuracy(predictions, batch.label)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qOvKbRaZjpLY"
   },
   "source": [
    "We'll also create a function to tell us how long an epoch takes to compare training times between models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jKPleAJchQgz"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1rytha2WjxoZ"
   },
   "source": [
    "We then train the model through multiple epochs, an epoch being a complete pass through all examples in the training and validation sets.\n",
    "\n",
    "At each epoch, if the validation loss is the best we have seen so far, we'll save the parameters of the model and then after training has finished we'll use that model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z47ipa8fhQey",
    "outputId": "60dc38ec-499d-4b34-fd92-a1c5d274292d"
   },
   "outputs": [],
   "source": [
    "N_EPOCHS = 5\n",
    "best_valid_loss = float('inf')\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, validation_iterator, criterion)\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yrvi4XSzkH8I"
   },
   "source": [
    "You may have noticed the loss is not really decreasing and the accuracy is poor. This is due to several issues with the model which we'll improve in the next notebook.\n",
    "\n",
    "Finally, the metric we actually care about, the test loss and accuracy, which we get from our parameters that gave us the best validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3OFk7fTKhQaQ",
    "outputId": "65788fc8-0a4c-42fc-dd12-52f90c9c7c34"
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('tut1-model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NvSslfNwhQXI"
   },
   "source": [
    "### Next Steps\n",
    "In the next notebook, the improvements we will make are:\n",
    "\n",
    "* packed padded sequences\n",
    "* pre-trained word embeddings\n",
    "* different RNN architecture\n",
    "* bidirectional RNN\n",
    "* multi-layer RNN\n",
    "* regularization\n",
    "* a different optimizer\n",
    "\n",
    "This will allow us to achieve ~84% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ospkjEfNhRHo"
   },
   "source": [
    "### Credits:\n",
    "\n",
    "* [bentrevett](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/2%20-%20Upgraded%20Sentiment%20Analysis.ipynb)\n",
    "* [PyTorch](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cdq86ChChexb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "02_Updated_Sentiment_Analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
