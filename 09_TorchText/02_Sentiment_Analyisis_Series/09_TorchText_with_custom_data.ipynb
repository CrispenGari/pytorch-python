{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "loved-default",
   "metadata": {},
   "source": [
    "### TorchText with our custom data\n",
    "From the previous notebook we have been using the `IMDB` dataset for sentiment analyisis classification. In real world we will want to work with our own dataset. In this notebook we are going to cover that with TorchText helper functions which have have been using all long. We:\n",
    "\n",
    "1. Define the Fields\n",
    "2. Loaded the Dataset\n",
    "3. Created the Splits\n",
    "\n",
    "Recall:\n",
    "```\n",
    "TEXT = data.Field()\n",
    "LABEL = data.LabelField()\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "train_data, valid_data = train_data.split()\n",
    "```\n",
    "Torch text is cappable of reading 3 files which are:\n",
    "\n",
    "1. json -> javascript object notation\n",
    "2. csv -> comma serperated values\n",
    "3. tsv -> tab seperated values\n",
    "\n",
    "**`Json` is the best we will explain why later on.**\n",
    "\n",
    "We have files that are in the data Folder. The `train.json` has the following formart.\n",
    "\n",
    "```json\n",
    "{\"name\": \"Jocko\", \"quote\": \"You must own everything in your world. There is no one else to blame.\", \"score\":1}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "plain-bubble",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.legacy import data, datasets\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removable-survey",
   "metadata": {},
   "source": [
    "### Define the Fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "complimentary-walter",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = lambda x: x.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "pleased-mercury",
   "metadata": {},
   "outputs": [],
   "source": [
    "QOUTE = data.Field(tokenize=tokenizer, lower=True)\n",
    "LABEL = data.LabelField(dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contrary-assembly",
   "metadata": {},
   "source": [
    "Next, we must tell TorchText which fields apply to which elements of the json object.\n",
    "\n",
    "For `json` data, we must create a dictionary where:\n",
    "\n",
    "* the key matches the key of the json object\n",
    "* the value is a tuple where:\n",
    "* the first element becomes \n",
    "    * the batch object's attribute name\n",
    "    * the second element is the name of the Field\n",
    "    \n",
    "**What do we mean when we say \"becomes the batch object's attribute name\"?**\n",
    "\n",
    "Recall in the previous notebooks where we accessed the `TEXT` and `LABEL` fields in the `train/evaluation` loop by using `batch.text` and `batch.label`, this is because `TorchText` sets the `text` object to have a `text` and `label` attribute, each being a tensor containing either the `text` or the `label`.\n",
    "\n",
    "**Take Home Notes**:\n",
    "1. The order of the keys in the fields dictionary does not matter, as long as its keys match the json data keys.\n",
    "2. The Field name does not have to match the key in the json object, e.g. you can use `LABEL` for the \"score\" field.\n",
    "3. When dealing with json data, not all of the keys have to be used, e.g. we did not use the \"name\" field.\n",
    "4. Also, if the values of json field are a string then the Fields tokenization is applied (default is to split the string on spaces), however if the values are a list then no tokenization is applied. Usually it is a good idea for the data to already be tokenized into a list, this saves time as you don't have to wait for TorchText to do it.\n",
    "5. The value of the json fields do not have to be the same type. Some examples can have their \"quote\" as a string, and some as a list. The tokenization will only get applied to the ones with their \"quote\" as a string.\n",
    "6. If you are using a json field, every single example must have an instance of that field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "scheduled-details",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = {\n",
    "    'quote': ('q' , QOUTE),\n",
    "    'score': ('s', LABEL)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thick-franchise",
   "metadata": {},
   "source": [
    "\n",
    "Now, in a training loop we can iterate over the data iterator and access the qoute via batch.q, the score/label via batch.s\n",
    "\n",
    "We then create our datasets (train_data and test_data) with the TabularDataset.splits function.\n",
    "\n",
    "The path argument specifices the **top level folder** (in our case `data`) common among both datasets, and the `train` and `test` arguments specify the filename of each dataset, e.g. here the train dataset is located at data/train.json.**We can also specify the validation if we have a file containing validation data**.\n",
    "\n",
    "We tell the function we are using json data, and pass in our fields dictionary defined previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "tight-constraint",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data, test_data = data.TabularDataset.splits(\n",
    "            path = 'data',\n",
    "            train=\"train.json\", \n",
    "            test=\"test.json\", \n",
    "            format=\"json\", \n",
    "            fields=fields\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "activated-gabriel",
   "metadata": {},
   "source": [
    "We can then view an example to make sure it has worked correctly.\n",
    "**Notice how the field names (q and s) match up with what was defined in the fields dictionary.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "alpine-quest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'q': ['you', 'must', 'own', 'everything', 'in', 'your', 'world.', 'there', 'is', 'no', 'one', 'else', 'to', 'blame.'], 's': 1}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-black",
   "metadata": {},
   "source": [
    "We can now use `train_data`, `test_data` and `valid_data`(if available) to build a `vocabulary` and create `iterators`, as in the previous `notebooks`. We can access all attributes by using batch.s and batch.q.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chubby-entrepreneur",
   "metadata": {},
   "source": [
    "### Reading CSV/TSV\n",
    "`csv` and `tsv` are very similar, except `csv` has elements separated by commas and `tsv` by tabs.\n",
    "\n",
    "Using the same example above, our tsv data will be in the form of:\n",
    "\n",
    "```tsv\n",
    "name\tquote\tscore\n",
    "Jocko\tYou must own everything in your world. There is no one else to blame.\t1\n",
    "Bruce Lee\tDo not pray for an easy life, pray for the strength to endure a difficult one.\t1\n",
    "Potato guy\tStand tall, and rice like a potato!\t0\n",
    "```\n",
    "\n",
    "That is, on each row the elements are separated by tabs and we have one example per row. The first row is usually a header (i.e. the name of each of the columns), but sometimes with on header.\n",
    "\n",
    "**You cannot have lists within tsv or csv data.**\n",
    "\n",
    "The way the fields are defined is a bit different to json. We now use a list of tuples, where each element is also a tuple. The first element of these inner tuples will become the batch object's attribute name, second element is the Field name.\n",
    "\n",
    "Unlike the json data, the tuples have to be in the same order that they are within the tsv data. Due to this, when skipping a column of data a tuple of Nones needs to be used.\n",
    "\n",
    "However, if you only wanted to use the name and svore column, you could just use two tuples as they are the first two columns.\n",
    "\n",
    "We change our TabularDataset to read the correct `.tsv` files, and change the format argument to 'tsv'.\n",
    "\n",
    "If your data has a header, which ours does, it must be skipped by passing `skip_header = True`. If not, TorchText will think the header is an example. By default, `skip_header` will be `False.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "charged-shade",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [(None, None), ('q', QOUTE) , ('s', LABEL)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fancy-semester",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,  test_data = data.TabularDataset.splits(\n",
    "                                        path = 'data',\n",
    "                                        train = 'train.csv',\n",
    "                                        test = 'test.csv',\n",
    "                                        format = 'csv',\n",
    "                                        fields = fields,\n",
    "                                        skip_header = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chicken-kidney",
   "metadata": {},
   "source": [
    "### If you decide to specify field names as a dictionery like before you can do it as follows:\n",
    "\n",
    "```python\n",
    "fields = {\n",
    "    'quote': ('q' , QOUTE),\n",
    "    'score': ('s', LABEL)\n",
    "}\n",
    "train_data,  test_data = data.TabularDataset.splits(\n",
    "                                        path = 'data',\n",
    "                                        train = 'train.csv',\n",
    "                                        test = 'test.csv',\n",
    "                                        format = 'csv',\n",
    "                                        fields = fields,\n",
    "                                        skip_header = False # should be false\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "gross-cotton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'q': ['you', 'must', 'own', 'everything', 'in', 'your', 'world.', 'there', 'is', 'no', 'one', 'else', 'to', 'blame.'], 's': '1'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fluid-morris",
   "metadata": {},
   "source": [
    "### Why `JSON` over `CSV/TSV`?\n",
    "1. Your csv or tsv data cannot be stored lists. This means data cannot be already be tokenized, thus everytime you run your Python script that reads this data via TorchText, it has to be tokenized. Using advanced tokenizers, such as the spaCy tokenizer, takes a non-negligible amount of time. Thus, it is better to tokenize your datasets and store them in the json lines format.\n",
    "\n",
    "2. If tabs appear in your tsv data, or commas appear in your csv data, TorchText will think they are delimiters between columns. This will cause your data to be parsed incorrectly. Worst of all TorchText will not alert you to this as it cannot tell the difference between a tab/comma in a field and a tab/comma as a delimiter. As json data is essentially a dictionary, you access the data within the fields via its key, so do not have to worry about \"surprise\" delimiters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elementary-tsunami",
   "metadata": {},
   "source": [
    "### Building the Vocabularies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "infrared-appearance",
   "metadata": {},
   "outputs": [],
   "source": [
    "QOUTE.build_vocab(train_data)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "coupled-photography",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<pad>', 'a', 'for', 'pray', 'to', 'an', 'and', 'blame.', 'difficult', 'do', 'easy', 'else', 'endure', 'everything', 'in', 'is', 'life,', 'like', 'must', 'no', 'not', 'one', 'one.', 'own', 'potato!', 'rice', 'stand', 'strength', 'tall,', 'the', 'there', 'world.', 'you', 'your']\n"
     ]
    }
   ],
   "source": [
    "print(QOUTE.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "referenced-building",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(None, {'1': 0, '0': 1})\n",
      "['1', '0']\n"
     ]
    }
   ],
   "source": [
    "print(LABEL.vocab.stoi)\n",
    "print(LABEL.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "upset-county",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('to', 2), ('pray', 2)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QOUTE.vocab.freqs.most_common(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "junior-graduate",
   "metadata": {},
   "source": [
    "### Iterating over a dataset using the `BucketIterator`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twenty-rolling",
   "metadata": {},
   "source": [
    "* Then, we can create the iterators after defining our `batch size` and `device`.\n",
    "\n",
    "* By default, the `train` data is `shuffled` each epoch, but the `validation/test` data is `sorted`. However, TorchText doesn't know what to use to sort our data and it would throw an error if we don't tell it.\n",
    "\n",
    "There are two ways to handle this, you can either tell the iterator not to sort the `validation/test` data by passing `sort = False`, or you can tell it how to sort the data by passing a `sort_key`. **A sort key is a function that returns a key on which to sort the data on**. For example:\n",
    "```py\n",
    "lambda x: x.q \n",
    "```\n",
    "will sort the examples by their q attribute, i.e their quote. Ideally, you want to use a sort key as the BucketIterator will then be able to sort your examples and then minimize the amount of padding within each batch.\n",
    "\n",
    "We can then iterate over our iterator to get batches of data. **Note how by default TorchText has the batch dimension second.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "delayed-spell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(DEVICE)\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "train_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, test_data),\n",
    "    device = DEVICE,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_key = lambda x: x.q,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offensive-bailey",
   "metadata": {},
   "source": [
    "### Train Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "changing-mineral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[torchtext.legacy.data.batch.Batch of size 1]\n",
      "\t[.q]:[torch.LongTensor of size 14x1]\n",
      "\t[.s]:[torch.FloatTensor of size 1] tensor([[33],\n",
      "        [19],\n",
      "        [24],\n",
      "        [14],\n",
      "        [15],\n",
      "        [34],\n",
      "        [32],\n",
      "        [31],\n",
      "        [16],\n",
      "        [20],\n",
      "        [22],\n",
      "        [12],\n",
      "        [ 5],\n",
      "        [ 8]])\n",
      "\n",
      "[torchtext.legacy.data.batch.Batch of size 1]\n",
      "\t[.q]:[torch.LongTensor of size 16x1]\n",
      "\t[.s]:[torch.FloatTensor of size 1] tensor([[10],\n",
      "        [21],\n",
      "        [ 4],\n",
      "        [ 3],\n",
      "        [ 6],\n",
      "        [11],\n",
      "        [17],\n",
      "        [ 4],\n",
      "        [ 3],\n",
      "        [30],\n",
      "        [28],\n",
      "        [ 5],\n",
      "        [13],\n",
      "        [ 2],\n",
      "        [ 9],\n",
      "        [23]])\n",
      "\n",
      "[torchtext.legacy.data.batch.Batch of size 1]\n",
      "\t[.q]:[torch.LongTensor of size 7x1]\n",
      "\t[.s]:[torch.FloatTensor of size 1] tensor([[27],\n",
      "        [29],\n",
      "        [ 7],\n",
      "        [26],\n",
      "        [18],\n",
      "        [ 2],\n",
      "        [25]])\n"
     ]
    }
   ],
   "source": [
    "for data in train_iterator:\n",
    "    print(data, data.q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concerned-missouri",
   "metadata": {},
   "source": [
    "### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "nuclear-nudist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27],\n",
      "        [29],\n",
      "        [ 7],\n",
      "        [26],\n",
      "        [18],\n",
      "        [ 2],\n",
      "        [25]])\n",
      "tensor([[10],\n",
      "        [21],\n",
      "        [ 4],\n",
      "        [ 3],\n",
      "        [ 6],\n",
      "        [11],\n",
      "        [17],\n",
      "        [ 4],\n",
      "        [ 3],\n",
      "        [30],\n",
      "        [28],\n",
      "        [ 5],\n",
      "        [13],\n",
      "        [ 2],\n",
      "        [ 9],\n",
      "        [23]])\n",
      "tensor([[33],\n",
      "        [19],\n",
      "        [24],\n",
      "        [14],\n",
      "        [15],\n",
      "        [34],\n",
      "        [32],\n",
      "        [31],\n",
      "        [16],\n",
      "        [20],\n",
      "        [22],\n",
      "        [12],\n",
      "        [ 5],\n",
      "        [ 8]])\n"
     ]
    }
   ],
   "source": [
    "for data in train_iterator:\n",
    "    print(data.q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "judicial-hampshire",
   "metadata": {},
   "source": [
    "### That's how we can load our own dataset using `TorchText`\n",
    "\n",
    "### Credits.\n",
    "* [bentrevett](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/A%20-%20Using%20TorchText%20with%20Your%20Own%20Datasets.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "novel-enhancement",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
