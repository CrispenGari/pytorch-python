{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "07_Muticlass_Sentiment_Analyisis_FastText.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5Vbx9Wxv4Zq"
      },
      "source": [
        "### Multi-class Sentiment Analyisis using `FastText`.\n",
        "\n",
        "Up to now we have been classifying text with two outcomes either positive or otherwise. In this Notebook we are going to take it futher and be able to classify multiclass text.\n",
        "When we have more than 2 examples, our output must be a $C$ dimensional vector, where $C$ is the number of classes.\n",
        "\n",
        "We are going to use the dataset with **6** (TREC dataset) classes it's a dataset of questions and the task is to classify what category the question belongs to. We do not need to set the dtype in the ``LABEL`` field. When doing a ``mutli-class`` problem, PyTorch expects the labels to be numericalized **LongTensors**.\n",
        "The fine_grained argument allows us to use the fine-grained labels (of which there are 50 classes) or not (in which case they'll be 6 classes). You can change this how you please."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EN0JnjuvxOh"
      },
      "source": [
        "import torch\n",
        "from torchtext.legacy import data, datasets\n",
        "import numpy as np\n",
        "import random"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OhzBkzAv4Io"
      },
      "source": [
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yc4d8ldXOH7A"
      },
      "source": [
        "\n",
        "### FastTex.\n",
        "One of the key concepts in the FastText paper is that they calculate the ``n-grams`` of an input sentence and append them to the end of a sentence. Here, we'll use ``tri-grams``.\n",
        "\n",
        "We are going to create a ``generate_ngrams`` function takes a sentence that has already been tokenized, calculates the bi-grams and appends them to the end of the tokenized list.\n",
        "\n",
        "We are going to modify our `generate_bigrams` function from [this](https://github.com/CrispenGari/PyTorch-Python/blob/main/09_TorchText/02_Sentiment_Analyisis_Series/03_Faster_Sentiment_Analyisis.ipynb) notebook so that it will be a generic function that will generate, `n-grams` pairs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6tKxreTOfhq",
        "outputId": "89dd76f5-b5ed-43db-d491-fc2d9c936b8e"
      },
      "source": [
        "def generate_ngrams(x, n=2):\n",
        "  n_grams = set(zip(*[x[i:] for i in range(n)]))\n",
        "  for n_gram in n_grams:\n",
        "      x.append(' '.join(n_gram))\n",
        "  return x\n",
        "generate_ngrams(['This', 'film', 'is', 'terrible'], 3)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This', 'film', 'is', 'terrible', 'This film is', 'film is terrible']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dk9YDgfsv4FA"
      },
      "source": [
        "TEXT = data.Field(tokenize=\"spacy\", \n",
        "                  tokenizer_language=\"en_core_web_sm\",\n",
        "                  preprocessing = generate_ngrams\n",
        "                  )\n",
        "LABEL = data.LabelField()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDOTidhgv4Cm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3314d90-e1ad-4270-d0bf-851620415347"
      },
      "source": [
        "train_data, test_data = datasets.TREC.splits(TEXT, LABEL, fine_grained=False)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading train_5500.label\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "train_5500.label: 100%|██████████| 336k/336k [00:00<00:00, 1.10MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading TREC_10.label\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TREC_10.label: 100%|██████████| 23.4k/23.4k [00:00<00:00, 311kB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoQd6PpGv3_Z"
      },
      "source": [
        "validation_data, test_data = test_data.split(random_state=random.seed(SEED))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OR7ouoZv38l",
        "outputId": "2a551f57-fd45-4e3f-b9a5-dbed3ed53855"
      },
      "source": [
        "print(f\"TRAINING: \\t {len(train_data)}\")\n",
        "print(f\"TESTING: \\t {len(test_data)}\")\n",
        "print(f\"VALIDATION: \\t {len(validation_data)}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAINING: \t 5452\n",
            "TESTING: \t 150\n",
            "VALIDATION: \t 350\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYQTupxEzzKZ"
      },
      "source": [
        "### Let's look at some example of the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_c3H2DBv35R",
        "outputId": "36b0a2e0-42c3-4870-c947-48628d8ddfb5"
      },
      "source": [
        "print(vars(train_data[1]))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'text': ['What', 'films', 'featured', 'the', 'character', 'Popeye', 'Doyle', '?', 'Doyle ?', 'the character', 'Popeye Doyle', 'character Popeye', 'What films', 'films featured', 'featured the'], 'label': 'ENTY'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0ZWkfuk0V7F"
      },
      "source": [
        "### Building a vocabulary.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxqghotCv32x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6000ab0-4f14-411d-8ed7-79a6fd30121a"
      },
      "source": [
        "MAX_VOCAB_SIZE = 25_000\n",
        "TEXT.build_vocab(\n",
        "    train_data,\n",
        "    max_size = MAX_VOCAB_SIZE,\n",
        "    vectors= \"glove.6B.100d\",\n",
        "    unk_init = torch.Tensor.normal_\n",
        ")\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:42, 5.30MB/s]                           \n",
            "100%|█████████▉| 399755/400000 [00:21<00:00, 19080.31it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRNGM9yN04j3"
      },
      "source": [
        "### Checking Labels.\n",
        "\n",
        "\n",
        "The 6 labels (for the non-fine-grained case) correspond to the 6 types of questions in the dataset:\n",
        "\n",
        "* HUM for questions about humans\n",
        "* ENTY for questions about entities\n",
        "* DESC for questions asking you for a description\n",
        "* NUM for questions where the answer is numerical\n",
        "* LOC for questions where the answer is a location\n",
        "* ABBR for questions asking about abbreviations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUyi-RSqv30A",
        "outputId": "07a6c0e6-458d-479c-e67b-d0e73268aadf"
      },
      "source": [
        "LABEL.vocab.stoi"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(None,\n",
              "            {'ABBR': 5, 'DESC': 2, 'ENTY': 0, 'HUM': 1, 'LOC': 4, 'NUM': 3})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85Ts84-31gmU"
      },
      "source": [
        "### Creating iterators.\n",
        "\n",
        "As usual we want to use the `BucketIterator` to create iterators for all sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnmUAtkEv3xg"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "train_iterator, validation_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, validation_data, test_data),\n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device\n",
        ")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhYnUI7c3IdA"
      },
      "source": [
        "### Creating A `CNN` model to classify Text.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTif6iagv3uY"
      },
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn import  functional as F"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0ihanj0C_nL"
      },
      "source": [
        "class FastText(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, output_dim, pad_idx):\n",
        "    super(FastText, self).__init__()\n",
        "\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
        "    self.fc = nn.Linear(embedding_dim, output_dim )\n",
        "\n",
        "  def forward(self, text):\n",
        "    embedded = self.embedding(text).permute(1, 0, 2)\n",
        "    pooled = F.avg_pool2d(embedded, (embedded.shape[1], 1)).squeeze(1) \n",
        "    return self.fc(pooled)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5y2GtbUv4XpR"
      },
      "source": [
        "### Hyper parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAgVRi4dv3o4",
        "outputId": "9c694709-9069-4409-de3a-75f7f1f1b740"
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab) # # 25002\n",
        "EMBEDDING_DIM = 100\n",
        "OUTPUT_DIM = len(LABEL.vocab.stoi)\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token] # 0\n",
        "\n",
        "model = FastText(INPUT_DIM, \n",
        "            EMBEDDING_DIM, \n",
        "            OUTPUT_DIM,  \n",
        "            PAD_IDX)\n",
        "model"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FastText(\n",
              "  (embedding): Embedding(25002, 100, padding_idx=1)\n",
              "  (fc): Linear(in_features=100, out_features=6, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_5g5nLb4kIv"
      },
      "source": [
        "### Trainable parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofhxizcx4nzp",
        "outputId": "2138e5b9-63b4-4db4-acb0-06941116aba4"
      },
      "source": [
        "def count_trainable_params(model):\n",
        "  return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has  {count_trainable_params(model):,} trainable parameters')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has  2,500,806 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n95_R_oY5M32"
      },
      "source": [
        "### Loading the pretrainned embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evoBPJo-4nxT"
      },
      "source": [
        "pretrained_embeddings = TEXT.vocab.vectors"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJ4PwUQI4nuQ",
        "outputId": "7e11d984-752c-482b-f4c0-6497d4ad3a4b"
      },
      "source": [
        "model.embedding.weight.data.copy_(pretrained_embeddings)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.9269,  1.4873,  0.9007,  ...,  0.1233,  0.3499,  0.6173],\n",
              "        [ 0.7262,  0.0912, -0.3891,  ...,  0.0821,  0.4440, -0.7240],\n",
              "        [ 0.1638,  0.6046,  1.0789,  ..., -0.3140,  0.1844,  0.3624],\n",
              "        ...,\n",
              "        [-1.0721,  2.5816, -0.4311,  ..., -0.0361,  0.7994,  1.2356],\n",
              "        [ 0.0943,  0.4924,  1.0734,  ...,  0.0651,  0.5112,  0.5391],\n",
              "        [-0.0063,  1.0709, -0.4854,  ..., -1.4409, -1.1118, -0.8337]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Znr3BZw5dQd"
      },
      "source": [
        "### Make the `<pad>` and `<unk>` layers zeros.\n",
        "The first two layers should have zeros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bOZufLS4nr3",
        "outputId": "1a9a1c7b-48ab-4868-a6c9-de0ad71e1bea"
      },
      "source": [
        "for i in range(2):\n",
        "  model.embedding.weight.data[i] = torch.zeros(EMBEDDING_DIM)\n",
        "\n",
        "model.embedding.weight.data"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.1638,  0.6046,  1.0789,  ..., -0.3140,  0.1844,  0.3624],\n",
              "        ...,\n",
              "        [-1.0721,  2.5816, -0.4311,  ..., -0.0361,  0.7994,  1.2356],\n",
              "        [ 0.0943,  0.4924,  1.0734,  ...,  0.0651,  0.5112,  0.5391],\n",
              "        [-0.0063,  1.0709, -0.4854,  ..., -1.4409, -1.1118, -0.8337]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vX35E6dQ6FuW"
      },
      "source": [
        "### Trainning the Model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6B_FzjS4nph"
      },
      "source": [
        "Generally:\n",
        "\n",
        "``CrossEntropyLoss`` is used when our examples exclusively belong to one of $C$ classes\n",
        "\n",
        "``BCEWithLogitsLoss`` is used when our examples exclusively belong to only 2 classes (0 and 1) and is also used in the case where our examples belong to between 0 and $C$ classes (aka multilabel classification).\n",
        "\n",
        "In this Notebook we are going to use ``CrossEntropyLoss``."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWFS7JN84nlt"
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUW7ZjR26mUO"
      },
      "source": [
        "### Pushing the model and Criterion to the device."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEw3g4r_4nij"
      },
      "source": [
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKpdpOyn4nc1"
      },
      "source": [
        "### Accuracy On Multi-Class Classification\n",
        "\n",
        "Before, we had a function that calculated accuracy in the binary label case, where we said if the value was over ``0.5`` then we would assume it is positive. In the case where we have more than 2 classes, our model outputs a $C$ dimensional vector, where the value of each element is the beleief that the example belongs to that class.\n",
        "\n",
        "For example, in our labels we have:\n",
        "\n",
        " ``**'HUM' = 0, 'ENTY' = 1, 'DESC' = 2, 'NUM' = 3, 'LOC' = 4 and 'ABBR' = 5**.``\n",
        "\n",
        "If the output of our model was something like: \n",
        "\n",
        "``[5.1, 0.3, 0.1, 2.1, 0.2, 0.6]`` \n",
        "\n",
        "this means that the model strongly believes the example belongs to class ``0``, a question about a human, and slightly believes the example belongs to class 3, a numerical question.\n",
        "\n",
        "We calculate the accuracy by performing an argmax to get the index of the maximum value in the prediction for each element in the batch, and then counting how many times this equals the actual label. We then average this across the batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6AtZJZu4nZv"
      },
      "source": [
        "def categorical_accuracy(preds, y):\n",
        "    top_pred = preds.argmax(1, keepdim = True)\n",
        "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
        "    acc = correct.float() / y.shape[0]\n",
        "    return acc"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHu5tkoL8Ahe"
      },
      "source": [
        "The training loop and evaluating is similar to before, without the need to squeeze the model predictions as CrossEntropyLoss expects the input to be ``[batch size, n classes]`` and the label to be ``[batch size]``.`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMyOdhwK4nWj"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    model.train()\n",
        "    for batch in iterator:\n",
        "        optimizer.zero_grad()\n",
        "        text = batch.text\n",
        "        predictions = model(text).squeeze(1)\n",
        "        loss = criterion(predictions, batch.label)\n",
        "        acc = categorical_accuracy(predictions, batch.label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            text = batch.text\n",
        "            predictions = model(text).squeeze(1)\n",
        "            loss = criterion(predictions, batch.label)\n",
        "            acc = categorical_accuracy(predictions, batch.label)\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBE1W_Gs8lxp"
      },
      "source": [
        ""
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTi6evc-8mbj"
      },
      "source": [
        "We'll also create a function to tell us how long an epoch takes to compare training times between models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdENIcef4e26"
      },
      "source": [
        "import time\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tI6gVjA8sNV",
        "outputId": "30772050-5864-43fe-d8e5-32b76f977d5b"
      },
      "source": [
        "N_EPOCHS = 20\n",
        "best_valid_loss = float('inf')\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, validation_iterator, criterion)\n",
        "    end_time = time.time()\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'best-model.pt')\n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.226 | Train Acc: 97.04%\n",
            "\t Val. Loss: 0.435 |  Val. Acc: 84.32%\n",
            "Epoch: 02 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.198 | Train Acc: 97.27%\n",
            "\t Val. Loss: 0.430 |  Val. Acc: 84.32%\n",
            "Epoch: 03 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.178 | Train Acc: 97.60%\n",
            "\t Val. Loss: 0.425 |  Val. Acc: 84.84%\n",
            "Epoch: 04 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.153 | Train Acc: 97.89%\n",
            "\t Val. Loss: 0.416 |  Val. Acc: 85.10%\n",
            "Epoch: 05 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.143 | Train Acc: 98.07%\n",
            "\t Val. Loss: 0.409 |  Val. Acc: 85.89%\n",
            "Epoch: 06 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.123 | Train Acc: 98.33%\n",
            "\t Val. Loss: 0.403 |  Val. Acc: 85.63%\n",
            "Epoch: 07 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.120 | Train Acc: 98.58%\n",
            "\t Val. Loss: 0.404 |  Val. Acc: 86.18%\n",
            "Epoch: 08 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.102 | Train Acc: 98.78%\n",
            "\t Val. Loss: 0.402 |  Val. Acc: 86.44%\n",
            "Epoch: 09 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.095 | Train Acc: 98.82%\n",
            "\t Val. Loss: 0.396 |  Val. Acc: 87.52%\n",
            "Epoch: 10 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.090 | Train Acc: 98.91%\n",
            "\t Val. Loss: 0.401 |  Val. Acc: 87.26%\n",
            "Epoch: 11 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.081 | Train Acc: 98.98%\n",
            "\t Val. Loss: 0.405 |  Val. Acc: 87.26%\n",
            "Epoch: 12 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.073 | Train Acc: 99.09%\n",
            "\t Val. Loss: 0.403 |  Val. Acc: 87.26%\n",
            "Epoch: 13 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train Acc: 99.13%\n",
            "\t Val. Loss: 0.397 |  Val. Acc: 87.78%\n",
            "Epoch: 14 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train Acc: 99.15%\n",
            "\t Val. Loss: 0.400 |  Val. Acc: 87.52%\n",
            "Epoch: 15 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train Acc: 99.24%\n",
            "\t Val. Loss: 0.404 |  Val. Acc: 87.52%\n",
            "Epoch: 16 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train Acc: 99.35%\n",
            "\t Val. Loss: 0.401 |  Val. Acc: 87.52%\n",
            "Epoch: 17 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.052 | Train Acc: 99.36%\n",
            "\t Val. Loss: 0.406 |  Val. Acc: 87.52%\n",
            "Epoch: 18 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.048 | Train Acc: 99.36%\n",
            "\t Val. Loss: 0.400 |  Val. Acc: 87.78%\n",
            "Epoch: 19 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.045 | Train Acc: 99.47%\n",
            "\t Val. Loss: 0.411 |  Val. Acc: 87.78%\n",
            "Epoch: 20 | Epoch Time: 0m 0s\n",
            "\tTrain Loss: 0.040 | Train Acc: 99.51%\n",
            "\t Val. Loss: 0.408 |  Val. Acc: 87.78%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xT86RNrg-s-7"
      },
      "source": [
        "### Evaluating the `model`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0i6BFN8Y84Si",
        "outputId": "b47a36ab-3776-482f-e953-35a1eca816d5"
      },
      "source": [
        "model.load_state_dict(torch.load('best-model.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.448 | Test Acc: 87.17%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOPHRtXX-xh8"
      },
      "source": [
        "### User Input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kkmxzt7-oAV"
      },
      "source": [
        "import spacy\n",
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()\n",
        "\n",
        "def predict_class(model, sentence):\n",
        "  model.eval()\n",
        "  tokenized = generate_ngrams([tok.text for tok in nlp.tokenizer(sentence)])\n",
        "  indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
        "  tensor = torch.LongTensor(indexed).to(device)\n",
        "  tensor = tensor.unsqueeze(1)\n",
        "  prediction = torch.argmax(model(tensor))\n",
        "  return prediction.item()"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpTBE148_dB0"
      },
      "source": [
        "labels = dict(LABEL.vocab.stoi)\n",
        "labels = dict([(v, k) for (k, v) in labels.items()])"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoTqjN-A-_eJ"
      },
      "source": [
        "### Getting the user input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRA4HBJW_pXD",
        "outputId": "080e9a0f-9c31-4f32-dda2-e15c0fde5284"
      },
      "source": [
        "while True:\n",
        "  qn = input(\"Enter a question\\nOR 'exit' to close:\\n\")\n",
        "\n",
        "  if qn.lower() == \"exit\":\n",
        "    break\n",
        "  pred_class = predict_class(model, qn)\n",
        "  print(f'Predicted class is: {pred_class} = {labels[pred_class]}')"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter a question\n",
            "OR 'exit' to close:\n",
            "What is your name?\n",
            "Predicted class is: 2 = DESC\n",
            "Enter a question\n",
            "OR 'exit' to close:\n",
            "Where do you live?\n",
            "Predicted class is: 4 = LOC\n",
            "Enter a question\n",
            "OR 'exit' to close:\n",
            "Who is your father?\n",
            "Predicted class is: 1 = HUM\n",
            "Enter a question\n",
            "OR 'exit' to close:\n",
            "How are you?\n",
            "Predicted class is: 2 = DESC\n",
            "Enter a question\n",
            "OR 'exit' to close:\n",
            "exit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOBw-Yn1R14Q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}