{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ranging-interview",
   "metadata": {},
   "source": [
    "### TorchText\n",
    "* The torchtext package consists of data processing utilities and popular datasets for natural language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "checked-development",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy, os\n",
    "from torchtext.legacy.data import  Field, TabularDataset, BucketIterator\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-church",
   "metadata": {},
   "source": [
    "\n",
    "1 . **Field** - Defines a datatype together with instructions for converting to Tensor.\n",
    "```python\n",
    "torchtext.data.Field(sequential=True, use_vocab=True, init_token=None, eos_token=None, fix_length=None, dtype=torch.int64, preprocessing=None, postprocessing=None, lower=False, tokenize=None, tokenizer_language='en', include_lengths=False, batch_first=False, pad_token='<pad>', unk_token='<unk>', pad_first=False, truncate_first=False, stop_words=None, is_target=False)\n",
    "```\n",
    "* [Docs](https://text-docs.readthedocs.io/en/latest/data.html#fields)\n",
    "2 . **TabularDataset** - Defines a Dataset of columns stored in CSV, TSV, or JSON format.\n",
    "```python\n",
    "torchtext.data.TabularDataset(path, format, fields, skip_header=False, csv_reader_params={}, **kwargs)\n",
    "```        \n",
    "3 . **BucketIterator** - Defines an iterator that batches examples of similar lengths together. Minimizes amount of padding needed while producing freshly shuffled batches for each new epoch. See pool for the bucketing procedure used.\n",
    "```py\n",
    " torchtext.data.BucketIterator(dataset, batch_size, sort_key=None, device=None, batch_size_fn=None, train=True, repeat=False, shuffle=None, sort=None, sort_within_batch=None)        \n",
    "```\n",
    "* [Docs](https://text-docs.readthedocs.io/en/latest/data.html#iterators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "surprising-scanning",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_english = spacy.load('en_core_web_sm')\n",
    "def tokenize(sent):\n",
    "    return [tok.text for tok in spacy_english.tokenizer(sent)]\n",
    "\n",
    "tokenizer2 = lambda x: x.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deadly-parts",
   "metadata": {},
   "source": [
    "> We have files in the `data` for this example of text processing. We are going to use one of those files as an example.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "rental-element",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>quote</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jocko</td>\n",
       "      <td>You must own everything in your world. There i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bruce Lee</td>\n",
       "      <td>Do not pray for an easy life, pray for the str...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Potato guy</td>\n",
       "      <td>Stand tall, and rice like a potato!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         name                                              quote  score\n",
       "0       Jocko  You must own everything in your world. There i...      1\n",
       "1   Bruce Lee  Do not pray for an easy life, pray for the str...      1\n",
       "2  Potato guy                Stand tall, and rice like a potato!      0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/train.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "japanese-traveler",
   "metadata": {},
   "source": [
    "> So here we are interested in the `quote` and `score`. We are going to create a `Field` for each key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fleet-marathon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torchtext.legacy.data.field.Field at 0x1cb58b154c0>,\n",
       " <torchtext.legacy.data.field.Field at 0x1cb58b15fa0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qoute = Field(sequential=True, use_vocab=True, tokenize=tokenize, lower=True)\n",
    "score = Field(sequential=False, use_vocab=False)\n",
    "qoute, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "opened-difference",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'quote': ('quote', <torchtext.legacy.data.field.Field at 0x1cb58b154c0>),\n",
       " 'score': ('score', <torchtext.legacy.data.field.Field at 0x1cb58b15fa0>)}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fields = {\n",
    "    \"quote\": (\"quote\", qoute),\n",
    "    \"score\": (\"score\", score)\n",
    "}\n",
    "fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increasing-reconstruction",
   "metadata": {},
   "source": [
    "> Creating dataset, train and test using `TabularDataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "distributed-provider",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = TabularDataset.splits(path = 'data',\n",
    "                             train=\"train.csv\", \n",
    "                             test=\"test.csv\", \n",
    "                             format=\"csv\", \n",
    "                             fields=fields\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uniform-multimedia",
   "metadata": {},
   "source": [
    "> Note that we can also load the `json`, `tsv` the same way for example:\n",
    "\n",
    "##### Loading `json`\n",
    "```python\n",
    "train, test = TabularDataset.splits('data',\n",
    "                             train=\"train.json\", \n",
    "                             test=\"test.json\", \n",
    "                             validation = \"validation.json\",\n",
    "                             format=\"json\", \n",
    "                             fields=fields\n",
    "                             )\n",
    "```\n",
    "\n",
    "##### Loading `tsv`\n",
    "```\n",
    "train, test = TabularDataset.splits('data',\n",
    "                             train=\"train.tsv\", \n",
    "                             test=\"test.tsv\", \n",
    "                             validation = \"validation.tsv\",\n",
    "                             format=\"tsv\", \n",
    "                             fields=fields\n",
    "                             )\n",
    "```\n",
    "##### Loading `csv`\n",
    "```python\n",
    "train, test = TabularDataset.splits('data',\n",
    "                             train=\"train.csv\", \n",
    "                             test=\"test.csv\", \n",
    "                             format=\"csv\", \n",
    "                             validation = \"validation.csv\",\n",
    "                             fields=fields\n",
    "                             )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "formed-dress",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['you',\n",
       " 'must',\n",
       " 'own',\n",
       " 'everything',\n",
       " 'in',\n",
       " 'your',\n",
       " 'world',\n",
       " '.',\n",
       " 'there',\n",
       " 'is',\n",
       " 'no',\n",
       " 'one',\n",
       " 'else',\n",
       " 'to',\n",
       " 'blame',\n",
       " '.']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0].quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "missing-aside",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache\\glove.6B.zip: 862MB [34:19, 419kB/s]                                                                     \n",
      "100%|███████████████████████████████████████████████████████████████████████▉| 399999/400000 [00:53<00:00, 7513.45it/s]\n"
     ]
    }
   ],
   "source": [
    "qoute.build_vocab(train, max_size=10000, min_freq=1, vectors=\"glove.6B.100d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "noble-planet",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = BucketIterator.splits(\n",
    "    (train, test), batch_size=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "governing-trader",
   "metadata": {},
   "outputs": [],
   "source": [
    "for X in train_set:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "close-louisiana",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[torchtext.legacy.data.batch.Batch of size 2]\n",
       "\t[.quote]:[torch.LongTensor of size 16x2]\n",
       "\t[.score]:[torch.LongTensor of size 2]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-cloud",
   "metadata": {},
   "outputs": [],
   "source": [
    "> Creating a `LSTM_RNN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "saving-webmaster",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(qoute.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "waiting-architecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "proof-agency",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN_LSTM(\n",
       "  (emb): Embedding(37, 100)\n",
       "  (lstm): LSTM(100, 512, num_layers=2)\n",
       "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = len(qoute.vocab)\n",
    "hidden_size = 512\n",
    "num_layers = 2\n",
    "embedding_size = 100\n",
    "learning_rate = 0.005\n",
    "num_epochs = 10\n",
    "class RNN_LSTM(nn.Module):\n",
    "    def __init__(self, input_size, embed_size, hidden_size, num_layers):\n",
    "        super(RNN_LSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.emb = nn.Embedding(input_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_0 = torch.zeros(self.num_layers, x.size(1), self.hidden_size)\n",
    "        c_0 = torch.zeros(self.num_layers, x.size(1), self.hidden_size)\n",
    "        \n",
    "        embeded = self.emb(x)\n",
    "        output, _ = self.lstm(embeded, (h_0, c_0))\n",
    "        output = output[:, -1, :]\n",
    "        return self.fc(output)\n",
    "    \n",
    "net = RNN_LSTM(input_size,embedding_size, hidden_size, num_layers)\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "hungry-coaching",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.zeros(32, 10)\n",
    "a.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "public-diabetes",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "exotic-gamma",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.3398,  0.2094,  0.4635,  ..., -0.2339,  0.4730, -0.0288],\n",
       "        ...,\n",
       "        [ 0.4918,  1.1164,  1.1424,  ..., -0.5088,  0.6256,  0.4392],\n",
       "        [-0.4989,  0.7660,  0.8975,  ..., -0.4118,  0.4054,  0.7850],\n",
       "        [-0.5718,  0.0463,  0.8673,  ..., -0.3566,  0.9293,  0.8995]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_embeddings = qoute.vocab.vectors\n",
    "net.emb.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "apparent-reality",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "concrete-insulin",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([2])) must be the same as input size (torch.Size([18]))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-89-fca6ccea7e43>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;31m# forward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;31m# backward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    712\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    713\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 714\u001b[1;33m         return F.binary_cross_entropy_with_logits(input, target,\n\u001b[0m\u001b[0;32m    715\u001b[0m                                                   \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    716\u001b[0m                                                   \u001b[0mpos_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[1;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[0;32m   2825\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2826\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2827\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Target size ({}) must be the same as input size ({})\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2828\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2829\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary_cross_entropy_with_logits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction_enum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Target size (torch.Size([2])) must be the same as input size (torch.Size([18]))"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "# Train Network\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, batch in enumerate(train_set):\n",
    "        # Get data to cuda if possible\n",
    "        data = batch.quote\n",
    "        targets = batch.score\n",
    "\n",
    "        # forward\n",
    "        scores = net(data)\n",
    "        loss = criterion(scores.squeeze(1), targets.type_as(scores))\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient descent\n",
    "        optimizer.step()\n",
    "    print(\"loss\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-variance",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
