{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_Character_Level_Text_Generation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNucv6yiW6vJ"
      },
      "source": [
        "### Character Level text generator\n",
        "\n",
        "This notebook is the first notebook in the character level text generation. We are going to create a model that will generate text. \n",
        "\n",
        "_\"The model will be fed with a word and will predict what the next character in the sentence will be. This process will repeat itself until we generate a sentence of our desired length\"._\n",
        "\n",
        "### Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Ftc0QphfW6kq",
        "outputId": "4de620c8-2bba-4fdd-d8ca-5527cd0a1d91"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import os, time, pickle, string, random\n",
        "import numpy as np\n",
        "\n",
        "torch.__version__"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.9.0+cu102'"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mq3jWa3UXsvY"
      },
      "source": [
        "### Device"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0Nt2gYpW6ho",
        "outputId": "8e7835a5-81b0-421a-c633-b906bbe8e885"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yB05im5QX8C3"
      },
      "source": [
        "### Data\n",
        "We are going to use the dataset that I've downloaded [here](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt) we are then going to load it from google drive and the steps will be as follows:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUkux6zRW6ea",
        "outputId": "b8586653-d567-4acf-e452-9e93c743e055"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2p7xG1SW6ba",
        "outputId": "526801ba-0486-4f84-a09a-2e27dc704605"
      },
      "source": [
        "file_path = \"/content/drive/My Drive/NLP Data/text-gen/input.txt\"\n",
        "os.path.exists(file_path)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "re4DQWwwY6nw"
      },
      "source": [
        "### Loading the dataset.\n",
        "\n",
        "First, we'll define the sentences that we want our model to output the first few characters. Our dataset is a text file containing Shakespeare's plays or books that we will extract sequence of chars to use as input to our model. Then our model will learn how to complete sentences like \"Shakespeare would do\".\n",
        "\n",
        "### SEEDS\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJG0ThpFW6Yi"
      },
      "source": [
        "SEED = 42\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf4RIfW2Z5g_"
      },
      "source": [
        "### Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dg4noe2aW6Vr"
      },
      "source": [
        "def load_text_data(filename, init_dialog=False):\n",
        "  \"\"\"\n",
        "  Setting init_dialog = True will remove lines where the character who is going to speak is indicate\n",
        "  \"\"\"\n",
        "  sentences = []\n",
        "  with open(filename, 'r') as reader:\n",
        "    for line in reader:\n",
        "      if init_dialog or \":\" not in line:\n",
        "        sentences.append(line[:-1])\n",
        "\n",
        "  return sentences\n",
        "  "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-TOFo2GW6St",
        "outputId": "ef553a2e-cb22-4c1f-d794-a57bba1d5283"
      },
      "source": [
        "sentences = load_text_data(file_path)\n",
        "print('Number of sentences: ', len(sentences))\n",
        "print(sentences[:20])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sentences:  29723\n",
            "['Before we proceed any further, hear me speak.', '', 'Speak, speak.', '', 'You are all resolved rather to die than to famish?', '', 'Resolved. resolved.', '', 'First, you know Caius Marcius is chief enemy to the people.', '', \"We know't, we know't.\", '', \"Let us kill him, and we'll have corn at our own price.\", \"Is't a verdict?\", '', '', 'One word, good citizens.', '', 'We are accounted poor citizens, the patricians good.', 'would yield us but the superfluity, while it were']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPCxCtj6bEe7"
      },
      "source": [
        "### Data Cleaning\n",
        "We will convert to lowercase the text and remove non alphanumeric chracters (a parameter configuration)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tppWbqp-W6Pt"
      },
      "source": [
        "def clean_text(sentences, alpha=False):\n",
        "  if alpha:\n",
        "    # Remove non alphabetic character\n",
        "    cleaned_text = [''.join([t.lower() for t in text if t.isalpha() or t.isspace()])\n",
        "      for text in sentences]\n",
        "  else:\n",
        "    cleaned_text = [t.lower() for t in sentences]\n",
        "  \n",
        "  return [t for t in cleaned_text if t!='']\n",
        "  "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1w17i4llW6Mr",
        "outputId": "a6dbca4c-4031-40cd-9a58-9ae590af74a5"
      },
      "source": [
        "# Clean the sentences\n",
        "sentences = clean_text(sentences)\n",
        "# Join all the sentences in a one long string\n",
        "sentences = ' '.join(sentences)\n",
        "print('Number of characters: ', len(sentences))\n",
        "print(sentences[:100])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of characters:  894876\n",
            "before we proceed any further, hear me speak. speak, speak. you are all resolved rather to die than \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcYXSQLjcumC"
      },
      "source": [
        "### Creating the dictionary\n",
        "\n",
        "Now we'll create a dictionary out of all the characters that we have in the sentences and map them to an integer. This will allow us to convert our input characters to their respective integers (char2int) and viceversa (int2char)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShJeB5f4W6Jw"
      },
      "source": [
        "class CharVocab:\n",
        "  def __init__(self, type_vocab,\n",
        "               pad_token = \"<pad>\",\n",
        "               eos_token = \"<eos>\",\n",
        "               unk_token = \"<unk>\"\n",
        "               ):\n",
        "    self.type = type_vocab\n",
        "    self.int2char = []\n",
        "    if pad_token !=None:\n",
        "      self.int2char += [pad_token]\n",
        "    if eos_token !=None:\n",
        "      self.int2char += [eos_token]\n",
        "    if unk_token !=None: \n",
        "      self.int2char += [unk_token]\n",
        "\n",
        "    self.char2int = {}\n",
        "  \n",
        "  def __call__(self, text):\n",
        "    chars = set(''.join(text))\n",
        "    self.int2char += list(chars)\n",
        "    self.char2int = {char: ind for ind, char in enumerate(self.int2char)}"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBbRyY7wW6Gz",
        "outputId": "969bbd36-75b4-4505-8e96-8409082f4afb"
      },
      "source": [
        "\n",
        "vocab = CharVocab('char', None, None,'<unk>')\n",
        "vocab(sentences)\n",
        "print('Length of vocabulary: ', len(vocab.int2char))\n",
        "print('Int to Char: ', vocab.int2char)\n",
        "print('Char to Int: ', vocab.char2int)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of vocabulary:  38\n",
            "Int to Char:  ['<unk>', '.', 'q', 'p', 'n', 't', ',', 's', 'o', 'e', 'h', 'j', 'g', '!', 'u', '-', '3', 'a', 'b', 'r', 'i', '?', '&', 'c', 'k', 'x', 'm', 'y', 'z', \"'\", ' ', 'v', 'l', ';', '$', 'd', 'f', 'w']\n",
            "Char to Int:  {'<unk>': 0, '.': 1, 'q': 2, 'p': 3, 'n': 4, 't': 5, ',': 6, 's': 7, 'o': 8, 'e': 9, 'h': 10, 'j': 11, 'g': 12, '!': 13, 'u': 14, '-': 15, '3': 16, 'a': 17, 'b': 18, 'r': 19, 'i': 20, '?': 21, '&': 22, 'c': 23, 'k': 24, 'x': 25, 'm': 26, 'y': 27, 'z': 28, \"'\": 29, ' ': 30, 'v': 31, 'l': 32, ';': 33, '$': 34, 'd': 35, 'f': 36, 'w': 37}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Np4ReBM7eftJ"
      },
      "source": [
        "We are then going to save the dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Id9Y6Mb-W6EC",
        "outputId": "755e1a6a-eaef-4040-87e8-a08391f87d6c"
      },
      "source": [
        "with open( \"/content/drive/My Drive/NLP Data/text-gen/char_dict.plk\", \"wb\") as f:\n",
        "  pickle.dump(vocab.char2int, f)\n",
        "\n",
        "with open( \"/content/drive/My Drive/NLP Data/text-gen/int_dict.plk\", \"wb\") as f:\n",
        "  pickle.dump(vocab.int2char, f)\n",
        "\n",
        "print(\"Done\")\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMqR-PQqfA_w"
      },
      "source": [
        "### Creating the input data and labels for training.\n",
        "\n",
        "As we're going to predict the next character in the sequence at each time step, we'll have to divide each sentence into\n",
        "\n",
        "* **Input data:** The last input character should be excluded as it does not need to be fed into the model\n",
        "** *Target/Ground Truth Label:** One time-step ahead of the Input data as this will be the \"correct answer\" for the model at each time step corresponding to the input data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRNTPk9wW6BK"
      },
      "source": [
        "def one_hot_encode(indices, dict_size):\n",
        "  features = np.eye(dict_size, dtype=\"float32\")[indices.flatten()]\n",
        "  features = features.reshape((*indices.shape, dict_size))\n",
        "  return features\n",
        "\n",
        "def encode_text(input_text, vocab, one_hot=False):\n",
        "  ''' Encode the input_text replacing the char by its integer number based on the dictionary vocab'''\n",
        "  output = [vocab.char2int.get(character,0) for character in input_text]\n",
        "  if one_hot:\n",
        "    dict_size = len(vocab.char2int)\n",
        "    return one_hot_encode(output, dict_size)\n",
        "  else:\n",
        "    return np.array(output)\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85J3dCcv1Z3p"
      },
      "source": [
        "Now, we can encode our text, replacing every character by the integer value in the dictionary. When we have our dataset unified and prepared, we should do a quick check and see an example of the data our model will be trained on. This is generally a good idea as it allows you to see how each of the further processing steps affects the reviews and it also ensures that the data has been loaded correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SCrdAyzW57f",
        "outputId": "40f6c9f2-a6ab-4f62-82cc-15c89eef5834"
      },
      "source": [
        "train_data = encode_text(sentences, vocab, one_hot=False)\n",
        "\n",
        "# Create the input sequence, from 0 to len-1\n",
        "input_seq =train_data[:-1]\n",
        "\n",
        "# Create the target sequence, from 1 to len. It is right-shifted one place\n",
        "\n",
        "target_seq=train_data[1:]\n",
        "print('\\nOriginal text:')\n",
        "print(sentences[:100])\n",
        "print('\\nEncoded text:')\n",
        "print(train_data[:100])\n",
        "print('\\nInput sequence:')\n",
        "print(input_seq[:100])\n",
        "print('\\nTarget sequence:')\n",
        "print(target_seq[:100])\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Original text:\n",
            "before we proceed any further, hear me speak. speak, speak. you are all resolved rather to die than \n",
            "\n",
            "Encoded text:\n",
            "[18  9 36  8 19  9 30 37  9 30  3 19  8 23  9  9 35 30 17  4 27 30 36 14\n",
            " 19  5 10  9 19  6 30 10  9 17 19 30 26  9 30  7  3  9 17 24  1 30  7  3\n",
            "  9 17 24  6 30  7  3  9 17 24  1 30 27  8 14 30 17 19  9 30 17 32 32 30\n",
            " 19  9  7  8 32 31  9 35 30 19 17  5 10  9 19 30  5  8 30 35 20  9 30  5\n",
            " 10 17  4 30]\n",
            "\n",
            "Input sequence:\n",
            "[18  9 36  8 19  9 30 37  9 30  3 19  8 23  9  9 35 30 17  4 27 30 36 14\n",
            " 19  5 10  9 19  6 30 10  9 17 19 30 26  9 30  7  3  9 17 24  1 30  7  3\n",
            "  9 17 24  6 30  7  3  9 17 24  1 30 27  8 14 30 17 19  9 30 17 32 32 30\n",
            " 19  9  7  8 32 31  9 35 30 19 17  5 10  9 19 30  5  8 30 35 20  9 30  5\n",
            " 10 17  4 30]\n",
            "\n",
            "Target sequence:\n",
            "[ 9 36  8 19  9 30 37  9 30  3 19  8 23  9  9 35 30 17  4 27 30 36 14 19\n",
            "  5 10  9 19  6 30 10  9 17 19 30 26  9 30  7  3  9 17 24  1 30  7  3  9\n",
            " 17 24  6 30  7  3  9 17 24  1 30 27  8 14 30 17 19  9 30 17 32 32 30 19\n",
            "  9  7  8 32 31  9 35 30 19 17  5 10  9 19 30  5  8 30 35 20  9 30  5 10\n",
            " 17  4 30  5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9rgA3kLW54z"
      },
      "source": [
        "Now we can save our encoded dataset to a file, so we can restore it whenever it is necessary. It is important to note the format of the data that we are saving as we will need to know it when we write the training code. In our case, we will save the dataset as a pickle object, it is the array containing the whole dataset encoded as an integer value for every character."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnj3WdiEWvdV",
        "outputId": "a33ef0c9-0c92-4b83-d1b9-55fad811475b"
      },
      "source": [
        "with open( \"/content/drive/My Drive/NLP Data/text-gen/input_data.plk\", \"wb\") as f:\n",
        "  pickle.dump(train_data, f)\n",
        "\n",
        "print(\"Done\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AnCATVZ9f7e"
      },
      "source": [
        "Lets check our one-hot-encode function that we will use later during the training phase:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_R_O0xW9eCv",
        "outputId": "eaad6147-45b6-4bc5-c251-45509ff7ff0e"
      },
      "source": [
        "print('Encoded characters: ',train_data[100:102])\n",
        "print('One-hot-encoded characters: ',one_hot_encode(train_data[100:102], 28))\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded characters:  [5 8]\n",
            "One-hot-encoded characters:  [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSib-veX9ydJ"
      },
      "source": [
        "### Creating a batch data generator\n",
        "\n",
        "When training on the dataset, we need to extract a batch size examples from the inputs and targets, forward and backward the RNN and then repite the iteration with another batch size examples. A batch generator will help us to extract a batch size examples from our datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKEiWg1d9h0U"
      },
      "source": [
        "def batch_generator_sequence(\n",
        "    features_seq, label_seq, batch_size, seq_len\n",
        "  ):\n",
        "  \"\"\"Generator function that yields batches of data (input and target)\n",
        "\n",
        "  Args:\n",
        "      features_seq: sequence of chracters, feature of our model.\n",
        "      label_seq: sequence of chracters, the target label of our model\n",
        "      batch_size (int): number of examples (in this case, sentences) per batch.\n",
        "      seq_len (int): maximum length of the output tensor.\n",
        "\n",
        "  Yields:\n",
        "      x_epoch: sequence of features for the epoch\n",
        "      y_epoch: sequence of labels for the epoch\n",
        "  \"\"\"\n",
        "  num_batches = len(features_seq) //(batch_size * seq_len)\n",
        "  if num_batches == 0:\n",
        "    raise ValueError(\"No batches created. Use smaller batch size or sequence length.\")\n",
        "\n",
        "  # calculate effective length of text to use\n",
        "  rounded_len = num_batches * batch_size * seq_len\n",
        "  # Reshape the features matrix in batch size x num_batches * seq_len\n",
        "  x = np.reshape(features_seq[: rounded_len], [batch_size, num_batches * seq_len])\n",
        "  # Reshape the target matrix in batch size x num_batches * seq_len\n",
        "  y = np.reshape(label_seq[: rounded_len], [batch_size, num_batches * seq_len])\n",
        "\n",
        "  epoch = 0\n",
        "  while True:\n",
        "    # roll so that no need to reset rnn states over epochs\n",
        "    x_epoch = np.split(np.roll(x, -epoch, axis=0), num_batches, axis=1)\n",
        "    y_epoch = np.split(np.roll(y, -epoch, axis=0), num_batches, axis=1)\n",
        "    for batch in range(num_batches):\n",
        "        yield x_epoch[batch], y_epoch[batch]\n",
        "    epoch += 1\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fW-DXdkb_h0r"
      },
      "source": [
        "### Define RNN model\n",
        "\n",
        "The model is very simple:\n",
        "* An LSTM layer to encode the input (there is no need for an embedding layer because the data is one-hot-encoded)\n",
        "* A dropout layer to reduce overfitting\n",
        "* The decoder, a fully connected layer mapping to a vocabulary size outputs\n",
        "\n",
        "The output provides the probability of every item in the vocabulary to be the next char.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4pXmSgc_yTY"
      },
      "source": [
        "class RNNModel(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_size,\n",
        "               hidden_dim, n_layers, dropout=.2):\n",
        "    super(RNNModel, self).__init__()\n",
        "\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.embedding_size = embedding_size\n",
        "    self.n_layers = n_layers\n",
        "    self.vocab_size = vocab_size\n",
        "    self.dropout = dropout\n",
        "    self.char2int = None\n",
        "    self.int2char = None\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.rnn = nn.LSTM(embedding_size, hidden_dim, \n",
        "                        n_layers, dropout=dropout, batch_first = True)\n",
        "    self.decoder = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "  def forward(self, x, state):\n",
        "    # input shape: [batch_size, seq_len, embedding_size]\n",
        "    out, state = self.rnn(x, state)\n",
        "    # out shape: [batch_size, seq_len, rnn_size]\n",
        "    # hidden shape: [2, num_layers, batch_size, rnn_size]\n",
        "    out = self.dropout(out)\n",
        "    out = out.contiguous().view(-1, self.hidden_dim)\n",
        "\n",
        "    logits = self.decoder(out)\n",
        "    # output shape: [seq_len * batch_size, vocab_size]\n",
        "    return logits, state\n",
        "\n",
        "  def init_state(self, device, batch_size=1):\n",
        "    \"\"\"\n",
        "    initialises rnn states.\n",
        "    \"\"\"\n",
        "    return (torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device),\n",
        "                torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device))\n",
        "    \n",
        "  def predict(self, input):\n",
        "    # input shape: [seq_len, batch_size]\n",
        "    logits, hidden = self.forward(input)\n",
        "    # logits shape: [seq_len * batch_size, vocab_size]\n",
        "    # hidden shape: [2, num_layers, batch_size, rnn_size]\n",
        "    probs = F.softmax(logits)\n",
        "    # shape: [seq_len * batch_size, vocab_size]\n",
        "    probs = probs.view(input.size(0), input.size(1), probs.size(1))\n",
        "    # output shape: [seq_len, batch_size, vocab_size]\n",
        "    return probs, hidden\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCniV2XDBcHp"
      },
      "source": [
        "### Training the model\n",
        "During training:\n",
        "* In every epoch get the next batch data, move the tensors to the device, call the model (Forward pass), calculate the loss function, get the gradients and update the weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0NEtAfuB_Vi"
      },
      "source": [
        "from tqdm import tqdm"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRmbuKBbBah3"
      },
      "source": [
        "def train_main(model, optimizer, loss_fn,\n",
        "               batch_data, num_batches, val_batches, \n",
        "               batch_size, seq_len, n_epochs,\n",
        "               clip_norm, device\n",
        "  ):\n",
        "  # Training Run\n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "    # Store the loss in every batch iteration\n",
        "    #epoch_losses = torch.Tensor(num_batches)\n",
        "    epoch_losses = []\n",
        "    # Init the hidden state\n",
        "    hidden = model.init_state(device, batch_size)\n",
        "    # Train all the batches in every epoch\n",
        "    print(\"Epoch {}/{}\".format(epoch, n_epochs+1))\n",
        "    for i in range(num_batches-val_batches):\n",
        "      # Get the next batch data for input and target\n",
        "      input_batch, target_batch = next(batch_data)\n",
        "      # Onr hot encode the input data\n",
        "      input_batch = one_hot_encode(input_batch, model.vocab_size)\n",
        "      # Tranform to tensor\n",
        "      input_data = torch.from_numpy(input_batch)\n",
        "      target_data = torch.from_numpy(target_batch)\n",
        "      # Create a new variable for the hidden state, necessary to calculate the gradients\n",
        "      hidden = tuple(([Variable(var.data) for var in hidden]))\n",
        "      # Move the input data to the device\n",
        "      input_data = input_data.to(device)\n",
        "      #print('Input shape: ', input_data.shape)\n",
        "      #print('Hidden shape: ', hidden[0].shape, hidden[1].shape)\n",
        "      # Set the model to train and prepare the gradients\n",
        "      model.train()\n",
        "      optimizer.zero_grad() # Clears existing gradients from previous epoch\n",
        "      # Pass Fordward the RNN\n",
        "      output, hidden = model(input_data, hidden)\n",
        "      #print('Output shape: ', output.shape)\n",
        "      output = output.to(device)\n",
        "      #print('Output shape: ', output.shape)\n",
        "      #print('Target shape; ', target_data.shape)\n",
        "      # Move the target data to the device\n",
        "      target_data = target_data.to(device)\n",
        "      #print('Target shape; ', target_data.shape)\n",
        "      target_data = torch.reshape(target_data, (batch_size*seq_len,))\n",
        "      #print('Target shape; ', target_data.shape)\n",
        "      loss = loss_fn(output, target_data.view(batch_size*seq_len))\n",
        "      #print(loss)\n",
        "      # Save the loss\n",
        "      #epoch_losses[i] = loss.item() #data[0]\n",
        "      epoch_losses.append(loss.item()) #data[0]\n",
        "  \n",
        "      loss.backward() # Does backpropagation and calculates gradients\n",
        "      # clip gradient norm\n",
        "      nn.utils.clip_grad_norm_(model.parameters(), clip_norm)\n",
        "      \n",
        "      optimizer.step() # Updates the weights accordingly\n",
        "\n",
        "    # Now, when epoch is finished, evaluate the model on validation data\n",
        "    model.eval()\n",
        "    val_hidden = model.init_state(device, batch_size)\n",
        "    val_losses = []\n",
        "    print(\"Val Epoch {}/{}\".format(epoch, n_epochs+1))\n",
        "    for i in range(val_batches):\n",
        "      # Get the next batch data for input and target\n",
        "      input_batch, target_batch = next(batch_data)\n",
        "      # Onr hot encode the input data\n",
        "      input_batch = one_hot_encode(input_batch, model.vocab_size)\n",
        "      # Tranform to tensor\n",
        "      input_data = torch.from_numpy(input_batch)\n",
        "      target_data = torch.from_numpy(target_batch)\n",
        "      # Create a new variable for the hidden state, necessary to calculate the gradients\n",
        "      hidden = tuple(([Variable(var.data) for var in val_hidden]))\n",
        "      # Move the input data to the device\n",
        "      input_data = input_data.to(device)\n",
        "      # Pass Fordward the RNN\n",
        "      output, hidden = model(input_data, hidden)\n",
        "      #print('Output shape: ', output.shape)\n",
        "      output = output.to(device)\n",
        "      #print('Output shape: ', output.shape)\n",
        "      #print('Target shape; ', target_data.shape)\n",
        "      # Move the target data to the device\n",
        "      target_data = target_data.to(device)\n",
        "      #print('Target shape; ', target_data.shape)\n",
        "      target_data = torch.reshape(target_data, (batch_size*seq_len,))\n",
        "      #print('Target shape; ', target_data.shape)\n",
        "      loss = loss_fn(output, target_data.view(batch_size*seq_len))\n",
        "      #print(loss)\n",
        "      # Save the loss\n",
        "      val_losses.append(loss.item()) #data[0]\n",
        "\n",
        "    model.train()                  \n",
        "    print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
        "    print(\"Train Loss: {:.4f}\".format(np.mean(epoch_losses)), end=' ')\n",
        "    print(\"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n",
        "    print()\n",
        "\n",
        "    if epoch != n_epochs:\n",
        "      print(\"> Next epoch\")\n",
        "  return epoch_losses\n"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8kBR9LdCu8C"
      },
      "source": [
        "After defining the model above, we'll have to instantiate the model with the relevant parameters and define our hyperparamters as well. The hyperparameters we're defining below are:\n",
        "\n",
        "* n_epochs: Number of Epochs --> This refers to the number of times our model will go through the entire training dataset\n",
        "* lr: Learning Rate --> This affects the rate at which our model updates the weights in the cells each time backpropogation is done\n",
        "  * A smaller learning rate means that the model changes the values of the weight with a smaller magnitude\n",
        "  * A larger learning rate means that the weights are updated to a larger extent for each time step\n",
        "* batch_size: Number of examples to train on every train step\n",
        "maxlen: Length of the input sequence of char\n",
        "* embedding_size: the vocab size because the input feature is one-hot-encoded\n",
        "* hidden_dim: the number of hidden units in our LSTM module\n",
        "* n_layers: number of layers of our LSTM module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WWJdPIqC7U0"
      },
      "source": [
        "# Define hyperparameters for training\n",
        "n_epochs = 5\n",
        "lr=0.01\n",
        "batch_size=64\n",
        "maxlen=64\n",
        "clip_norm=5\n",
        "val_fraction = 0.1\n",
        "\n",
        "# Define hypeparameters of the model\n",
        "hidden_dim = 64 #64\n",
        "n_layers = 1\n",
        "embedding_size=len(vocab.char2int)\n",
        "dict_size = len(vocab.char2int)\n",
        "drop_rate = 0.2"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hmAgnH2DDyi"
      },
      "source": [
        "Optimizer and loss functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXTry7D5DB8Y",
        "outputId": "4cd4c69c-264f-4256-a09a-f0c302b5bad8"
      },
      "source": [
        "model = RNNModel(dict_size,embedding_size, hidden_dim, n_layers)\n",
        "model"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNNModel(\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              "  (rnn): LSTM(38, 64, batch_first=True, dropout=0.2)\n",
              "  (decoder): Linear(in_features=64, out_features=38, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A92hN22bDMqy"
      },
      "source": [
        "Counting model parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvRM3LObDJwM",
        "outputId": "b34f53e6-5438-4a95-c149-ba3f1676b86c"
      },
      "source": [
        "def count_trainable_params(model):\n",
        "  n_t_params =sum(p.numel() for p in model.parameters() if p.requires_grad == True)\n",
        "  return n_t_params\n",
        "\n",
        "print(f'The model has {count_trainable_params(model):,} trainable parameters')\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 29,094 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8DVZ-oVDT7_",
        "outputId": "2727b3a1-6563-4b75-fe15-f171f62e0c9d"
      },
      "source": [
        "model = model.to(device)\n",
        "print(model)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNNModel(\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (rnn): LSTM(38, 64, batch_first=True, dropout=0.2)\n",
            "  (decoder): Linear(in_features=64, out_features=38, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKJ2hBVhDaHX"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63B4UidrDdUi",
        "outputId": "eef042ce-2181-453e-a6f5-c9286980eb2f"
      },
      "source": [
        "\n",
        "# Calculate the number of batches to train\n",
        "num_batches = len(input_seq) // (batch_size*maxlen)\n",
        "val_batches = int(num_batches*val_fraction)\n",
        "\n",
        "# Create the batch data generator\n",
        "batch_data = batch_generator_sequence(input_seq, target_seq, batch_size, maxlen)\n",
        "losses = train_main(model, optimizer, criterion, batch_data, num_batches, val_batches, batch_size, \n",
        "                    maxlen, n_epochs, clip_norm, device)\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6\n",
            "Val Epoch 1/6\n",
            "Epoch: 1/5............. Train Loss: 1.7377 Val Loss: 1.6951\n",
            "\n",
            "> Next epoch\n",
            "Epoch 2/6\n",
            "Val Epoch 2/6\n",
            "Epoch: 2/5............. Train Loss: 1.7166 Val Loss: 1.6787\n",
            "\n",
            "> Next epoch\n",
            "Epoch 3/6\n",
            "Val Epoch 3/6\n",
            "Epoch: 3/5............. Train Loss: 1.6984 Val Loss: 1.6667\n",
            "\n",
            "> Next epoch\n",
            "Epoch 4/6\n",
            "Val Epoch 4/6\n",
            "Epoch: 4/5............. Train Loss: 1.6862 Val Loss: 1.6552\n",
            "\n",
            "> Next epoch\n",
            "Epoch 5/6\n",
            "Val Epoch 5/6\n",
            "Epoch: 5/5............. Train Loss: 1.6749 Val Loss: 1.6461\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q63o5BtfF206"
      },
      "source": [
        "### Predicting the input sequences\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qC4n208pFY1i"
      },
      "source": [
        "def sample_from_probs(probs, top_n=10):\n",
        "    \"\"\"\n",
        "    truncated weighted random choice.\n",
        "    \"\"\"\n",
        "    _, indices = torch.sort(probs)\n",
        "    # set probabilities after top_n to 0\n",
        "    probs[indices.data[:-top_n]] = 0\n",
        "    #print(probs.shape)\n",
        "    sampled_index = torch.multinomial(probs, 1)\n",
        "    return sampled_index\n",
        "\n",
        "def predict_probs(model, hidden, character, vocab):\n",
        "    # One-hot encoding our input to fit into the model\n",
        "    character = np.array([[vocab.char2int[c] for c in character]])\n",
        "    #character = one_hot_encode(character, len(vocab.char2int), character.shape[1], 1)\n",
        "    character = one_hot_encode(character, model.vocab_size)\n",
        "    character = torch.from_numpy(character)\n",
        "    character = character.to(device)\n",
        "    \n",
        "    out, hidden = model(character, hidden)\n",
        "\n",
        "    prob = nn.functional.softmax(out[-1], dim=0).data\n",
        "\n",
        "    return prob, hidden"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4hRRXzAGAJb"
      },
      "source": [
        "Letâ€™s test our model now and see what kind of output we will get."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJ9ac190F-bP"
      },
      "source": [
        "def generate_from_text(model, out_len, vocab, top_n=1, start='hey'):\n",
        "    model.eval() # eval mode\n",
        "    start = start.lower()\n",
        "    # First off, run through the starting characters\n",
        "    chars = [ch for ch in start]\n",
        "    size = out_len - len(chars)\n",
        "    # Generate the initial hidden state\n",
        "    state = model.init_state(device, 1)\n",
        "    \n",
        "    # Warm up the initial state, predicting on the initial string\n",
        "    for ch in chars:\n",
        "        #char, state = predict(model, ch, state, top_n=top_k)\n",
        "        probs, state = predict_probs(model, state, ch, vocab)\n",
        "        next_index = sample_from_probs(probs, top_n)\n",
        "\n",
        "    # Now pass in the previous characters and get a new one\n",
        "    for ii in range(size):\n",
        "        #char, h = predict_char(model, chars, vocab)\n",
        "        probs, state = predict_probs(model, state, chars, vocab)\n",
        "        next_index = sample_from_probs(probs, top_n)\n",
        "        # append to sequence\n",
        "        chars.append(vocab.int2char[next_index.data[0]])\n",
        "\n",
        "    return ''.join(chars)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZHKf91kGDpH",
        "outputId": "e5e5509c-428e-4dd6-8868-7964a8e40b5e"
      },
      "source": [
        "text_predicted = generate_from_text(model, 30, vocab, 3, 'we want ')\n",
        "print(text_predicted)\n",
        "print(len(text_predicted))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "we want and the these she thee\n",
            "30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWauFJSmGH4A"
      },
      "source": [
        "\n",
        "def generate_from_char(model, out_len, vocab, top_n=1, start='hey'):\n",
        "    model.eval() # eval mode\n",
        "    start = start.lower()\n",
        "    # First off, run through the starting characters\n",
        "    chars = [ch for ch in start]\n",
        "    size = out_len - len(chars)\n",
        "    # Generate the initial hidden state\n",
        "    state = model.init_state(device, 1)\n",
        "    # Warm up the initial state, predicting on the initial string\n",
        "    for ch in chars:\n",
        "        #char, state = predict(model, ch, state, top_n=top_k)\n",
        "        probs, state = predict_probs(model, state, ch, vocab)\n",
        "        next_index = sample_from_probs(probs, top_n)\n",
        "        \n",
        "    # Include the last char predicted to the predicted output\n",
        "    chars.append(vocab.int2char[next_index.data[0]])   \n",
        "    \n",
        "    # Now pass in the previous characters and get a new one\n",
        "    for ii in range(size-1):\n",
        "        #char, h = predict_char(model, chars, vocab)\n",
        "        probs, state = predict_probs(model, state, chars[-1], vocab)\n",
        "        next_index = sample_from_probs(probs, top_n)\n",
        "        # append to sequence\n",
        "        chars.append(vocab.int2char[next_index.data[0]])\n",
        "\n",
        "    return ''.join(chars)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRUbQw2wGRtL",
        "outputId": "b71cd00c-01d0-4499-c345-5aa63eb9b706"
      },
      "source": [
        "text_predicted = generate_from_char(model, 30, vocab, 3, 'we want ')\n",
        "print(text_predicted)\n",
        "print(len(text_predicted))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "we want and strike and this so\n",
            "30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMXJT4qsGZIL"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "In the next Notebook we will try to improve the model performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbYUF4JnGTHk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}