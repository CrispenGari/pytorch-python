{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "09_Custom_dataset_for_matchine_translation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpmbY84-byaj"
      },
      "source": [
        "### Custom dataset.\n",
        "In this notebook we are going to leran how we can load our own custom dataset from files. The dataset that i am using was found on [this site](http://www.statmt.org/europarl/).\n",
        "\n",
        "First we will define the path where our files are located as the base_path. In my case i am using google drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GYW9G70aaqI"
      },
      "source": [
        "base_path = '/content/drive/MyDrive/NLP Data/seq2seq/fr-eng'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivn_JM5meyD4"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WycTq-tdbX70"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "from torchtext.legacy import data, datasets\n",
        "import json\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cX5wZXj7fgfh"
      },
      "source": [
        "We have two text files for the french and english sentences with the following file names:\n",
        "\n",
        "```py\n",
        "fr = \"europarl-v7.fr-en.fr\"\n",
        "en = \"europarl-v7.fr-en.en\"\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjdnXgN_bX5L"
      },
      "source": [
        "fr_path = \"europarl-v7.fr-en.fr\"\n",
        "en_path = \"europarl-v7.fr-en.en\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqX61y1EgLZ3"
      },
      "source": [
        "Now let's load the text into list of strings. We are going to use the new line as the surperator of each sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJmValxHbX2c"
      },
      "source": [
        "eng_sentences = open(os.path.join(base_path, en_path), encoding='utf8').read().split('\\n')\n",
        "fr_sentences = open(os.path.join(base_path, fr_path), encoding='utf8').read().split('\\n')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7qPDEPehCBv"
      },
      "source": [
        "### Next we will check how many examples do we have for each language."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJKF9ufmbXz1",
        "outputId": "f864551e-26bc-4053-f632-a6988e2e9b5a"
      },
      "source": [
        "print(\"eng: \", len(eng_sentences))\n",
        "print(\"fr: \", len(fr_sentences))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "eng:  2007724\n",
            "fr:  2007724\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "optPrKTPholf"
      },
      "source": [
        "### Creating a pandas dataframe\n",
        "Creatting the pd dataframe will help us to split the sets into train and test and the convert the splitted dataframes into either `.json` or `.csv` files which are the formats that are accepted by the `torchtext`. To make this very simple Im going to use only `500` sentence french to english pairs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NR4Vy-G7hTVG"
      },
      "source": [
        "size = 500\n",
        "raw_data ={\n",
        "    'eng': [sent for sent in eng_sentences[:size]],\n",
        "    'fr': [sent for sent in fr_sentences[:size]],\n",
        "}\n",
        "\n",
        "dataframe = pd.DataFrame(raw_data, columns=['eng', 'fr'])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9IAl4YGi0Su"
      },
      "source": [
        "### Checking our dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "OZnV-1-EhTSN",
        "outputId": "de2887ee-b644-4a6c-d9f0-5774dd6b838d"
      },
      "source": [
        "dataframe.head(4)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>eng</th>\n",
              "      <th>fr</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Resumption of the session</td>\n",
              "      <td>Reprise de la session</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I declare resumed the session of the European ...</td>\n",
              "      <td>Je déclare reprise la session du Parlement eur...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Although, as you will have seen, the dreaded '...</td>\n",
              "      <td>Comme vous avez pu le constater, le grand \"bog...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>You have requested a debate on this subject in...</td>\n",
              "      <td>Vous avez souhaité un débat à ce sujet dans le...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 eng                                                 fr\n",
              "0                          Resumption of the session                              Reprise de la session\n",
              "1  I declare resumed the session of the European ...  Je déclare reprise la session du Parlement eur...\n",
              "2  Although, as you will have seen, the dreaded '...  Comme vous avez pu le constater, le grand \"bog...\n",
              "3  You have requested a debate on this subject in...  Vous avez souhaité un débat à ce sujet dans le..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6kGQsKdjApO"
      },
      "source": [
        "### Spliting the datasets.\n",
        "We are going to use `sklearn` `train_test_split` to split these two datasets for the train and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LoplwpwhTO_",
        "outputId": "dfbd29a2-755a-4996-a106-a2b67cd2a946"
      },
      "source": [
        "train, val = train_test_split(dataframe, test_size=.2)\n",
        "len(train), len(val)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(400, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qa1HDtBPjanz"
      },
      "source": [
        "### Creating json files.\n",
        "\n",
        "We are going to create `json` files and save them to the `base_path` for these two sets. We will be using the `.to_json()` method to do this. \n",
        "\n",
        "**Note** you can also use the `.to_csv()` to create `csv` files for example:\n",
        "\n",
        "```py\n",
        "train.to_csv(\"train.csv\", index=False)\n",
        "val.to_csv(\"val.csv\", index=False)\n",
        "```\n",
        "\n",
        "**Note**: When you are using `.to_json()` we should pass the arg `orient=\"records\"` so that these json files will be the files that can be accepted by the `torchtext`. Basically what this is doing is to add json files as records by removing the list `[]` brakets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTUDZ5hQhTMM"
      },
      "source": [
        "train.to_json(os.path.join(base_path, 'train.json'), orient=\"records\", lines=True)\n",
        "val.to_json(os.path.join(base_path, 'val.json'), orient=\"records\", lines=True)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5eIM7Y9lGdm"
      },
      "source": [
        "Now each record has the following format:\n",
        "\n",
        "```json\n",
        "{\"eng\":\"For us new members, it was the first time, and this was a very interesting process.\",\"fr\":\"C' \\u00e9tait pour nous, nouveaux d\\u00e9put\\u00e9s, la premi\\u00e8re fois, et c' est un processus extr\\u00eamement int\\u00e9ressant.\"}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-bP32VSlaT8"
      },
      "source": [
        "### Let's load the tokenizer models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gq15NQqxleEN",
        "outputId": "9195617a-584a-4e4b-f1b3-b491ee86d8ce"
      },
      "source": [
        "import spacy\n",
        "import spacy.cli\n",
        "spacy.cli.download('fr_core_news_sm')\n",
        "import fr_core_news_sm, en_core_web_sm\n",
        "spacy_fr = spacy.load('fr_core_news_sm')\n",
        "spacy_en = spacy.load('en_core_web_sm')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('fr_core_news_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwYGijsrliZ8"
      },
      "source": [
        "def tokenize_fr(sent):\n",
        "  sent = sent.lower()\n",
        "  return [tok for tok in spacy_fr.tokenizer(sent)]\n",
        "\n",
        "def tokenize_en(sent):\n",
        "  sent = sent.lower()\n",
        "  return [tok for tok in spacy_en.tokenizer(sent)]"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gw3_3KALlSV2"
      },
      "source": [
        "### Creating fields"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9or7oqxmLR8"
      },
      "source": [
        "SRC = data.Field(\n",
        "    tokenize = tokenize_fr,\n",
        "    init_token = \"<sos>\",\n",
        "    eos_token = \"<eos>\"\n",
        ")\n",
        "TRG = data.Field(\n",
        "    tokenize = tokenize_en,\n",
        "    init_token = \"<sos>\",\n",
        "    eos_token = \"<eos>\"\n",
        ")"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acE5YxbVhTHN"
      },
      "source": [
        "fields ={\n",
        "    \"fr\": (\"src\", SRC),\n",
        "    \"eng\": (\"trg\", TRG)\n",
        "}"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtJ4lne2mavw"
      },
      "source": [
        "### We are now ready to create our dataset.\n",
        "\n",
        "We are going to use the `TabularDataset.splits()` method to create the train and validation datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILtEfo5DhTEW"
      },
      "source": [
        "train_data, val_data = data.TabularDataset.splits(\n",
        "  base_path,\n",
        "  format=\"json\",\n",
        "  train=\"train.json\",\n",
        "  validation= 'val.json',\n",
        "  fields=fields\n",
        ")"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaydvzkiqmLK",
        "outputId": "7d9384bb-a88b-4225-f225-3f9dad55251a"
      },
      "source": [
        "print(vars(train_data.examples[0]))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'src': [c, ', était, pour, nous, ,, nouveaux, députés, ,, la, première, fois, ,, et, c, ', est, un, processus, extrêmement, intéressant, .], 'trg': [for, us, new, members, ,, it, was, the, first, time, ,, and, this, was, a, very, interesting, process, .]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lt9I9Bi0oQ1s"
      },
      "source": [
        "### Building the vocabulary\n",
        "Now we are ready to build the vocabulary.\n",
        "\n",
        "**Note** In this simple example we will build the vocab on both sets. It is recomended that _when building the vocabulary we only need to build it on the train set_.\n",
        "\n",
        "We will be building the vocab as follows without `min_freq=2` args since our dataset is small:\n",
        "\n",
        "**Note**: The `min_freq=2` allows us to set the minimum frequency of each word meaning a word that appears less than two times will be converted to `<unk>` token.\n",
        "\n",
        "```py\n",
        "SRC.build_vocab(train_data, val_data, max_size=1000)\n",
        "TRG.build_vocab(train_data, val_data, max_size=1000)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1YbKCu7hTBX"
      },
      "source": [
        "SRC.build_vocab(train_data, val_data, max_size=1000)\n",
        "TRG.build_vocab(train_data, val_data, max_size=1000)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3IeZ1zHp6zV",
        "outputId": "3c6cdd9d-bcb4-4a03-9afa-18dc17bfd096"
      },
      "source": [
        "TRG.vocab.itos[11]"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "all"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTOUH9P8pJM4"
      },
      "source": [
        "### Creating iterators\n",
        "\n",
        " Now you can create iterators and then load the iterators to the models. Again we are going to use the `BucketIterator`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0R4uWb0BpToi"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "BATCH_SIZE = 128"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8-Pweimphy7"
      },
      "source": [
        "train_iter, val_iter = data.BucketIterator.splits(\n",
        "    (train_data, val_data),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    device=device,\n",
        "    sort_key=lambda x: len(x.src)\n",
        ")"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G69pFtnmsOWI"
      },
      "source": [
        "### Checking the a single batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zws7QXJ5sSO0",
        "outputId": "fb098d0a-9f03-47c2-c4c5-7af1fff8b3fe"
      },
      "source": [
        "batch = next(iter(train_iter))\n",
        "batch.src"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  2,   2,   2,  ...,   2,   2,   2],\n",
              "        [136,  97, 244,  ..., 127, 116, 355],\n",
              "        [625, 607,   0,  ...,   0, 846, 709],\n",
              "        ...,\n",
              "        [  1,   1,   1,  ...,   1,   1,   1],\n",
              "        [  1,   1,   1,  ...,   1,   1,   1],\n",
              "        [  1,   1,   1,  ...,   1,   1,   1]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFU9XpizbYOk"
      },
      "source": [
        "### Resources used.\n",
        "\n",
        "1. [This Blog Post](https://towardsdatascience.com/how-to-use-torchtext-for-neural-machine-translation-plus-hack-to-make-it-5x-faster-77f3884d95)\n",
        "2. [Datasets List](http://www.statmt.org/europarl/)\n",
        "3. [Alen Nie](https://anie.me/On-Torchtext/)\n",
        "\n",
        "### Extra resources\n",
        "1 [Harvard](http://nlp.seas.harvard.edu/2018/04/03/attention.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nojIRpu9bw3J"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}