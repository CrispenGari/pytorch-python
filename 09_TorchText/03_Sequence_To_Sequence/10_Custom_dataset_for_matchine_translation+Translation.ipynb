{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "09_Custom_dataset_for_matchine_translation+Translation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpmbY84-byaj"
      },
      "source": [
        "### Custom dataset + Translation.\n",
        "\n",
        "Based on our previous notebook and othor previous notebooks we are going to try and use our custom dataset to create and train a translation model that translate text from french to english.\n",
        "\n",
        "### Path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GYW9G70aaqI"
      },
      "source": [
        "base_path = '/content/drive/MyDrive/NLP Data/seq2seq/fr-eng'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivn_JM5meyD4"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WycTq-tdbX70"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "from torchtext.legacy import data, datasets\n",
        "import json\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cX5wZXj7fgfh"
      },
      "source": [
        "We have two text files for the french and english sentences with the following file names:\n",
        "\n",
        "```py\n",
        "fr = \"europarl-v7.fr-en.fr\"\n",
        "en = \"europarl-v7.fr-en.en\"\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjdnXgN_bX5L"
      },
      "source": [
        "fr_path = \"europarl-v7.fr-en.fr\"\n",
        "en_path = \"europarl-v7.fr-en.en\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqX61y1EgLZ3"
      },
      "source": [
        "Now let's load the text into list of strings. We are going to use the new line as the surperator of each sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJmValxHbX2c"
      },
      "source": [
        "eng_sentences = open(os.path.join(base_path, en_path), encoding='utf8').read().split('\\n')\n",
        "fr_sentences = open(os.path.join(base_path, fr_path), encoding='utf8').read().split('\\n')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7qPDEPehCBv"
      },
      "source": [
        "### Next we will check how many examples do we have for each language."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJKF9ufmbXz1",
        "outputId": "6c277bef-2835-4335-a0f4-8bc9ba4c249e"
      },
      "source": [
        "print(\"eng: \", len(eng_sentences))\n",
        "print(\"fr: \", len(fr_sentences))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "eng:  2007724\n",
            "fr:  2007724\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "optPrKTPholf"
      },
      "source": [
        "### Creating a pandas dataframe\n",
        "Creatting the pd dataframe will help us to split the sets into train and test and the convert the splitted dataframes into either `.json` or `.csv` files which are the formats that are accepted by the `torchtext`. To make this very simple Im going to use only `500` sentence french to english pairs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NR4Vy-G7hTVG"
      },
      "source": [
        "size = 500\n",
        "raw_data ={\n",
        "    'eng': [sent for sent in eng_sentences[:size]],\n",
        "    'fr': [sent for sent in fr_sentences[:size]],\n",
        "}\n",
        "\n",
        "dataframe = pd.DataFrame(raw_data, columns=['eng', 'fr'])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9IAl4YGi0Su"
      },
      "source": [
        "### Checking our dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "OZnV-1-EhTSN",
        "outputId": "c75ff69e-216c-4654-ee43-8f6085efedf4"
      },
      "source": [
        "dataframe.head(4)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>eng</th>\n",
              "      <th>fr</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Resumption of the session</td>\n",
              "      <td>Reprise de la session</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I declare resumed the session of the European ...</td>\n",
              "      <td>Je déclare reprise la session du Parlement eur...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Although, as you will have seen, the dreaded '...</td>\n",
              "      <td>Comme vous avez pu le constater, le grand \"bog...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>You have requested a debate on this subject in...</td>\n",
              "      <td>Vous avez souhaité un débat à ce sujet dans le...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 eng                                                 fr\n",
              "0                          Resumption of the session                              Reprise de la session\n",
              "1  I declare resumed the session of the European ...  Je déclare reprise la session du Parlement eur...\n",
              "2  Although, as you will have seen, the dreaded '...  Comme vous avez pu le constater, le grand \"bog...\n",
              "3  You have requested a debate on this subject in...  Vous avez souhaité un débat à ce sujet dans le..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6kGQsKdjApO"
      },
      "source": [
        "### Spliting the datasets.\n",
        "We are going to use `sklearn` `train_test_split` to split these two datasets for the train and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LoplwpwhTO_",
        "outputId": "b9290b02-8a4b-4ae9-d9c2-a78304b95cd6"
      },
      "source": [
        "train, val = train_test_split(dataframe, test_size=.005)\n",
        "train, test = train_test_split(train, test_size=.005)\n",
        "len(train), len(val), len(test)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(494, 3, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qa1HDtBPjanz"
      },
      "source": [
        "### Creating json files.\n",
        "\n",
        "We are going to create `json` files and save them to the `base_path` for these two sets. We will be using the `.to_json()` method to do this. \n",
        "\n",
        "**Note** you can also use the `.to_csv()` to create `csv` files for example:\n",
        "\n",
        "```py\n",
        "train.to_csv(\"train.csv\", index=False)\n",
        "val.to_csv(\"val.csv\", index=False)\n",
        "```\n",
        "\n",
        "**Note**: When you are using `.to_json()` we should pass the arg `orient=\"records\"` so that these json files will be the files that can be accepted by the `torchtext`. Basically what this is doing is to add json files as records by removing the list `[]` brakets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTUDZ5hQhTMM"
      },
      "source": [
        "train.to_json(os.path.join(base_path, 'train.json'), orient=\"records\", lines=True)\n",
        "val.to_json(os.path.join(base_path, 'val.json'), orient=\"records\", lines=True)\n",
        "test.to_json(os.path.join(base_path, 'test.json'), orient=\"records\", lines=True)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5eIM7Y9lGdm"
      },
      "source": [
        "Now each record has the following format:\n",
        "\n",
        "```json\n",
        "{\"eng\":\"For us new members, it was the first time, and this was a very interesting process.\",\"fr\":\"C' \\u00e9tait pour nous, nouveaux d\\u00e9put\\u00e9s, la premi\\u00e8re fois, et c' est un processus extr\\u00eamement int\\u00e9ressant.\"}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-bP32VSlaT8"
      },
      "source": [
        "### Let's load the tokenizer models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gq15NQqxleEN",
        "outputId": "a50052e2-a156-4a1d-9188-4496b0741b2a"
      },
      "source": [
        "import spacy\n",
        "import spacy.cli\n",
        "spacy.cli.download('fr_core_news_sm')\n",
        "import fr_core_news_sm, en_core_web_sm\n",
        "spacy_fr = spacy.load('fr_core_news_sm')\n",
        "spacy_en = spacy.load('en_core_web_sm')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('fr_core_news_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwYGijsrliZ8"
      },
      "source": [
        "def tokenize_fr(sent):\n",
        "  sent = sent.lower()\n",
        "  return [tok for tok in spacy_fr.tokenizer(sent)]\n",
        "\n",
        "def tokenize_en(sent):\n",
        "  sent = sent.lower()\n",
        "  return [tok for tok in spacy_en.tokenizer(sent)]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gw3_3KALlSV2"
      },
      "source": [
        "### Creating fields"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9or7oqxmLR8"
      },
      "source": [
        "SRC = data.Field(\n",
        "    tokenize = tokenize_fr,\n",
        "    init_token = \"<sos>\",\n",
        "     eos_token = \"<eos>\",\n",
        "     include_lengths =True\n",
        ")\n",
        "TRG = data.Field(\n",
        "    tokenize = tokenize_en,\n",
        "    init_token = \"<sos>\",\n",
        "     eos_token = \"<eos>\"\n",
        ")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acE5YxbVhTHN"
      },
      "source": [
        "fields ={\n",
        "    \"fr\": (\"src\", SRC),\n",
        "    \"eng\": (\"trg\", TRG)\n",
        "}"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtJ4lne2mavw"
      },
      "source": [
        "### We are now ready to create our dataset.\n",
        "\n",
        "We are going to use the `TabularDataset.splits()` method to create the train and validation datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILtEfo5DhTEW"
      },
      "source": [
        "train_data, test_data, val_data = data.TabularDataset.splits(\n",
        "  base_path,\n",
        "  format=\"json\",\n",
        "  train=\"train.json\",\n",
        "  test=\"test.json\",\n",
        "  validation= 'val.json',\n",
        "  fields=fields\n",
        ")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaydvzkiqmLK",
        "outputId": "d29f0064-a9c5-478c-a2bf-3fff2756e86e"
      },
      "source": [
        "print(vars(train_data.examples[0]))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'src': [je, propose, que, nous, votions, sur, la, demande, du, groupe, socialiste, visant, à, réinscrire, la, déclaration, de, la, commission, sur, ses, objectifs, stratégiques, .], 'trg': [i, propose, that, we, vote, on, the, request, of, the, group, of, the, party, of, european, socialists, that, the, commission, statement, on, its, strategic, objectives, should, be, reinstated, .]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lt9I9Bi0oQ1s"
      },
      "source": [
        "### Building the vocabulary\n",
        "Now we are ready to build the vocabulary.\n",
        "\n",
        "**Note** In this simple example we will build the vocab on both sets. It is recomended that _when building the vocabulary we only need to build it on the train set_.\n",
        "\n",
        "We will be building the vocab as follows without `min_freq=2` args since our dataset is small:\n",
        "\n",
        "**Note**: The `min_freq=2` allows us to set the minimum frequency of each word meaning a word that appears less than two times will be converted to `<unk>` token.\n",
        "\n",
        "```py\n",
        "SRC.build_vocab(train_data, val_data, max_size=1000)\n",
        "TRG.build_vocab(train_data, val_data, max_size=1000)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1YbKCu7hTBX"
      },
      "source": [
        "max_size = 25000\n",
        "\n",
        "SRC.build_vocab(train_data, max_size=max_size)\n",
        "TRG.build_vocab(train_data, max_size=max_size)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3IeZ1zHp6zV",
        "outputId": "176d62e5-e7d6-44b6-af72-e14379ea3e5b"
      },
      "source": [
        "len(SRC.vocab.itos)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16321"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTOUH9P8pJM4"
      },
      "source": [
        "### Creating iterators\n",
        "\n",
        " Now you can create iterators and then load the iterators to the models. Again we are going to use the `BucketIterator`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0R4uWb0BpToi"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "BATCH_SIZE = 1"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8-Pweimphy7"
      },
      "source": [
        "train_iter,val_iter, test_iter = data.BucketIterator.splits(\n",
        "    (train_data, val_data, test_data),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    device=device,\n",
        "    sort_key=lambda x: len(x.src)\n",
        ")"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G69pFtnmsOWI"
      },
      "source": [
        "### Checking the a single batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zws7QXJ5sSO0",
        "outputId": "ab8d33a1-a5fe-4415-f22f-68175e499d33"
      },
      "source": [
        "batch = next(iter(train_iter))\n",
        "batch.src"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[    2],\n",
              "         [  273],\n",
              "         [  669],\n",
              "         [  877],\n",
              "         [ 1194],\n",
              "         [ 1657],\n",
              "         [ 2302],\n",
              "         [ 3472],\n",
              "         [ 3743],\n",
              "         [ 4236],\n",
              "         [ 4654],\n",
              "         [ 4986],\n",
              "         [ 5225],\n",
              "         [ 6099],\n",
              "         [ 6721],\n",
              "         [ 7004],\n",
              "         [ 7693],\n",
              "         [ 7909],\n",
              "         [ 8246],\n",
              "         [ 8495],\n",
              "         [ 8819],\n",
              "         [ 9418],\n",
              "         [ 9592],\n",
              "         [10032],\n",
              "         [10685],\n",
              "         [10830],\n",
              "         [10926],\n",
              "         [11360],\n",
              "         [11537],\n",
              "         [11856],\n",
              "         [    3]], device='cuda:0'), tensor([31], device='cuda:0'))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPCJQsob7ND3"
      },
      "source": [
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import random"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MA3UWSVqjsYC"
      },
      "source": [
        "### Encoder, Attention, Decoder and `Seq2Seq`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2_bt28R6IO1"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.embedding = nn.Embedding(input_dim, embedding_dim=emb_dim)\n",
        "    self.gru = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
        "    self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, src, src_len):\n",
        "\n",
        "    embedded = self.dropout(self.embedding(src)) # embedded = [src len, batch size, emb dim]\n",
        "    # need to explicitly put lengths on cpu!\n",
        "    packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len.to('cpu'),  enforce_sorted=False)\n",
        "    packed_outputs, hidden = self.gru(packed_embedded)\n",
        "\n",
        "    outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs) \n",
        "    hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
        "    return outputs, hidden\n",
        "\n",
        "class Attention(nn.Module):\n",
        "  def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "    super(Attention, self).__init__()\n",
        "    self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
        "    self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
        "\n",
        "  def forward(self, hidden, encoder_outputs, mask):\n",
        "\n",
        "    batch_size = encoder_outputs.shape[1]\n",
        "    src_len = encoder_outputs.shape[0]\n",
        "    # repeat decoder hidden state src_len times\n",
        "    hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "    encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "    energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) # energy = [batch size, src len, dec hid dim]\n",
        "    attention = self.v(energy).squeeze(2) # attention= [batch size, src len]\n",
        "    attention = attention.masked_fill(mask == 0, -1e10)\n",
        "    return F.softmax(attention, dim=1)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.output_dim = output_dim\n",
        "    self.attention = attention\n",
        "\n",
        "    self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "    self.gru = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
        "    self.fc = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "  def forward(self, input, hidden, encoder_outputs, mask):\n",
        "    input = input.unsqueeze(0) # input = [1, batch size]\n",
        "    embedded = self.dropout(self.embedding(input)) # embedded = [1, batch size, emb dim]\n",
        "    a = self.attention(hidden, encoder_outputs, mask)# a = [batch size, src len]\n",
        "    a = a.unsqueeze(1) # a = [batch size, 1, src len]\n",
        "    encoder_outputs = encoder_outputs.permute(1, 0, 2) # encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
        "    weighted = torch.bmm(a, encoder_outputs) # weighted = [batch size, 1, enc hid dim * 2]\n",
        "    weighted = weighted.permute(1, 0, 2) # weighted = [1, batch size, enc hid dim * 2]\n",
        "    rnn_input = torch.cat((embedded, weighted), dim = 2) # rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\n",
        "    output, hidden = self.gru(rnn_input, hidden.unsqueeze(0))\n",
        "    \n",
        "    assert (output == hidden).all()\n",
        "    embedded = embedded.squeeze(0)\n",
        "    output = output.squeeze(0)\n",
        "    weighted = weighted.squeeze(0)\n",
        "\n",
        "    prediction = self.fc(torch.cat((output, weighted, embedded), dim = 1)) # prediction = [batch size, output dim]\n",
        "    return prediction, hidden.squeeze(0), a.squeeze(1)\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self, encoder, decoder, src_pad_idx, device):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.device = device\n",
        "    self.src_pad_idx = src_pad_idx\n",
        "  \n",
        "  def create_mask(self, src):\n",
        "    mask = (src != self.src_pad_idx).permute(1, 0)\n",
        "    return mask\n",
        "  def forward(self, src, src_len, trg, teacher_forcing_ratio = 0.5):\n",
        "\n",
        "    trg_len, batch_size = trg.shape\n",
        "    trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "    # tensor to store decoder outputs\n",
        "    outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "    encoder_outputs, hidden = self.encoder(src, src_len)     \n",
        "    # first input to the decoder is the <sos> tokens\n",
        "    input = trg[0,:]\n",
        "    mask = self.create_mask(src) # mask = [batch size, src len]\n",
        "    for t in range(1, trg_len):\n",
        "      # insert input token embedding, previous hidden state and all encoder hidden states and mask\n",
        "      # receive output tensor (predictions) and new hidden state\n",
        "      output, hidden, _ = self.decoder(input, hidden, encoder_outputs, mask)\n",
        "      \n",
        "      # place predictions in a tensor holding predictions for each token\n",
        "      outputs[t] = output\n",
        "      \n",
        "      # decide if we are going to use teacher forcing or not\n",
        "      teacher_force = random.random() < teacher_forcing_ratio\n",
        "      \n",
        "      # get the highest predicted token from our predictions\n",
        "      top1 = output.argmax(1) \n",
        "      \n",
        "      # if teacher forcing, use actual next token as next input\n",
        "      # if not, use predicted token\n",
        "      input = trg[t] if teacher_force else top1\n",
        "    return outputs\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBIk2Pcuj18D"
      },
      "source": [
        "### Creating the model instance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCRgLBWF6IMh",
        "outputId": "edc7bac5-643b-4b5a-bced-254deb241c66"
      },
      "source": [
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "ENC_EMB_DIM = DEC_EMB_DIM = 256\n",
        "ENC_HID_DIM = DEC_HID_DIM = 512\n",
        "ENC_DROPOUT = DEC_DROPOUT = 0.5\n",
        "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
        "\n",
        "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
        "\n",
        "model = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)\n",
        "model"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(16321, 256)\n",
              "    (gru): GRU(256, 512, bidirectional=True)\n",
              "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (attention): Attention(\n",
              "      (attn): Linear(in_features=1536, out_features=512, bias=True)\n",
              "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
              "    )\n",
              "    (embedding): Embedding(14928, 256)\n",
              "    (gru): GRU(1280, 512)\n",
              "    (fc): Linear(in_features=1792, out_features=14928, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6OCFOHGj43X"
      },
      "source": [
        "### Initilize model weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCnqfZUl6IKO",
        "outputId": "a75c374b-359a-4e1a-d827-158e0213934f"
      },
      "source": [
        "def init_weights(m):\n",
        "  for name, param in m.named_parameters():\n",
        "    if 'weight' in name:\n",
        "        nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "    else:\n",
        "        nn.init.constant_(param.data, 0)   \n",
        "model.apply(init_weights)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(16321, 256)\n",
              "    (gru): GRU(256, 512, bidirectional=True)\n",
              "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (attention): Attention(\n",
              "      (attn): Linear(in_features=1536, out_features=512, bias=True)\n",
              "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
              "    )\n",
              "    (embedding): Embedding(14928, 256)\n",
              "    (gru): GRU(1280, 512)\n",
              "    (fc): Linear(in_features=1792, out_features=14928, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKKFq-iDj-Jl"
      },
      "source": [
        "### Counting model parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f880erWN6IGF",
        "outputId": "31ebc138-853f-4c85-b019-536a74018d31"
      },
      "source": [
        "def count_trainable_params(model):\n",
        "  return sum(p.numel() for p in model.parameters()), sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "n_params, trainable_params = count_trainable_params(model)\n",
        "print(f\"Total number of paramaters: {n_params:,}\\nTotal tainable parameters: {trainable_params:,}\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of paramaters: 41,198,928\n",
            "Total tainable parameters: 41,198,928\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HnPZQA7kCMC"
      },
      "source": [
        "### Optimizer and Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3eraLkI6IC6"
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axpQdrNekGIr"
      },
      "source": [
        "### Training and evaluation function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0NNT_dM6IAR"
      },
      "source": [
        "import math"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTsGL_hz6H9m"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for i, batch in enumerate(iterator):\n",
        "        src, src_len = batch.src\n",
        "        trg = batch.trg\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, src_len, trg)\n",
        "        \"\"\"\n",
        "        trg = [trg len, batch size]\n",
        "        output = [trg len, batch size, output dim]\n",
        "        \"\"\"\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        trg = trg[1:].view(-1)\n",
        "        \"\"\"\n",
        "        trg = [(trg len - 1) * batch size]\n",
        "        output = [(trg len - 1) * batch size, output dim]\n",
        "        \"\"\"\n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    with torch.no_grad():\n",
        "      for i, batch in enumerate(iterator):\n",
        "          src, src_len = batch.src\n",
        "          trg = batch.trg\n",
        "          optimizer.zero_grad()\n",
        "          output = model(src, src_len, trg, 0) ## Turn off the teacher forcing ratio.\n",
        "          \"\"\"\n",
        "          trg = [trg len, batch size]\n",
        "          output = [trg len, batch size, output dim]\n",
        "          \"\"\"\n",
        "          output_dim = output.shape[-1]\n",
        "          output = output[1:].view(-1, output_dim)\n",
        "          trg = trg[1:].view(-1)\n",
        "          \"\"\"\n",
        "          trg = [(trg len - 1) * batch size]\n",
        "          output = [(trg len - 1) * batch size, output dim]\n",
        "          \"\"\"\n",
        "          loss =  criterion(output, trg)\n",
        "          epoch_loss += loss.item()\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWk_iAiUkKF6"
      },
      "source": [
        "### Helper function that tell us how long each epoch took"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CC9smuPu6H6C"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "  elapsed_time = end_time - start_time\n",
        "  elapsed_mins = int(elapsed_time / 60)\n",
        "  elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "  return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYAh9RG-kPkM"
      },
      "source": [
        "### Running the training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neuH3dp-71h0",
        "outputId": "7b8982ac-d0d9-47e8-b6d9-9ba0bcc7b524"
      },
      "source": [
        "import time\n",
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "best_valid_loss = float('inf')\n",
        "for epoch in range(N_EPOCHS):\n",
        "  start_time = time.time()\n",
        "  train_loss = train(model, train_iter, optimizer, criterion, CLIP)\n",
        "  valid_loss = evaluate(model, val_iter, criterion)\n",
        "  end_time = time.time()\n",
        "  epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "  if valid_loss < best_valid_loss:\n",
        "      best_valid_loss = valid_loss\n",
        "      torch.save(model.state_dict(), 'best-model.pt')\n",
        "  \n",
        "  print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "  print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "  print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 2m 19s\n",
            "\tTrain Loss: 9.412 | Train PPL: 12229.659\n",
            "\t Val. Loss: 9.294 |  Val. PPL: 10873.185\n",
            "Epoch: 02 | Time: 2m 18s\n",
            "\tTrain Loss: 9.481 | Train PPL: 13107.400\n",
            "\t Val. Loss: 9.959 |  Val. PPL: 21148.458\n",
            "Epoch: 03 | Time: 2m 19s\n",
            "\tTrain Loss: 8.831 | Train PPL: 6846.489\n",
            "\t Val. Loss: 10.588 |  Val. PPL: 39672.123\n",
            "Epoch: 04 | Time: 2m 18s\n",
            "\tTrain Loss: 7.804 | Train PPL: 2449.558\n",
            "\t Val. Loss: 11.436 |  Val. PPL: 92563.089\n",
            "Epoch: 05 | Time: 2m 18s\n",
            "\tTrain Loss: 6.571 | Train PPL: 713.912\n",
            "\t Val. Loss: 11.768 |  Val. PPL: 129096.973\n",
            "Epoch: 06 | Time: 2m 19s\n",
            "\tTrain Loss: 5.345 | Train PPL: 209.572\n",
            "\t Val. Loss: 12.472 |  Val. PPL: 260952.143\n",
            "Epoch: 07 | Time: 2m 19s\n",
            "\tTrain Loss: 3.942 | Train PPL:  51.512\n",
            "\t Val. Loss: 13.818 |  Val. PPL: 1002316.166\n",
            "Epoch: 08 | Time: 2m 19s\n",
            "\tTrain Loss: 2.691 | Train PPL:  14.749\n",
            "\t Val. Loss: 14.749 |  Val. PPL: 2542320.809\n",
            "Epoch: 09 | Time: 2m 19s\n",
            "\tTrain Loss: 2.224 | Train PPL:   9.240\n",
            "\t Val. Loss: 15.224 |  Val. PPL: 4091551.732\n",
            "Epoch: 10 | Time: 2m 20s\n",
            "\tTrain Loss: 2.218 | Train PPL:   9.185\n",
            "\t Val. Loss: 16.127 |  Val. PPL: 10093478.502\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYXjB8LPkXFH"
      },
      "source": [
        "### Conclusion\n",
        "As we can see that the model is `Terrible`. We will find the way to improve the training speed and training metrics as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFU9XpizbYOk"
      },
      "source": [
        "### Resources used.\n",
        "\n",
        "1. [This Blog Post](https://towardsdatascience.com/how-to-use-torchtext-for-neural-machine-translation-plus-hack-to-make-it-5x-faster-77f3884d95)\n",
        "2. [Datasets List](http://www.statmt.org/europarl/)\n",
        "3. [Alen Nie](https://anie.me/On-Torchtext/)\n",
        "\n",
        "### Extra resources\n",
        "1 [Harvard](http://nlp.seas.harvard.edu/2018/04/03/attention.html)"
      ]
    }
  ]
}