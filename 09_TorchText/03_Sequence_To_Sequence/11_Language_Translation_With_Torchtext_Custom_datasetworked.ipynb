{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "11_Language_Translation_With_Torchtext_Custom_datasetworked.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRoEpNHb0ubp"
      },
      "source": [
        "### Language Translation with TorchText.\n",
        "\n",
        "This notebook is based on [pytorch tutorial](https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html) tutorial. In this notebook we are going to learn how we can use `torchtext` with a well known dataset to create a model that translates sentences from German to English."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cr42CZ1a1T8Q"
      },
      "source": [
        "### Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qJYMm0x0es-"
      },
      "source": [
        "import torchtext\n",
        "import torch\n",
        "\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from collections import Counter\n",
        "from torchtext.legacy.vocab import Vocab\n",
        "from torchtext.utils import download_from_url, extract_archive\n",
        "\n",
        "import os, io, time, random\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1y-MDSq2DYr"
      },
      "source": [
        "### SEEDS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bc18q-Jk0tpU"
      },
      "source": [
        "SEED = 42\n",
        "\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "random.seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deteministic = True"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l166tTEj2ReE"
      },
      "source": [
        "### URLs and Paths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBxh5cvr0trq"
      },
      "source": [
        "url_base = 'https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/'\n",
        "\n",
        "train_urls = ('train.de.gz', 'train.en.gz')\n",
        "val_urls = ('val.de.gz', 'val.en.gz')\n",
        "test_urls = ('test_2016_flickr.de.gz', 'test_2016_flickr.en.gz')\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnRlejij0tuu"
      },
      "source": [
        "train_filepaths = [\n",
        "      extract_archive(\n",
        "          download_from_url(url_base + url)\n",
        "      )[0] for url in train_urls\n",
        "]\n",
        "test_filepaths = [\n",
        "      extract_archive(\n",
        "          download_from_url(url_base + url)\n",
        "      )[0] for url in test_urls\n",
        "]\n",
        "val_filepaths = [\n",
        "      extract_archive(\n",
        "          download_from_url(url_base + url)\n",
        "      )[0] for url in test_urls\n",
        "]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qRECM3s0tw_",
        "outputId": "9f7e2af8-240f-4ab9-b3b9-0ff332e4d4ab"
      },
      "source": [
        "train_filepaths"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/.data/train.de', '/content/.data/train.en']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHE-C-k53Pho"
      },
      "source": [
        "### Creating Tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKahszyA3hsZ"
      },
      "source": [
        "!python -m spacy download en\n",
        "!python -m spacy download de"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwfD28SE0tzn"
      },
      "source": [
        "de_tokenizer = get_tokenizer('spacy', language=\"de\")\n",
        "en_tokenizer = get_tokenizer('spacy', language=\"en\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YF2rOW738E8"
      },
      "source": [
        "### `build_vocab()`\n",
        "This function will build the vocabulary for us."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSA0C0Md0t1-"
      },
      "source": [
        "def build_vocab(filepath, tokenizer):\n",
        "  counter = Counter()\n",
        "  with io.open(filepath, encoding=\"utf8\") as f:\n",
        "    for string_ in f:\n",
        "      counter.update(tokenizer(string_))\n",
        "  return Vocab(counter,specials=['<unk>', '<pad>', '<bos>', '<eos>'] )\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aQx54240t5u"
      },
      "source": [
        "de_vocab = build_vocab(train_filepaths[0], de_tokenizer)\n",
        "en_vocab = build_vocab(train_filepaths[1], en_tokenizer)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBJnYHq97knn"
      },
      "source": [
        "### `data_process()`\n",
        "\n",
        "This function will process the data for us. This function will read the text files line by line then tokenize each line, after tokenizing then they will convert each sentence to a vector of numbers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZwRDE0f0t8h"
      },
      "source": [
        "def data_process(filepaths):\n",
        "  raw_de_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n",
        "  raw_en_iter = iter(io.open(filepaths[1], encoding=\"utf8\"))\n",
        "  data = []\n",
        "  for raw_de, raw_en in zip(raw_de_iter, raw_en_iter):\n",
        "    \n",
        "    de_tensor_ = torch.tensor([de_vocab[token] for token in de_tokenizer(raw_de)])\n",
        "    en_tensor_ = torch.tensor([en_vocab[token] for token in en_tokenizer(raw_de)])\n",
        "    \n",
        "    data.append((de_tensor_, en_tensor_))\n",
        "  return data\n",
        "  "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzzD9OeK-ztc"
      },
      "source": [
        "Creating the sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FTHvqDv0t_1",
        "outputId": "ddf2ddea-c69d-4ed1-fe8b-04eeacba46d7"
      },
      "source": [
        "train_data = data_process(train_filepaths)\n",
        "test_data = data_process(test_filepaths)\n",
        "val_data = data_process(val_filepaths)\n",
        "\n",
        "val_data[:1]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(tensor([  6,  13,  11,   7, 179, 109,   9,  17,  79,   0,   5,   4]),\n",
              "  tensor([ 0,  0,  0,  0,  0,  0, 16,  0,  0,  0,  6,  5]))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76duYnsE_G-9"
      },
      "source": [
        "#### DataLoader\n",
        "\n",
        "We are going to use the DataLoader class to create the dataset. DataLoader combines a dataset and a sampler, and provides an iterable over the given dataset. The DataLoader supports both map-style and iterable-style datasets with single- or multi-process loading, customizing loading order and optional automatic batching (collation) and memory pinning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYwhmffS_eu8"
      },
      "source": [
        "### Device"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSIShe38_AEt"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCLNGpeO_jgh"
      },
      "source": [
        "### Hyper params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksdhQGhk_GpA",
        "outputId": "e489a95c-0647-4c77-d3c6-7b35bc954e60"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "PAD_IDX = de_vocab['<pad>']\n",
        "BOS_IDX = de_vocab['<bos>']\n",
        "EOS_IDX = de_vocab['<eos>']\n",
        "\n",
        "PAD_IDX, BOS_IDX, EOS_IDX"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 2, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJnMjJNp_weN"
      },
      "source": [
        "### Imports 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJhzgGEp_Glz"
      },
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3NGADkRAGeK"
      },
      "source": [
        "### `generate_batch()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDgjxAup_Gh2"
      },
      "source": [
        "def generate_batch(data_batch):\n",
        "  de_batch = []\n",
        "  en_batch = []\n",
        "  for de_item, en_item in data_batch:\n",
        "    de_batch.append(\n",
        "        torch.cat([torch.tensor([BOS_IDX]),\n",
        "                   de_item, torch.tensor([EOS_IDX])], dim=0\n",
        "                  )\n",
        "    )\n",
        "    en_batch.append(\n",
        "        torch.cat([torch.tensor([BOS_IDX]),\n",
        "                   en_item, torch.tensor([EOS_IDX])], dim=0\n",
        "                  )\n",
        "    )\n",
        "  \n",
        "  de_batch = pad_sequence(de_batch, padding_value=PAD_IDX)\n",
        "  en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)\n",
        "  return de_batch, en_batch"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6cNeJV0Bipl"
      },
      "source": [
        "### Creating the Iterators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vAgBepI_GeP"
      },
      "source": [
        "train_iter = DataLoader(train_data, batch_size=BATCH_SIZE,\n",
        "                        shuffle=True, collate_fn=generate_batch)\n",
        "valid_iter = DataLoader(val_data, batch_size=BATCH_SIZE,\n",
        "                        shuffle=True, collate_fn=generate_batch)\n",
        "test_iter = DataLoader(test_data, batch_size=BATCH_SIZE,\n",
        "                       shuffle=True, collate_fn=generate_batch)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cy_BoYlRBvfi"
      },
      "source": [
        "### Creating our model.\n",
        "\n",
        "* The model achitecture that we will be using if found [here](https://arxiv.org/abs/1409.0473)\n",
        "* The coded version of it will be found [here](https://github.com/SethHWeidman/pytorch-seq2seq/blob/master/3%20-%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbN46FiZDO9W"
      },
      "source": [
        "### Import 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFM7P_Kl_Ga5"
      },
      "source": [
        "from typing import Tuple\n",
        "from torch import Tensor\n",
        "from torch import optim"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USszvvvmDc6l"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRQ4eL3cDQ4V"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, \n",
        "               input_dim,\n",
        "               emb_dim,\n",
        "               enc_hid_dim,\n",
        "               dec_hid_dim,\n",
        "               dropout\n",
        "               ):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.input_dim = input_dim\n",
        "    self.emb_dim = emb_dim\n",
        "    self.enc_hid_dim = enc_hid_dim\n",
        "    self.dec_hid_dim = dec_hid_dim\n",
        "    self.dropout = dropout\n",
        "\n",
        "    self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "    self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
        "    self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, src):\n",
        "    embedded = self.dropout(self.embedding(src))\n",
        "    outputs, hidden = self.rnn(embedded)\n",
        "    hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
        "    return outputs, hidden\n",
        "  "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDrBqAYJDnhi"
      },
      "source": [
        "### Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9H3nZ5x8DphQ"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "  def __init__(self,\n",
        "              enc_hid_dim,\n",
        "              dec_hid_dim,\n",
        "              attn_dim\n",
        "               ):\n",
        "    super(Attention, self).__init__()\n",
        "\n",
        "    self.enc_hid_dim = enc_hid_dim\n",
        "    self.dec_hid_dim = dec_hid_dim\n",
        "    self.attn_in = (enc_hid_dim * 2) + dec_hid_dim\n",
        "    self.attn = nn.Linear(self.attn_in, attn_dim)\n",
        "\n",
        "  def forward(self,\n",
        "              decoder_hidden,\n",
        "              encoder_outputs\n",
        "              ):\n",
        "    src_len = encoder_outputs.shape[0]\n",
        "    repeated_decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, \n",
        "                                                                 src_len\n",
        "                                                                 , 1)\n",
        "    encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "    energy = torch.tanh(self.attn(torch.cat((\n",
        "        repeated_decoder_hidden,\n",
        "        encoder_outputs),\n",
        "        dim = 2)))\n",
        "    attention = torch.sum(energy, dim=2)\n",
        "    return F.softmax(attention, dim=1)\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkP0vEGDDegh"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDwhN7r6DQ15"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self,\n",
        "               output_dim,\n",
        "               emb_dim,\n",
        "               enc_hid_dim,\n",
        "               dec_hid_dim,\n",
        "               dropout,\n",
        "               attention\n",
        "               ):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.emb_dim = emb_dim\n",
        "    self.enc_hid_dim = enc_hid_dim\n",
        "    self.dec_hid_dim = dec_hid_dim\n",
        "    self.output_dim = output_dim\n",
        "    self.dropout = dropout\n",
        "    self.attention = attention\n",
        "\n",
        "    self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "    self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
        "    self.out = nn.Linear(self.attention.attn_in + emb_dim, output_dim)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def _weighted_encoder_rep(self,\n",
        "                            decoder_hidden,\n",
        "                            encoder_outputs\n",
        "                            ):\n",
        "    a = self.attention(decoder_hidden, encoder_outputs)\n",
        "    a = a.unsqueeze(1)\n",
        "    encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "    weighted_encoder_rep = torch.bmm(a, encoder_outputs)\n",
        "    weighted_encoder_rep = weighted_encoder_rep.permute(1, 0, 2)\n",
        "    return weighted_encoder_rep\n",
        "\n",
        "  def forward(self,\n",
        "              input,\n",
        "              decoder_hidden,\n",
        "              encoder_outputs,\n",
        "             ):\n",
        "    input = input.unsqueeze(0)\n",
        "    embedded = self.dropout(self.embedding(input))\n",
        "    weighted_encoder_rep = self._weighted_encoder_rep(decoder_hidden,\n",
        "                                                      encoder_outputs)\n",
        "    rnn_input = torch.cat((embedded, weighted_encoder_rep), dim = 2)\n",
        "    output, decoder_hidden = self.rnn(rnn_input, \n",
        "                                      decoder_hidden.unsqueeze(0)\n",
        "                                      )\n",
        "    embedded = embedded.squeeze(0)\n",
        "    output = output.squeeze(0)\n",
        "    weighted_encoder_rep = weighted_encoder_rep.squeeze(0)\n",
        "    output = self.out(torch.cat((output,\n",
        "                                  weighted_encoder_rep,\n",
        "                                  embedded), dim = 1))\n",
        "\n",
        "    return output, decoder_hidden.squeeze(0)\n",
        "      "
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJ_rpbdUDrBm"
      },
      "source": [
        "### Seq2Seq"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91_jE389DQx2"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self,\n",
        "                 encoder,\n",
        "                 decoder,\n",
        "                 device):\n",
        "    super(Seq2Seq, self).__init__()\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.device = device\n",
        "    \n",
        "  def forward(self, src, trg, teacher_forcing_ratio=.5):\n",
        "    batch_size = src.shape[1]\n",
        "    max_len = trg.shape[0]\n",
        "    trg_vocab_size = self.decoder.output_dim\n",
        "    outputs = torch.zeros(max_len, batch_size,\n",
        "                          trg_vocab_size).to(self.device)\n",
        "    encoder_outputs, hidden = self.encoder(src)\n",
        "    # first input to the decoder is the <sos> token\n",
        "    output = trg[0,:]\n",
        "    for t in range(1, max_len):\n",
        "      output, hidden = self.decoder(output, hidden, encoder_outputs)\n",
        "      outputs[t] = output\n",
        "      teacher_force = random.random() < teacher_forcing_ratio\n",
        "      top1 = output.max(1)[1]\n",
        "      output = (trg[t] if teacher_force else top1)\n",
        "      \n",
        "    return outputs\n",
        "  "
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9CUnyZWtFXu"
      },
      "source": [
        "### Hyper parameters\n",
        "\n",
        "We are going to use these hyper params:\n",
        "\n",
        "```\n",
        "ENC_EMB_DIM = 32\n",
        "DEC_EMB_DIM = 32\n",
        "ENC_HID_DIM = 64\n",
        "DEC_HID_DIM = 64\n",
        "ATTN_DIM = 8\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "```\n",
        "\n",
        "Insteady of these:\n",
        "```\n",
        "# ENC_EMB_DIM = 256\n",
        "# DEC_EMB_DIM = 256\n",
        "# ENC_HID_DIM = 512\n",
        "# DEC_HID_DIM = 512\n",
        "# ATTN_DIM = 64\n",
        "# ENC_DROPOUT = 0.5\n",
        "# DEC_DROPOUT = 0.5\n",
        "```\n",
        "**This is so the model will not takes long to train**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmPnxLHaDQwH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8dee1ea-3794-49f9-ea1c-38d8dbbe862e"
      },
      "source": [
        "INPUT_DIM = len(de_vocab)\n",
        "OUTPUT_DIM = len(en_vocab)\n",
        "\n",
        "ENC_EMB_DIM = 32\n",
        "DEC_EMB_DIM = 32\n",
        "ENC_HID_DIM = 64\n",
        "DEC_HID_DIM = 64\n",
        "ATTN_DIM = 8\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "\n",
        "attn = Attention(ENC_HID_DIM, DEC_HID_DIM, ATTN_DIM)\n",
        "\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)\n",
        "model\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(19206, 32)\n",
              "    (rnn): GRU(32, 64, bidirectional=True)\n",
              "    (fc): Linear(in_features=128, out_features=64, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (attention): Attention(\n",
              "      (attn): Linear(in_features=192, out_features=8, bias=True)\n",
              "    )\n",
              "    (embedding): Embedding(10840, 32)\n",
              "    (rnn): GRU(160, 64)\n",
              "    (out): Linear(in_features=224, out_features=10840, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7I6dV-v_twFT"
      },
      "source": [
        "### Initializing model weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4r9cnlzrDQtD"
      },
      "source": [
        "def init_weights(m: nn.Module):\n",
        "  for name, param in m.named_parameters():\n",
        "    if 'weight' in name:\n",
        "      nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "    else:\n",
        "      nn.init.constant_(param.data, 0)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWfjPOEIDQro",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2d2b4e8-d156-4f45-8f9e-9abeb5df63fc"
      },
      "source": [
        "model.apply(init_weights)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(19206, 32)\n",
              "    (rnn): GRU(32, 64, bidirectional=True)\n",
              "    (fc): Linear(in_features=128, out_features=64, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (attention): Attention(\n",
              "      (attn): Linear(in_features=192, out_features=8, bias=True)\n",
              "    )\n",
              "    (embedding): Embedding(10840, 32)\n",
              "    (rnn): GRU(160, 64)\n",
              "    (out): Linear(in_features=224, out_features=10840, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTRa1U_DuFH_"
      },
      "source": [
        "### Model parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5ydcu2wuHcj",
        "outputId": "c6446992-e1f8-4552-9cd1-826a52fd5f30"
      },
      "source": [
        "def count_trainable_params(model):\n",
        "  return sum(p.numel() for p in model.parameters()), sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "n_params, trainable_params = count_trainable_params(model)\n",
        "print(f\"Total number of paramaters: {n_params:,}\\nTotal tainable parameters: {trainable_params:,}\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of paramaters: 3,491,296\n",
            "Total tainable parameters: 3,491,296\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PowSmX4XuAxV"
      },
      "source": [
        "### Optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdQlxjV_uAQy"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters())"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74uOni1huaGk"
      },
      "source": [
        "### Criterion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOMlPQCOuANu"
      },
      "source": [
        "PAD_IDX = en_vocab.stoi['<pad>']\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX).to(device)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0FuYHejuu-j"
      },
      "source": [
        "### Training and evaluation functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pozVNOQuALX"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "  model.train()\n",
        "  epoch_loss = 0\n",
        "  for _, (src, trg) in enumerate(iterator):\n",
        "      src, trg = src.to(device), trg.to(device)\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      output = model(src, trg)\n",
        "      output = output[1:].view(-1, output.shape[-1])\n",
        "      trg = trg[1:].view(-1)\n",
        "\n",
        "      loss = criterion(output, trg)\n",
        "      loss.backward()\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "      optimizer.step()\n",
        "      epoch_loss += loss.item()\n",
        "\n",
        "  return epoch_loss / len(iterator)\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "  model.eval()\n",
        "  epoch_loss = 0\n",
        "  with torch.no_grad():\n",
        "    for _, (src, trg) in enumerate(iterator):\n",
        "      src, trg = src.to(device), trg.to(device)\n",
        "      output = model(src, trg, 0) #turn off teacher forcing\n",
        "      output = output[1:].view(-1, output.shape[-1])\n",
        "      trg = trg[1:].view(-1)\n",
        "      loss = criterion(output, trg)\n",
        "      epoch_loss += loss.item()\n",
        "\n",
        "  return epoch_loss / len(iterator)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtGZ2ZLrv1_F"
      },
      "source": [
        "A function that will help us to visualize how long an epoch took to train."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDIoUj-_uAJO"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "  elapsed_time = end_time - start_time\n",
        "  elapsed_mins = int(elapsed_time / 60)\n",
        "  elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "  return elapsed_mins, elapsed_secs\n",
        "  "
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oh21-KjHwGoL"
      },
      "source": [
        "Train loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3FNikhXuAFq",
        "outputId": "3c7be6a6-7ad8-4fa2-b50c-12a597027621"
      },
      "source": [
        "import math\n",
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "for epoch in range(N_EPOCHS):\n",
        "  start_time = time.time()\n",
        "\n",
        "  train_loss = train(model, train_iter, optimizer, criterion, CLIP)\n",
        "  valid_loss = evaluate(model, valid_iter, criterion)\n",
        "\n",
        "  end_time = time.time()\n",
        "  epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "  print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "  print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "  print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
        "\n",
        "test_loss = evaluate(model, test_iter, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 1m 8s\n",
            "\tTrain Loss: 2.137 | Train PPL:   8.472\n",
            "\t Val. Loss: 1.403 |  Val. PPL:   4.069\n",
            "Epoch: 02 | Time: 1m 7s\n",
            "\tTrain Loss: 1.104 | Train PPL:   3.015\n",
            "\t Val. Loss: 1.439 |  Val. PPL:   4.216\n",
            "Epoch: 03 | Time: 1m 7s\n",
            "\tTrain Loss: 1.076 | Train PPL:   2.933\n",
            "\t Val. Loss: 1.422 |  Val. PPL:   4.144\n",
            "Epoch: 04 | Time: 1m 8s\n",
            "\tTrain Loss: 1.033 | Train PPL:   2.809\n",
            "\t Val. Loss: 1.400 |  Val. PPL:   4.054\n",
            "Epoch: 05 | Time: 1m 7s\n",
            "\tTrain Loss: 1.032 | Train PPL:   2.807\n",
            "\t Val. Loss: 1.393 |  Val. PPL:   4.028\n",
            "Epoch: 06 | Time: 1m 8s\n",
            "\tTrain Loss: 1.013 | Train PPL:   2.753\n",
            "\t Val. Loss: 1.264 |  Val. PPL:   3.539\n",
            "Epoch: 07 | Time: 1m 8s\n",
            "\tTrain Loss: 0.905 | Train PPL:   2.473\n",
            "\t Val. Loss: 1.034 |  Val. PPL:   2.812\n",
            "Epoch: 08 | Time: 1m 7s\n",
            "\tTrain Loss: 0.830 | Train PPL:   2.294\n",
            "\t Val. Loss: 1.027 |  Val. PPL:   2.793\n",
            "Epoch: 09 | Time: 1m 7s\n",
            "\tTrain Loss: 0.803 | Train PPL:   2.233\n",
            "\t Val. Loss: 0.960 |  Val. PPL:   2.613\n",
            "Epoch: 10 | Time: 1m 7s\n",
            "\tTrain Loss: 0.738 | Train PPL:   2.091\n",
            "\t Val. Loss: 0.780 |  Val. PPL:   2.182\n",
            "| Test Loss: 0.782 | Test PPL:   2.186 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHDyzWyWCyGr"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "This tutorial give me a clear vision of how we can load our custom dataset. In this tutorial we use the `Multi30K` which is familiar.\n",
        "\n",
        "### Ref\n",
        "* [Torch Turorials](https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html)\n",
        "\n",
        "### Usefull resources\n",
        "* [Torch Turorials](https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html)\n",
        "* [SethHWeidman](https://github.com/SethHWeidman/pytorch-seq2seq/blob/master/3%20-%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate.ipynb)\n",
        "\n",
        "\n",
        "### Next\n",
        "\n",
        "We are going to expand from this notebook and forcus on how we can load our custom dataset from files.\n"
      ]
    }
  ]
}