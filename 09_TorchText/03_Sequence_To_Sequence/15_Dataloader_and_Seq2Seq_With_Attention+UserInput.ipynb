{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "15_Dataloader_and_Seq2Seq_With_Attention+UserInput.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwwyR820eJLI"
      },
      "source": [
        "### Seq2Seq With Attention + `User` Input\n",
        "\n",
        "In this notwbook we are going to copy the code from the previous notebook and expand it to accept user input and make translations.\n",
        "\n",
        "We are also going to allow the model to train for long probably `15` epochs.\n",
        "\n",
        "### Basic Imports\n",
        "WE are going to import modules we need as we go. These are the basic moodules that we may need to import first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "syKVBC2Zdv5P",
        "outputId": "e3e0f8ce-de8d-4382-f38f-2fea0c7e8eb7"
      },
      "source": [
        "import torch\n",
        "import time, os, math, random\n",
        "\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
        "\n",
        "import numpy as np \n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "torch.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.9.0+cu102'"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lh4tdLa4fOt_"
      },
      "source": [
        "### Seeds "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dfHPuKAfSHG"
      },
      "source": [
        "SEED = 42\n",
        "\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "random.seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deteministic = True"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OGuAJnEgFE_"
      },
      "source": [
        "### Device"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCpOQVo_gG6A",
        "outputId": "b9ad228c-f997-4546-8fdb-9d816687fa93"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWVMKFL9ewdm"
      },
      "source": [
        "### Data\n",
        "We are going to load the data from my google drive. I've uploaded a txt file that we can load and make some damage on it. First we need to mount the drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSJko1MIeCxb",
        "outputId": "6c249a9d-62e4-457f-a6fc-04934b5ac3a7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIF6IsVwgX-v"
      },
      "source": [
        "### Path to data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UI7prgGGeCuG",
        "outputId": "08d33ff5-b091-4726-af1a-83c095064814"
      },
      "source": [
        "base_path = '/content/drive/MyDrive/NLP Data/seq2seq/en-fr'\n",
        "os.path.exists(base_path)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3HesvGehBPP"
      },
      "source": [
        "### Data processing\n",
        "\n",
        "We are going to create a new file called `en-fr.txt` that will contain english to french sentence pairs. We are going this because our giant file `fra.txt` contain some unnecessary or extra pair on each line. We are going to trim that and come up with a fresh text file in a couple of code cells that follows:\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZLlSUUAj4JF",
        "outputId": "2f0e436c-7926-475e-916e-43f00f95f01c"
      },
      "source": [
        "lines = open(os.path.join(base_path, 'fra.txt')).read().split('\\n')\n",
        "lines[:2]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Go.\\tVa !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #1158250 (Wittydev)',\n",
              " 'Go.\\tMarche.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8090732 (Micsmithel)']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzT63ao8j3-3",
        "outputId": "05e01f26-e7f9-4f83-dc18-76eab87ea61b"
      },
      "source": [
        "clean_lines = []\n",
        "for line in lines:\n",
        "  try:\n",
        "    en, fr, _ = line.split('\\t')\n",
        "    clean_lines.append(f'{en}\\t{fr}')\n",
        "  except:\n",
        "    pass\n",
        "len(clean_lines)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "190206"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESV_-ur7ljuX",
        "outputId": "d130111f-14dc-4156-9f23-b2f64d62d4a8"
      },
      "source": [
        "clean_lines[:2]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Go.\\tVa !', 'Go.\\tMarche.']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2RKF9E4lZIw"
      },
      "source": [
        "Now that we have line that are paired. We are now ready to create our file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvO7XG_Ij35j",
        "outputId": "e957cca3-81be-47fa-b88a-e0243a2a1148"
      },
      "source": [
        "with open(os.path.join(base_path, 'en-fr.txt' ), 'w') as f:\n",
        "  f.write(\"\\n\".join(clean_lines))\n",
        "print(\"Created file en-fr.txt\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Created file en-fr.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxLyMGyZhliY"
      },
      "source": [
        "### Tokenizers for each language.\n",
        "We are going to use the spacy tokenizer to create two tokenizers. One will be for `french` and the other one will be for `english`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7nDkzDVeCrP"
      },
      "source": [
        "from torchtext.data.utils import get_tokenizer"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJs4mfx7iILw"
      },
      "source": [
        "!python -m spacy download en\n",
        "!python -m spacy download fr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fdp1NOdqeCoV"
      },
      "source": [
        "fr_tokenizer = get_tokenizer('spacy', language=\"fr\")\n",
        "en_tokenizer = get_tokenizer('spacy', language=\"en\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujgU4MxZiPXP",
        "outputId": "f4e6401c-766f-4d93-ae94-a6220f84eb9d"
      },
      "source": [
        "en_tokenizer(\"This is a boy?\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This', 'is', 'a', 'boy', '?']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwFZ-dsOmySg"
      },
      "source": [
        "Now we are ready to create our Dataset class. We are going to call this dataset class `EnglishFrenchDataset`. If you want to understand more about how to inherit from the pytorch `Dataset` class and the magic i recommend [this](https://github.com/CrispenGari/pytorch-python/blob/main/01_PyTorch_Basics/08_DataLoaders/DataLoader.ipynb)\n",
        "\n",
        "\n",
        "### `EnglishFrenchDataset`:\n",
        "\n",
        "Args:\n",
        "```\n",
        "  path:        path to a text file\n",
        "  transform:   list transformation\n",
        "  max_vocab:   maximum number for words\n",
        "  shuffle:     shuffling the data\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xq_IVzpwoG9M"
      },
      "source": [
        "PATH = os.path.join(base_path, 'en-fr.txt')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afmrgjnGeClQ"
      },
      "source": [
        "class EnglishFrenchDataset(Dataset):\n",
        "  def __init__(self, path:str, transform=None,\n",
        "               max_vocab = None):\n",
        "    self.max_vocab = max_vocab\n",
        "    self.transform = transform\n",
        "\n",
        "    # Special tokens\n",
        "    self.pad = '<pad>'\n",
        "    self.sos = '<sos>'\n",
        "    self.eos = '<eos>'\n",
        "    self.unk = '<unk>'\n",
        "\n",
        "    # Helper functions that flattens lists\n",
        "    self.flatten = lambda x: [sublist for lst in x for sublist in lst]\n",
        "    \n",
        "    # Loading the dataset\n",
        "    df = pd.read_csv(path, names=[\"en\", \"fr\"], sep=\"\\t\")\n",
        "    self.len = len(df)\n",
        "\n",
        "    # Tokenize src (en -> fr)\n",
        "    self.tokenize_df(df)\n",
        "   \n",
        "    # Replacing rare occuring words in the corpus to be <unk>\n",
        "    # self.replace_rare_tokens(df)\n",
        "\n",
        "    # tokens to index maping (stoi -> string to intenger)\n",
        "    self.stoi(df)\n",
        "    # Remove sequences with mostly <UNK>\n",
        "    if self.max_vocab is not None:\n",
        "      df = self.remove_mostly_unk(df)\n",
        "      \n",
        "    # Convert tokens to indices\n",
        "    self.tokens_to_indices(df)\n",
        "    self.df = df\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.len\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.tokens_pairs[index][0], self.tokens_pairs[index][1]\n",
        "  # Every sequence (input and target) should start with <sos>  and end with <eos>\n",
        "  def add_start_and_end_to_tokens(self, x):\n",
        "    return  [self.sos] + x + [self.eos]\n",
        "\n",
        "  def tokenize_df(self, df):\n",
        "    \"\"\"Turn src/trg into tokens\"\"\"\n",
        "    df['src'] = df.fr.apply(lambda x: x.lower()).apply(fr_tokenizer).apply(self.add_start_and_end_to_tokens)\n",
        "    df['trg'] = df.en.apply(lambda x: x.lower()).apply(en_tokenizer).apply(self.add_start_and_end_to_tokens)\n",
        "\n",
        "  def replace_rare_tokens(self, df):\n",
        "    \"\"\"replacing rare tokens with unk token\"\"\"\n",
        "    common_tokens_src = self.get_most_common_tokens(\n",
        "        df.src.tolist()\n",
        "    )\n",
        "    common_tokens_trg = self.get_most_common_tokens(\n",
        "        df.src.tolist()\n",
        "    )\n",
        "    df.loc[:, 'src'] = df.src.apply(\n",
        "          lambda tokens: [token if token in common_tokens_src \n",
        "                          else self.unk for token in tokens]\n",
        "      )\n",
        "    df.loc[:, 'trg'] = df.trg.apply(\n",
        "          lambda tokens: [token if token in common_tokens_trg \n",
        "                          else self.unk for token in tokens]\n",
        "    )\n",
        "\n",
        "  def get_most_common_tokens(self, tokens):\n",
        "    \"\"\"Return the max_vocab most common tokens.\"\"\"\n",
        "    all_tokens = self.flatten(tokens)\n",
        "    # Substract 4 for <pad>, <sos>, <eos>, and <unk>\n",
        "    common_tokens = set(list(zip(*Counter(all_tokens).most_common(\n",
        "            self.max_vocab - 4)))[0])\n",
        "    return common_tokens\n",
        "\n",
        "  def remove_mostly_unk(self, df, threshold=0.99):\n",
        "      \"\"\"Remove sequences with mostly <UNK>.\"\"\"\n",
        "      calculate_ratio = (\n",
        "          lambda tokens: sum(1 for token in tokens if token != '<UNK>')\n",
        "          / len(tokens) > threshold\n",
        "      )\n",
        "      df = df[df.src.apply(calculate_ratio)]\n",
        "      df = df[df.trg.apply(calculate_ratio)]\n",
        "      return df\n",
        "  def stoi(self, df):\n",
        "    unique_tokens_src = set(self.flatten(df.src))\n",
        "    unique_tokens_trg = set(self.flatten(df.trg))\n",
        "    for token in reversed([self.pad, self.sos, self.eos, self.unk]):\n",
        "      if token in unique_tokens_src:\n",
        "        unique_tokens_src.remove(token)\n",
        "      if token in unique_tokens_trg:\n",
        "        unique_tokens_trg.remove(token)\n",
        "            \n",
        "    unique_tokens_src = sorted(list(unique_tokens_src))\n",
        "    unique_tokens_trg = sorted(list(unique_tokens_trg))\n",
        "\n",
        "    # Add <pad>, <sos>, <eos>, and <unk> tokens\n",
        "    for token in reversed([self.pad, self.sos, self.eos, self.unk]):\n",
        "      unique_tokens_src = [token] + unique_tokens_src\n",
        "      unique_tokens_trg = [token] + unique_tokens_trg\n",
        "            \n",
        "      self.stoi_src = {token: idx for idx, token\n",
        "                                 in enumerate(unique_tokens_src)}\n",
        "      self.itos_src = {idx: token for token, idx\n",
        "                                 in self.stoi_src.items()}\n",
        "      self.stoi_trg = {token: idx for idx, token\n",
        "                                  in enumerate(unique_tokens_trg)}\n",
        "      self.itos_trg = {idx: token for token, idx\n",
        "                                  in self.stoi_trg.items()}\n",
        "    \n",
        "  def tokens_to_indices(self, df):\n",
        "    \"\"\"Convert tokens to indices.\"\"\"\n",
        "    df['src_tokens'] = df.src.apply(\n",
        "        lambda tokens: [self.stoi_src[token] for token in tokens])\n",
        "   \n",
        "    df['trg_tokens'] = df.trg.apply(\n",
        "        lambda tokens: [self.stoi_trg[token] for token in tokens])\n",
        "    self.tokens_pairs = list(zip(df.src_tokens, df.trg_tokens))\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDEtDa5p2Rjk"
      },
      "source": [
        "### Custom transformation\n",
        "We are going to create a custom transformation class that will convert list to tensors. We are going to call the `ToTensor` class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jqLQbWgeCim"
      },
      "source": [
        "class ToTensor:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def __call__(self, sample):\n",
        "    _, __ = sample\n",
        "    return torch.tensor(_), torch.tensor(__)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wz08Dc6n3JN3"
      },
      "source": [
        "### Creating the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGwa0jt4eCff",
        "outputId": "85371e99-a04c-4683-bfd8-0f7e8ab260f0"
      },
      "source": [
        "%%time\n",
        "dataset = EnglishFrenchDataset(\n",
        "    os.path.join(base_path, 'en-fr.txt'),\n",
        "    transform = ToTensor()\n",
        ")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 23.2 s, sys: 442 ms, total: 23.6 s\n",
            "Wall time: 23.7 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VT4kH0HFb4Wt",
        "outputId": "4675e469-ec9c-40b6-8574-5ea636ddff66"
      },
      "source": [
        "len(dataset)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "190206"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "iM4dAXBYdl9N",
        "outputId": "c96dd9b3-7c54-4a16-fabe-68780558a2bd"
      },
      "source": [
        "dataset.df.head(10).iloc[:, 2:]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>src</th>\n",
              "      <th>trg</th>\n",
              "      <th>src_tokens</th>\n",
              "      <th>trg_tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[&lt;sos&gt;, va, !, &lt;eos&gt;]</td>\n",
              "      <td>[&lt;sos&gt;, go, ., &lt;eos&gt;]</td>\n",
              "      <td>[1, 24169, 4, 2]</td>\n",
              "      <td>[1, 5989, 20, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[&lt;sos&gt;, marche, ., &lt;eos&gt;]</td>\n",
              "      <td>[&lt;sos&gt;, go, ., &lt;eos&gt;]</td>\n",
              "      <td>[1, 14284, 18, 2]</td>\n",
              "      <td>[1, 5989, 20, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[&lt;sos&gt;, bouge, !, &lt;eos&gt;]</td>\n",
              "      <td>[&lt;sos&gt;, go, ., &lt;eos&gt;]</td>\n",
              "      <td>[1, 3175, 4, 2]</td>\n",
              "      <td>[1, 5989, 20, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[&lt;sos&gt;, salut, !, &lt;eos&gt;]</td>\n",
              "      <td>[&lt;sos&gt;, hi, ., &lt;eos&gt;]</td>\n",
              "      <td>[1, 21175, 4, 2]</td>\n",
              "      <td>[1, 6532, 20, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[&lt;sos&gt;, salut, ., &lt;eos&gt;]</td>\n",
              "      <td>[&lt;sos&gt;, hi, ., &lt;eos&gt;]</td>\n",
              "      <td>[1, 21175, 18, 2]</td>\n",
              "      <td>[1, 6532, 20, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[&lt;sos&gt;, cours,  , !, &lt;eos&gt;]</td>\n",
              "      <td>[&lt;sos&gt;, run, !, &lt;eos&gt;]</td>\n",
              "      <td>[1, 5923, 25871, 4, 2]</td>\n",
              "      <td>[1, 11437, 4, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>[&lt;sos&gt;, courez,  , !, &lt;eos&gt;]</td>\n",
              "      <td>[&lt;sos&gt;, run, !, &lt;eos&gt;]</td>\n",
              "      <td>[1, 5904, 25871, 4, 2]</td>\n",
              "      <td>[1, 11437, 4, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>[&lt;sos&gt;, prenez, vos, jambes, à, vos, cous, !, ...</td>\n",
              "      <td>[&lt;sos&gt;, run, !, &lt;eos&gt;]</td>\n",
              "      <td>[1, 17825, 24735, 13126, 24988, 24735, 5946, 4...</td>\n",
              "      <td>[1, 11437, 4, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>[&lt;sos&gt;, file, !, &lt;eos&gt;]</td>\n",
              "      <td>[&lt;sos&gt;, run, !, &lt;eos&gt;]</td>\n",
              "      <td>[1, 10134, 4, 2]</td>\n",
              "      <td>[1, 11437, 4, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>[&lt;sos&gt;, filez, !, &lt;eos&gt;]</td>\n",
              "      <td>[&lt;sos&gt;, run, !, &lt;eos&gt;]</td>\n",
              "      <td>[1, 10138, 4, 2]</td>\n",
              "      <td>[1, 11437, 4, 2]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 src  ...        trg_tokens\n",
              "0                              [<sos>, va, !, <eos>]  ...  [1, 5989, 20, 2]\n",
              "1                          [<sos>, marche, ., <eos>]  ...  [1, 5989, 20, 2]\n",
              "2                           [<sos>, bouge, !, <eos>]  ...  [1, 5989, 20, 2]\n",
              "3                           [<sos>, salut, !, <eos>]  ...  [1, 6532, 20, 2]\n",
              "4                           [<sos>, salut, ., <eos>]  ...  [1, 6532, 20, 2]\n",
              "5                        [<sos>, cours,  , !, <eos>]  ...  [1, 11437, 4, 2]\n",
              "6                       [<sos>, courez,  , !, <eos>]  ...  [1, 11437, 4, 2]\n",
              "7  [<sos>, prenez, vos, jambes, à, vos, cous, !, ...  ...  [1, 11437, 4, 2]\n",
              "8                            [<sos>, file, !, <eos>]  ...  [1, 11437, 4, 2]\n",
              "9                           [<sos>, filez, !, <eos>]  ...  [1, 11437, 4, 2]\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AUFBygMNP6d"
      },
      "source": [
        "### Splitting Sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LhYKrzdNSzm",
        "outputId": "801c8660-9015-4bb0-a8c1-e0b2b9f59c70"
      },
      "source": [
        "train_size = int(len(dataset) * .9)\n",
        "valid_size = int(len(dataset) * .08)\n",
        "test_size = len(dataset) - train_size - valid_size\n",
        "train_dataset,valid_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size, test_size])\n",
        "\n",
        "train_size, test_size, valid_size"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(171185, 3805, 15216)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8j1PCzl_VuD"
      },
      "source": [
        "### Creating generators using DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHZHCrIseCXY"
      },
      "source": [
        "def collate(batch):\n",
        "  src = [torch.LongTensor(item[0]) for item in batch]\n",
        "  trg = [torch.LongTensor(item[1])  for item in batch]\n",
        "\n",
        "  padded_src = pad_sequence(src,\n",
        "                               padding_value=dataset.stoi_src[dataset.pad],\n",
        "                               batch_first=True)\n",
        "  padded_trg = pad_sequence(trg,\n",
        "                            padding_value=dataset.stoi_src[dataset.pad],\n",
        "                            batch_first=True)\n",
        "\n",
        "  # Sort by length for CUDA optimizations\n",
        "  lengths = torch.LongTensor([len(x) for x in src])\n",
        "  lengths, permutation = lengths.sort(dim=0, descending=True)\n",
        "\n",
        "  return padded_src[permutation].to(device), padded_trg[permutation].to(device), lengths.to(device)\n",
        " \n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNx8MnKreCUm"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=collate)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, collate_fn=collate)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=collate)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJBDqs3TNbbK"
      },
      "source": [
        "### Seq2Seq with Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuEQX_BROARq"
      },
      "source": [
        "### Encoder model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4Gin_fSeCRX"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, vocab_size,\n",
        "               embedding_dim,\n",
        "               hidden_size, batch_size):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_size = batch_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
        "    self.gru = nn.GRU(\n",
        "        self.embedding_dim,\n",
        "        self.hidden_size,\n",
        "        batch_first=True,\n",
        "    )\n",
        "    \n",
        "  def forward(self, src, lengths):\n",
        "    self.batch_size = src.size(0)\n",
        "    # Turn input indices into distributed embeddings\n",
        "    x = self.embedding(src)\n",
        "    # Remove padding for more efficient RNN application\n",
        "    x = pack_padded_sequence(x, lengths.to('cpu'), batch_first=True)\n",
        "\n",
        "    # Apply RNN to get hidden state at all timesteps (output)\n",
        "    # and hidden state of last output (self.hidden)\n",
        "    output, self.hidden = self.gru(x, self.init_hidden())\n",
        "    # Pad the sequences like they were before\n",
        "    output, _ = pad_packed_sequence(output)\n",
        "    return output, self.hidden\n",
        "  def init_hidden(self):\n",
        "    # Randomly initialize the weights of the RNN\n",
        "    return torch.randn(1, self.batch_size, self.hidden_size).to(device)\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCUXxT_nODfq"
      },
      "source": [
        "### Decoder Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b80mR7pOOFaH"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(\n",
        "      self, \n",
        "      vocab_size,\n",
        "      embedding_dim, \n",
        "      decoder_hidden_size,\n",
        "      encoder_hidden_size, \n",
        "      batch_size):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_size = batch_size\n",
        "    self.encoder_hidden_size = encoder_hidden_size\n",
        "    self.decoder_hidden_size = decoder_hidden_size\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
        "    self.gru = nn.GRU(\n",
        "        self.embedding_dim + self.encoder_hidden_size, \n",
        "        self.decoder_hidden_size,\n",
        "        batch_first=True,\n",
        "    )\n",
        "    self.fc = nn.Linear(self.encoder_hidden_size, self.vocab_size)\n",
        "    \n",
        "    # Attention weights\n",
        "    self.W1 = nn.Linear(self.encoder_hidden_size, self.decoder_hidden_size)\n",
        "    self.W2 = nn.Linear(self.encoder_hidden_size, self.decoder_hidden_size)\n",
        "    self.V = nn.Linear(self.encoder_hidden_size, 1)\n",
        "\n",
        "  def forward(self, trg, hidden, encoder_output):\n",
        "    self.batch_size = trg.size(0)\n",
        "    # Switch the dimensions of sequence_length and batch_size\n",
        "    encoder_output = encoder_output.permute(1, 0, 2)\n",
        "\n",
        "    # Add an extra axis for a time dimension\n",
        "    hidden_with_time_axis = hidden.permute(1, 0, 2)\n",
        "    \n",
        "    # Attention score (Bahdanaus)\n",
        "    score = torch.tanh(self.W1(encoder_output) + self.W2(hidden_with_time_axis))\n",
        "\n",
        "    # Attention weights\n",
        "    attention_weights = torch.softmax(self.V(score), dim=1)\n",
        "    \n",
        "    # Find the context vectors\n",
        "    context_vector = attention_weights * encoder_output\n",
        "    context_vector = torch.sum(context_vector, dim=1)\n",
        "    \n",
        "    # Turn target indices into distributed embeddings\n",
        "    x = self.embedding(trg)\n",
        "    \n",
        "    # Add the context representation to the target embeddings\n",
        "    x = torch.cat((context_vector.unsqueeze(1), x), -1)\n",
        "    \n",
        "    # Apply the RNN\n",
        "    output, state = self.gru(x, self.init_hidden())\n",
        "    \n",
        "    # Reshape the hidden states (output)\n",
        "    output = output.view(-1, output.size(2))\n",
        "    \n",
        "    # Apply a linear layer\n",
        "    x = self.fc(output)\n",
        "    \n",
        "    return x, state, attention_weights\n",
        "  \n",
        "  def init_hidden(self):\n",
        "    # Randomly initialize the weights of the RNN\n",
        "    return torch.randn(1, self.batch_size, self.decoder_hidden_size).to(device)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AouTsm1OKT5"
      },
      "source": [
        "### Encoder Decoder Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEwcKyjNONCk"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=dataset.stoi_src[dataset.pad])\n",
        "def loss_function(real, pred):\n",
        "  # Use mask to only consider non-zero inputs in the loss\n",
        "  mask = real.ge(1).float().to(device)\n",
        "  loss_ = criterion(pred, real) * mask \n",
        "  return torch.mean(loss_)\n",
        "\n",
        "\n",
        "class EncoderDecoder(nn.Module):\n",
        "\n",
        "  def __init__(self, inputs_vocab_size,\n",
        "                targets_vocab_size, hidden_size,\n",
        "                embedding_dim, batch_size, \n",
        "                targets_start_idx, targets_stop_idx):\n",
        "      super(EncoderDecoder, self).__init__()\n",
        "      self.batch_size = batch_size\n",
        "      self.targets_start_idx = targets_start_idx\n",
        "      self.targets_stop_idx = targets_stop_idx\n",
        "      \n",
        "      self.encoder = Encoder(inputs_vocab_size, embedding_dim,\n",
        "                              hidden_size, batch_size).to(device)\n",
        "      \n",
        "      self.decoder = Decoder(targets_vocab_size, embedding_dim,\n",
        "                              hidden_size, hidden_size, batch_size).to(device)\n",
        "      \n",
        "  def predict(self, inputs, lengths):\n",
        "    self.batch_size = inputs.size(0)\n",
        "    \n",
        "    encoder_output, encoder_hidden = self.encoder(\n",
        "        inputs.to(device),\n",
        "        lengths,\n",
        "    )\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    # Initialize the input of the decoder to be <SOS>\n",
        "    decoder_input = torch.LongTensor(\n",
        "        [[self.targets_start_idx]] * self.batch_size,\n",
        "    )\n",
        "    \n",
        "    # Output predictions instead of loss\n",
        "    output = []\n",
        "    for _ in range(20):\n",
        "      predictions, decoder_hidden, _ = self.decoder(\n",
        "          decoder_input.to(device), \n",
        "          decoder_hidden.to(device),\n",
        "          encoder_output.to(device),\n",
        "      )\n",
        "      prediction = torch.multinomial(F.softmax(predictions, dim=1), 1)\n",
        "      decoder_input = prediction\n",
        "      \n",
        "      prediction = prediction[0].item()\n",
        "      output.append(prediction)\n",
        "\n",
        "      if prediction == self.targets_stop_idx:\n",
        "          return output\n",
        "    return output\n",
        "\n",
        "  def forward(self, inputs, targets, lengths):\n",
        "    self.batch_size = inputs.size(0)\n",
        "    \n",
        "    encoder_output, encoder_hidden = self.encoder(\n",
        "        inputs.to(device),\n",
        "        lengths,\n",
        "    )\n",
        "    decoder_hidden = encoder_hidden\n",
        "    \n",
        "    # Initialize the input of the decoder to be <SOS>\n",
        "    decoder_input = torch.LongTensor(\n",
        "        [[self.targets_start_idx]] * self.batch_size,\n",
        "    )\n",
        "            \n",
        "    # Use teacher forcing to train the model. Instead of feeding the model's\n",
        "    # own predictions to itself, feed the target token at every timestep.\n",
        "    # This leads to faster convergence\n",
        "    loss = 0\n",
        "    for timestep in range(1, targets.size(1)):\n",
        "      predictions, decoder_hidden, _ = self.decoder(\n",
        "          decoder_input.to(device), \n",
        "          decoder_hidden.to(device),\n",
        "          encoder_output.to(device),\n",
        "      )\n",
        "      decoder_input = targets[:, timestep].unsqueeze(1)\n",
        "      \n",
        "      loss += loss_function(targets[:, timestep], predictions)\n",
        "    return loss / targets.size(1)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_XvgRkVW9o2"
      },
      "source": [
        "### Model Instance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVjDe_fmORrX",
        "outputId": "da641095-593e-4f66-c901-dd0202b161fb"
      },
      "source": [
        "model = EncoderDecoder(\n",
        "    inputs_vocab_size=len(dataset.stoi_src),\n",
        "    targets_vocab_size=len(dataset.stoi_trg),\n",
        "    hidden_size=256,\n",
        "    embedding_dim=100, \n",
        "    batch_size=BATCH_SIZE, \n",
        "    targets_start_idx=dataset.stoi_trg[dataset.sos],\n",
        "    targets_stop_idx=dataset.stoi_trg[dataset.eos],\n",
        ").to(device)\n",
        "\n",
        "model"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EncoderDecoder(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(25876, 100)\n",
              "    (gru): GRU(100, 256, batch_first=True)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(15139, 100)\n",
              "    (gru): GRU(356, 256, batch_first=True)\n",
              "    (fc): Linear(in_features=256, out_features=15139, bias=True)\n",
              "    (W1): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (W2): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (V): Linear(in_features=256, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24uhwyPyRBUT"
      },
      "source": [
        "### Counting model parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ix1DQaWCREbv",
        "outputId": "52620c59-de63-4c11-b3d8-780a5089bad0"
      },
      "source": [
        "def count_trainable_params(model):\n",
        "  return sum(p.numel() for p in model.parameters()), sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "n_params, trainable_params = count_trainable_params(model)\n",
        "print(f\"Total number of paramaters: {n_params:,}\\nTotal tainable parameters: {trainable_params:,}\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of paramaters: 8,870,560\n",
            "Total tainable parameters: 8,870,560\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFkql7fVRMGL"
      },
      "source": [
        "### Initializing model weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhLsIwgERSUj"
      },
      "source": [
        "def init_weights(m: nn.Module):\n",
        "  for name, param in m.named_parameters():\n",
        "    if 'weight' in name:\n",
        "      nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "    else:\n",
        "      nn.init.constant_(param.data, 0)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8x49EgCRUG6",
        "outputId": "f150141b-265f-4684-cf12-f6caeb714534"
      },
      "source": [
        "model.apply(init_weights)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EncoderDecoder(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(25876, 100)\n",
              "    (gru): GRU(100, 256, batch_first=True)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(15139, 100)\n",
              "    (gru): GRU(356, 256, batch_first=True)\n",
              "    (fc): Linear(in_features=256, out_features=15139, bias=True)\n",
              "    (W1): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (W2): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (V): Linear(in_features=256, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o69cESmSRaGj"
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters())"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APiwrOyNRjF_"
      },
      "source": [
        "### Training and evaluate functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9H0QEQmRmTF"
      },
      "source": [
        "def train(model, iterator, optimizer):\n",
        "  model.train()\n",
        "  total_loss = total = 0\n",
        "  for index, (inputs, targets, lengths) in enumerate(iterator):\n",
        "    optimizer.zero_grad()\n",
        "    loss = model(inputs, targets, lengths)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    total_loss += loss.item()\n",
        "    total += targets.size(1)\n",
        "  train_loss = total_loss / total\n",
        "  return train_loss\n",
        "\n",
        "def evaluate(model, iterator):\n",
        "  model.eval()\n",
        "  total_loss = total = 0\n",
        "  with torch.no_grad():\n",
        "      for index, (inputs, targets, lengths) in enumerate(iterator):\n",
        "        loss = model(inputs, targets, lengths)\n",
        "        total_loss += loss.item()\n",
        "        total += targets.size(1)\n",
        "  test_loss = total_loss / total\n",
        "  return test_loss\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjgPJynMTWFc"
      },
      "source": [
        "### Helper functions\n",
        "\n",
        "1. time to string"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6srn5n6TV3p"
      },
      "source": [
        "def hms_string(sec_elapsed):\n",
        "    h = int(sec_elapsed / (60 * 60))\n",
        "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
        "    s = sec_elapsed % 60\n",
        "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlODnp4ZTZcM"
      },
      "source": [
        "2. Tabulate training epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iin5TgXBQr-u"
      },
      "source": [
        "from prettytable import PrettyTable\n",
        "def tabulate_training(column_names, data, title):\n",
        "  table = PrettyTable(column_names)\n",
        "  table.title= title\n",
        "  table.align[column_names[0]] = 'l'\n",
        "  table.align[column_names[1]] = 'r'\n",
        "  table.align[column_names[2]] = 'r'\n",
        "  table.align[column_names[3]] = 'r'\n",
        "  for row in data:\n",
        "    table.add_row(row)\n",
        "  print(table)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zd66oJgCTSb7"
      },
      "source": [
        "### Train Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paJVJvBKTpXh",
        "outputId": "274edc61-050a-4731-8e6d-8674962e23bb"
      },
      "source": [
        "N_EPOCHS = 15\n",
        "best_valid_loss = float('inf')\n",
        "column_names = [\"SET\", \"LOSS\", \"PPL\", \"ETA\"]\n",
        "print(\"TRAINING STARTS....\")\n",
        "for epoch in range(N_EPOCHS):\n",
        "  start = time.time()\n",
        "  train_loss = train(model, train_loader, optimizer )\n",
        "  valid_loss = evaluate(model, valid_loader)\n",
        "  end = time.time()\n",
        "  title = f\"EPOCH: {epoch+1:02}/{N_EPOCHS:02} | {'saving model...' if valid_loss < best_valid_loss else 'not saving...'}\" \n",
        "  if valid_loss < best_valid_loss:\n",
        "      best_valid_loss = valid_loss\n",
        "      torch.save(model.state_dict(), 'best-model.pt')\n",
        "  rows_data =[\n",
        "        [\"train\", f\"{train_loss:.3f}\", f\"{math.exp(train_loss):7.3f}\", hms_string(end - start) ],\n",
        "        [\"val\", f\"{valid_loss:.3f}\", f\"{math.exp(train_loss):7.3f}\", '' ]\n",
        "  ]\n",
        "  tabulate_training(column_names, rows_data, title)\n",
        "\n",
        "print(\"TRAINING ENDS....\")\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAINING STARTS....\n",
            "+--------------------------------------+\n",
            "|    EPOCH: 01/15 | saving model...    |\n",
            "+-------+-------+---------+------------+\n",
            "| SET   |  LOSS |     PPL |        ETA |\n",
            "+-------+-------+---------+------------+\n",
            "| train | 0.096 |   1.101 | 0:03:51.42 |\n",
            "| val   | 0.084 |   1.101 |            |\n",
            "+-------+-------+---------+------------+\n",
            "+--------------------------------------+\n",
            "|    EPOCH: 02/15 | saving model...    |\n",
            "+-------+-------+---------+------------+\n",
            "| SET   |  LOSS |     PPL |        ETA |\n",
            "+-------+-------+---------+------------+\n",
            "| train | 0.074 |   1.076 | 0:03:51.56 |\n",
            "| val   | 0.063 |   1.076 |            |\n",
            "+-------+-------+---------+------------+\n",
            "+--------------------------------------+\n",
            "|    EPOCH: 03/15 | saving model...    |\n",
            "+-------+-------+---------+------------+\n",
            "| SET   |  LOSS |     PPL |        ETA |\n",
            "+-------+-------+---------+------------+\n",
            "| train | 0.056 |   1.058 | 0:03:51.50 |\n",
            "| val   | 0.050 |   1.058 |            |\n",
            "+-------+-------+---------+------------+\n",
            "+--------------------------------------+\n",
            "|    EPOCH: 04/15 | saving model...    |\n",
            "+-------+-------+---------+------------+\n",
            "| SET   |  LOSS |     PPL |        ETA |\n",
            "+-------+-------+---------+------------+\n",
            "| train | 0.043 |   1.044 | 0:03:51.66 |\n",
            "| val   | 0.040 |   1.044 |            |\n",
            "+-------+-------+---------+------------+\n",
            "+--------------------------------------+\n",
            "|    EPOCH: 05/15 | saving model...    |\n",
            "+-------+-------+---------+------------+\n",
            "| SET   |  LOSS |     PPL |        ETA |\n",
            "+-------+-------+---------+------------+\n",
            "| train | 0.034 |   1.035 | 0:03:51.59 |\n",
            "| val   | 0.034 |   1.035 |            |\n",
            "+-------+-------+---------+------------+\n",
            "+--------------------------------------+\n",
            "|    EPOCH: 06/15 | saving model...    |\n",
            "+-------+-------+---------+------------+\n",
            "| SET   |  LOSS |     PPL |        ETA |\n",
            "+-------+-------+---------+------------+\n",
            "| train | 0.028 |   1.029 | 0:03:51.46 |\n",
            "| val   | 0.031 |   1.029 |            |\n",
            "+-------+-------+---------+------------+\n",
            "+--------------------------------------+\n",
            "|    EPOCH: 07/15 | saving model...    |\n",
            "+-------+-------+---------+------------+\n",
            "| SET   |  LOSS |     PPL |        ETA |\n",
            "+-------+-------+---------+------------+\n",
            "| train | 0.024 |   1.024 | 0:03:51.31 |\n",
            "| val   | 0.028 |   1.024 |            |\n",
            "+-------+-------+---------+------------+\n",
            "+--------------------------------------+\n",
            "|    EPOCH: 08/15 | saving model...    |\n",
            "+-------+-------+---------+------------+\n",
            "| SET   |  LOSS |     PPL |        ETA |\n",
            "+-------+-------+---------+------------+\n",
            "| train | 0.021 |   1.022 | 0:03:51.43 |\n",
            "| val   | 0.027 |   1.022 |            |\n",
            "+-------+-------+---------+------------+\n",
            "+--------------------------------------+\n",
            "|    EPOCH: 09/15 | saving model...    |\n",
            "+-------+-------+---------+------------+\n",
            "| SET   |  LOSS |     PPL |        ETA |\n",
            "+-------+-------+---------+------------+\n",
            "| train | 0.019 |   1.019 | 0:03:52.20 |\n",
            "| val   | 0.026 |   1.019 |            |\n",
            "+-------+-------+---------+------------+\n",
            "+--------------------------------------+\n",
            "|    EPOCH: 10/15 | saving model...    |\n",
            "+-------+-------+---------+------------+\n",
            "| SET   |  LOSS |     PPL |        ETA |\n",
            "+-------+-------+---------+------------+\n",
            "| train | 0.017 |   1.018 | 0:03:52.38 |\n",
            "| val   | 0.025 |   1.018 |            |\n",
            "+-------+-------+---------+------------+\n",
            "+--------------------------------------+\n",
            "|    EPOCH: 11/15 | saving model...    |\n",
            "+-------+-------+---------+------------+\n",
            "| SET   |  LOSS |     PPL |        ETA |\n",
            "+-------+-------+---------+------------+\n",
            "| train | 0.016 |   1.016 | 0:03:52.49 |\n",
            "| val   | 0.025 |   1.016 |            |\n",
            "+-------+-------+---------+------------+\n",
            "+--------------------------------------+\n",
            "|     EPOCH: 12/15 | not saving...     |\n",
            "+-------+-------+---------+------------+\n",
            "| SET   |  LOSS |     PPL |        ETA |\n",
            "+-------+-------+---------+------------+\n",
            "| train | 0.015 |   1.015 | 0:03:52.27 |\n",
            "| val   | 0.025 |   1.015 |            |\n",
            "+-------+-------+---------+------------+\n",
            "+--------------------------------------+\n",
            "|    EPOCH: 13/15 | saving model...    |\n",
            "+-------+-------+---------+------------+\n",
            "| SET   |  LOSS |     PPL |        ETA |\n",
            "+-------+-------+---------+------------+\n",
            "| train | 0.014 |   1.014 | 0:03:52.72 |\n",
            "| val   | 0.024 |   1.014 |            |\n",
            "+-------+-------+---------+------------+\n",
            "+--------------------------------------+\n",
            "|    EPOCH: 14/15 | saving model...    |\n",
            "+-------+-------+---------+------------+\n",
            "| SET   |  LOSS |     PPL |        ETA |\n",
            "+-------+-------+---------+------------+\n",
            "| train | 0.013 |   1.013 | 0:03:52.62 |\n",
            "| val   | 0.024 |   1.013 |            |\n",
            "+-------+-------+---------+------------+\n",
            "+--------------------------------------+\n",
            "|     EPOCH: 15/15 | not saving...     |\n",
            "+-------+-------+---------+------------+\n",
            "| SET   |  LOSS |     PPL |        ETA |\n",
            "+-------+-------+---------+------------+\n",
            "| train | 0.012 |   1.012 | 0:03:53.06 |\n",
            "| val   | 0.024 |   1.012 |            |\n",
            "+-------+-------+---------+------------+\n",
            "TRAINING ENDS....\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUnab-iTVO28"
      },
      "source": [
        "### Evaluating the best model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KoDf1LyVSGb",
        "outputId": "dfc9cc84-1ac5-4fe5-ab50-addc20b54269"
      },
      "source": [
        "model.load_state_dict(torch.load('best-model.pt'))\n",
        "\n",
        "test_loss = evaluate(model, test_loader)\n",
        "title = \"Model Evaluation Summary\"\n",
        "data_rows = [[\"Test\", f'{test_loss:.3f}', f'{math.exp(test_loss):7.3f}', \"\"]]\n",
        "\n",
        "tabulate_training([\"SET\", \"LOSS\", \"PPL\", \"ETA\"], data_rows, title)\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------------------+\n",
            "|   Model Evaluation Summary   |\n",
            "+------+-------+---------+-----+\n",
            "| SET  |  LOSS |     PPL | ETA |\n",
            "+------+-------+---------+-----+\n",
            "| Test | 0.022 |   1.022 |     |\n",
            "+------+-------+---------+-----+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKaVDDf1OUiO"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYVQ-0qtOVup",
        "outputId": "e7101ae5-7be9-4df2-a72a-d8fc0067e4d0"
      },
      "source": [
        "model.eval()\n",
        "total_loss = total = 0\n",
        "with torch.no_grad():\n",
        "  for inputs, targets, lengths in test_loader:\n",
        "    print('input| >', ' '.join([\n",
        "        dataset.itos_src[idx]\n",
        "        for idx in inputs.cpu()[0].numpy()[1:-1]\n",
        "    ]))\n",
        "    print('target| >', ' '.join([\n",
        "        dataset.itos_trg[idx]\n",
        "        for idx in targets.cpu()[0].numpy()[1:-1]\n",
        "    ]))\n",
        "    # Forwards pass\n",
        "    outputs = model.predict(inputs, lengths)\n",
        "    prediction = ' '.join([\n",
        "        dataset.itos_trg[idx]\n",
        "        for idx in outputs[:-1]\n",
        "    ])\n",
        "    print()\n",
        "    print(\"predicted| =\", prediction)\n",
        "    print(\"*\" * 100)\n",
        "    print()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input| > un livre qui ne vaut pas la peine d' être lu ne vaut pas la peine d' être acheté dès le départ .\n",
            "target| > a book not worth reading is not worth buying in the first place .\n",
            "\n",
            "predicted| = a book who is worth doing not already purchased late for the delay not found so come .\n",
            "****************************************************************************************************\n",
            "\n",
            "input| > «   où es - tu allé   ?   » «   je suis allé à la gare saluer le départ d’ un ami .   »\n",
            "target| > where have you been ? \" i have been to the station to see a friend off . \"\n",
            "\n",
            "predicted| = where did you go ? \"\n",
            "****************************************************************************************************\n",
            "\n",
            "input| > son panier de couture , les tiroirs de son vaisselier et les étagères de son garde-manger sont tous systématiquement ordonnés de manière impeccable .\n",
            "target| > her sewing basket , dresser drawers and pantry shelves are all systematically arranged in apple - pie order .\n",
            "\n",
            "predicted| = his crowd of haiti , white thanksgiving and its services were all vicious kick into being actions revise their\n",
            "****************************************************************************************************\n",
            "\n",
            "input| > j' attends vraiment mars prochain , quand ils sortiront la nouvelle «   play station 2   » .\n",
            "target| > i 'm really looking forward to next march , when they roll out the new play station 2 .\n",
            "\n",
            "predicted| = i 'm really planning on plane next day when they took the new york slump in neutral , will\n",
            "****************************************************************************************************\n",
            "\n",
            "input| > au japon , il pleut pas mal pendant notre saison des pluies , qui dure de mi-juin à mi-juillet .\n",
            "target| > in japan , it rains quite a bit during our rainy season which is from mid - june until mid - july .\n",
            "\n",
            "predicted| = in japan , it rains , there was out for our family easter many years younger sort of size\n",
            "****************************************************************************************************\n",
            "\n",
            "input| > même si , grammaticalement , il n' y a pas de problème avec cette phrase , je doute que quelqu' un l' utilise vraiment .\n",
            "target| > though grammatically there 's nothing wrong with this sentence , i doubt if anyone would actually ever use it .\n",
            "\n",
            "predicted| = even if , leisure , there is no problems with him using it 's use it to me exist\n",
            "****************************************************************************************************\n",
            "\n",
            "input| > je n' arrive pas à croire que tes parents te laissent venir ici par tes propres moyens .\n",
            "target| > i ca n't believe your parents let you come here by yourself . <eos> <pad> <pad> <pad> <pad>\n",
            "\n",
            "predicted| = i ca n't believe your parents let you come here .\n",
            "****************************************************************************************************\n",
            "\n",
            "input| > j' ai dû démissionner parce que je ne m' entendais simplement pas avec le nouveau patron .\n",
            "target| > i had to resign because i just did n't get along with the new boss . <eos>\n",
            "\n",
            "predicted| = i had to resign because i just not to lie again .\n",
            "****************************************************************************************************\n",
            "\n",
            "input| > je ferai tout ce qui est en mon pouvoir pour faire en sorte que vos enfants soient en sécurité .\n",
            "target| > i 'll do everything within my power to make sure your children are safe . <eos>\n",
            "\n",
            "predicted| = i 'll do whatever it if i could do so for our children are safety .\n",
            "****************************************************************************************************\n",
            "\n",
            "input| > je pense que tu as besoin de passer un peu plus de temps à tes devoirs .\n",
            "target| > i think you need to spend a little more time on your homework .\n",
            "\n",
            "predicted| = i think you need to spend a little more time in your homework .\n",
            "****************************************************************************************************\n",
            "\n",
            "input| > «   pourquoi vas - tu au japon   ?   » «   pour participer à une conférence à tokyo .   »\n",
            "target| > why are you going to japan ? \" to attend a conference in tokyo . \" <eos> <pad>\n",
            "\n",
            "predicted| = why are you doing in japan ? \" right out to tokyo with her landing on and six ?\n",
            "****************************************************************************************************\n",
            "\n",
            "input| > le gâteau a brûlé par ma faute . j' étais au téléphone et n' ai pas fait attention à l' heure .\n",
            "target| > it 's my fault that the cake was burned . i was talking on the phone and did n't notice the time .\n",
            "\n",
            "predicted| = the catholic burned by my fault or one was n't plan on now .\n",
            "****************************************************************************************************\n",
            "\n",
            "input| > il y a huit ans , nous étions au début de ce qui allait devenir la pire crise économique de notre existence .\n",
            "target| > eight years ago , we were in the early stages of what would become the worst economic crisis of our lifetimes .\n",
            "\n",
            "predicted| = in 1960 . we were speaking partly where it would be gone worse , but in become worse products\n",
            "****************************************************************************************************\n",
            "\n",
            "input| > je me sentais mal , aussi je fus admis à l' hôpital . cependant , il s' avéra qu' il n' y avait rien qui n' allait réellement pas chez moi .\n",
            "target| > i felt bad , so i was admitted into the hospital . however , it turned out that there was nothing really wrong with me .\n",
            "\n",
            "predicted| = i felt bad as i was to school to used to be home .\n",
            "****************************************************************************************************\n",
            "\n",
            "input| > il commençait à faire sombre et , pour empirer les choses , il s' est mis à pleuvoir .\n",
            "target| > it was getting dark , and , what made matters worse , it began to rain . <eos> <pad>\n",
            "\n",
            "predicted| = he was going to make dark dark and the purpose to rain things it has to build .\n",
            "****************************************************************************************************\n",
            "\n",
            "input| > saviez - vous qu' il y avait plus d' une façon de faire ça   ?\n",
            "target| > did you know there 's more than one way to do this ? <eos> <pad>\n",
            "\n",
            "predicted| = did you know it were more rich i do this ?\n",
            "****************************************************************************************************\n",
            "\n",
            "input| > j' ai fait une réservation dans un bar près de la gare de shinjuku . c' est boisson à volonté pendant deux heures pour 2   500 yens .\n",
            "target| > i 've made a reservation at a drinking place near shinjuku station . it 's all you can drink for 2 hours for 2,500 yen .\n",
            "\n",
            "predicted| = i gave in a reservation during the coffee crash having a hundred years .\n",
            "****************************************************************************************************\n",
            "\n",
            "input| > je ne vois jamais une bibliothèque sans souhaiter avoir le temps de m' y rendre et d' y rester jusqu' à ce que j' y aie tout lu .\n",
            "target| > i never see a library without wishing i had time to go there and stay till i had read everything in it .\n",
            "\n",
            "predicted| = i never see a library without 18 to watch and have months , and there are about it .\n",
            "****************************************************************************************************\n",
            "\n",
            "input| > je lui dit que , si je pouvais être d' une quelconque utilité , je serais ravie d' aider .\n",
            "target| > i told her that if i could be of any use i would be glad to help . <eos>\n",
            "\n",
            "predicted| = i told that it if i could be no use more than to help to me .\n",
            "****************************************************************************************************\n",
            "\n",
            "input| > il était surpris de voir que le chef - d' œuvre du grand artiste était suspendu au mur à l' envers .\n",
            "target| > he was surprised to find the great artist 's masterpiece hung on the wall upside down . <eos>\n",
            "\n",
            "predicted| = he was surprised to see that the signal was washed his way was introduced on sunlight , was lying\n",
            "****************************************************************************************************\n",
            "\n",
            "input| > il est très aisé d' avoir l' air naturel dans votre langue natale et très aisé d' avoir l' air de manquer de naturel dans une autre langue .\n",
            "target| > it 's very easy to sound natural in your own native language , and very easy to sound unnatural in your non - native language .\n",
            "\n",
            "predicted| = it is very easy to have natural to your body more but some air and might very important person\n",
            "****************************************************************************************************\n",
            "\n",
            "input| > conformément au contrat , vous pouvez prendre trois jours de congé de décès pour les funérailles de votre oncle , mais un seul pour celles de votre neveu .\n",
            "target| > according to the contract you may take three days of bereavement leave for your uncle 's funeral , but only one for your nephew 's .\n",
            "\n",
            "predicted| = news to you have last two of your arms of their uncle 's the uncle 's the beatles that\n",
            "****************************************************************************************************\n",
            "\n",
            "input| > je crois que tom seul peut le faire . toutefois , il y a ceux qui croient que marie en est également capable .\n",
            "target| > i think that tom and only tom can do it . however , some people think that mary could do it , too .\n",
            "\n",
            "predicted| = i think that tom can do it as if they believe it was some facts trust them is also\n",
            "****************************************************************************************************\n",
            "\n",
            "input| > j' ai toutes ces pommes à porter , sans compter ce sac de pommes de terre .\n",
            "target| > i have all these apples to carry , not to mention this bag of potatoes .\n",
            "\n",
            "predicted| = i 've got those apple to wear , but could trust that green plans book .\n",
            "****************************************************************************************************\n",
            "\n",
            "input| > « mais vous ne pensez pas que ce soit un peu grand   ? » demanda la boutiquière .\n",
            "target| > but do n't you think that it 's a little big ? asked the shopkeeper .\n",
            "\n",
            "predicted| = but do n't think that 's a small private ?\n",
            "****************************************************************************************************\n",
            "\n",
            "input| > peu importe le temps que tu passeras à essayer de convaincre les gens que le chocolat est de la vanille , ça restera toujours du chocolat , même si tu réussis à convaincre toi et quelques autres que c' est de la vanille .\n",
            "target| > no matter how much you try to convince people that chocolate is vanilla , it 'll still be chocolate , even though you may manage to convince yourself and a few others that it 's vanilla .\n",
            "\n",
            "predicted| = there 's once more time you make it to ask the people that efficient like this car as if\n",
            "****************************************************************************************************\n",
            "\n",
            "input| > trouve un emploi , de façon à pouvoir subvenir aux besoins de ta femme et de tes enfants !\n",
            "target| > get a job so you can support your wife and kids . <eos> <pad> <pad> <pad> <pad> <pad>\n",
            "\n",
            "predicted| = just a good time as people want to support a woman and who your family and with your one\n",
            "****************************************************************************************************\n",
            "\n",
            "input| > c' était suffisamment grave qu' il ait l' habitude d' arriver en retard au travail , mais qu' il arrive soûl est un comble , et je vais devoir m' en séparer .\n",
            "target| > it was bad enough that he usually came to work late , but coming in drunk was the last straw , and i 'm going to have to let him go .\n",
            "\n",
            "predicted| = it was quite quick as he comes to go about it , right , so there is just at\n",
            "****************************************************************************************************\n",
            "\n",
            "input| > je veux présenter mes excuses pour la manière dont je t' ai parlé la dernière fois que nous nous sommes rencontrés .\n",
            "target| > i want to apologize for the way i talked to you the last time we met .\n",
            "\n",
            "predicted| = i want to apologize while i told you through the last time we met .\n",
            "****************************************************************************************************\n",
            "\n",
            "input| > en règle générale , je préfère les gens qui traitent ce genre de choses directement avec les personnes concernées .\n",
            "target| > as a rule , i prefer people who deal with matters of this kind directly with those involved .\n",
            "\n",
            "predicted| = generally speaking , i could use people who say so difficult person on stuff .\n",
            "****************************************************************************************************\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-aMEU0psurQ"
      },
      "source": [
        "### User Input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "w2Dt4BVN6rmP",
        "outputId": "af91427f-cc60-4855-f3f9-0f25f20ba5b3"
      },
      "source": [
        "def predict(inputs, lengths, model=model, device=device,\n",
        "            targets_stop_idx=dataset.stoi_trg[\"<eos>\"], targets_start_idx=\n",
        "            dataset.stoi_trg[\"<sos>\"], batch_size=1):\n",
        "  \n",
        "    encoder_output, encoder_hidden = model.encoder(\n",
        "        inputs.to(device),\n",
        "        lengths,\n",
        "    )\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    # Initialize the input of the decoder to be <SOS>\n",
        "    decoder_input = torch.LongTensor(\n",
        "        [[targets_start_idx]] * batch_size,\n",
        "    )\n",
        "    \n",
        "    # Output predictions instead of loss\n",
        "    output = []\n",
        "    for _ in range(20):\n",
        "      predictions, decoder_hidden, _ = model.decoder(\n",
        "          decoder_input.to(device), \n",
        "          decoder_hidden.to(device),\n",
        "          encoder_output.to(device),\n",
        "      )\n",
        "      prediction = torch.multinomial(F.softmax(predictions, dim=1), 1)\n",
        "      decoder_input = prediction\n",
        "      \n",
        "      prediction = prediction[0].item()\n",
        "      output.append(prediction)\n",
        "\n",
        "      if prediction == targets_stop_idx:\n",
        "          return output\n",
        "    return output\n",
        "\n",
        "def translate(sent):\n",
        "  tokens = fr_tokenizer(sent)\n",
        "  sequences = torch.LongTensor([[dataset.stoi_src[tok] for tok in tokens]])\n",
        "  lengths = torch.LongTensor([len(sequences)])\n",
        "  output = predict(inputs=sequences, lengths=lengths)\n",
        "  return ' '.join([\n",
        "        dataset.itos_trg[idx]\n",
        "        for idx in outputs[:-1]\n",
        "  ])\n",
        "\n",
        "\n",
        "translate(\"je veux présenter mes excuses pour la manière dont je t' ai parlé la dernière fois que nous nous sommes rencontrés .\")"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'generally speaking , i could use people who say so difficult person on stuff .'"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSxFe2gXZqrd"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "The model if able to pickup some words here and there. To improve this model we need to provide quality data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_u23j7rNke8U"
      },
      "source": [
        ""
      ],
      "execution_count": 36,
      "outputs": []
    }
  ]
}