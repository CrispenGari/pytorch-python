{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03_Faster_Sentiment_Analyisis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqJZTTU_d04c"
      },
      "source": [
        "### Faster Sentiment Analyisis.\n",
        "\n",
        "> \"This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore - CPU, and classify half a million sentences among~312K classes in less than a minute.\" - Fast Text\n",
        "\n",
        "This is based on this [paper](https://arxiv.org/abs/1607.01759)\n",
        "\n",
        "\n",
        "In this notebook we are going to implement a model based on that paper that yields comparable results from the previous model. This model will train for s short amount of time and with few trainable parameters.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_HrF4EFfuHr"
      },
      "source": [
        "### Data Preparation.\n",
        "One of the key concepts in the ``FastText`` paper is that they calculate the ``n-grams`` of an input sentence and append them to the end of a sentence. Here, we'll use ``bi-grams``.\n",
        "\n",
        "We are going to create a ``generate_bigrams`` function takes a sentence that has already been tokenized, calculates the bi-grams and appends them to the end of the tokenized list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KII_7PiSd0hH",
        "outputId": "e13f79f9-a2e2-4560-8ad6-d79a105d466e"
      },
      "source": [
        "\n",
        "def generate_bigrams(x):\n",
        "  n_grams = set(zip(*[x[i:] for i in range(2)]))\n",
        "  for n_gram in n_grams:\n",
        "      x.append(' '.join(n_gram))\n",
        "  return x\n",
        "generate_bigrams(['This', 'film', 'is', 'terrible'])"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This', 'film', 'is', 'terrible', 'film is', 'is terrible', 'This film']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7uYo9n2iAfb"
      },
      "source": [
        "**TorchText** ``Fields`` have a ``preprocessing`` argument which is a function be applied to a sentence after it has been tokenized, **but before it has been numericalized**. This is where we'll pass our ``generate_bigrams`` function.\n",
        "\n",
        "As we aren't using an ``RNN`` we can't use packed padded sequences, thus we do not need to set ``include_lengths = True``."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-zzadxVdv6K"
      },
      "source": [
        "import torch\n",
        "from torchtext.legacy import data, datasets"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNp_LYaCiCAs"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "SEED = 42\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4g6fjzzXiB9r"
      },
      "source": [
        "TEXT = data.Field(tokenize = 'spacy',\n",
        "                  tokenizer_language = 'en_core_web_sm',\n",
        "                  preprocessing = generate_bigrams)\n",
        "\n",
        "LABEL = data.LabelField(dtype = torch.float)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njv2-aTvjQ0r"
      },
      "source": [
        "### We need to split the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpVgjiIojVBG"
      },
      "source": [
        "import random"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_x2_Dr0XiB6x",
        "outputId": "f57cb0f6-3e1e-4cd1-c2be-6eb51d171e8e"
      },
      "source": [
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
        "train_data, valid_data = train_data.split(random_state = random.seed(SEED))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:05<00:00, 15.6MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wJfC7pRjnO5"
      },
      "source": [
        "### Build the vocab and load the pre-trained word embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ti8Cmj8miB3K",
        "outputId": "afb6deb2-38f8-4cab-c686-c6ade7a50a1c"
      },
      "source": [
        "MAX_VOCAB_SIZE = 25_000\n",
        "\n",
        "TEXT.build_vocab(train_data, \n",
        "                 max_size = MAX_VOCAB_SIZE, \n",
        "                 vectors = \"glove.6B.100d\", \n",
        "                 unk_init = torch.Tensor.normal_)\n",
        "\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:42, 5.29MB/s]                           \n",
            "100%|█████████▉| 399479/400000 [00:14<00:00, 28432.33it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOMkwVfYjx-I"
      },
      "source": [
        "# Create Iterators and push them to `device`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weeD05HNiBz_"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = BATCH_SIZE, \n",
        "    device = device\n",
        "    )"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1ClcddmkD7f"
      },
      "source": [
        "### Building a Model.\n",
        "This model has far fewer parameters than the previous model as it only has ``2`` layers that have any parameters, the ``embedding layer`` and the ``linear layer``. \n",
        "\n",
        "Instead, it first calculates the word embedding for each word using the Embedding layer _(blue)_, then calculates the average of all of the word embeddings _(pink)_ and feeds this through the Linear layer _(silver)_, and that's it!\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/bentrevett/pytorch-sentiment-analysis/raw/2b666b3cba7d629a2f192c7d9c66fadcc9f0c363/assets/sentiment8.png\"/>\n",
        "</p>\n",
        "\n",
        "We implement the averaging with the ``avg_pool2d`` (average pool 2-dimensions) function. Initially, you may think using a 2-dimensional pooling seems strange, surely our sentences are 1-dimensional, not 2-dimensional? However, you can think of the word embeddings as a 2-dimensional grid, where the words are along one axis and the dimensions of the word embeddings are along the other. The image below is an example sentence after being converted into 5-dimensional word embeddings, with the words along the vertical axis and the embeddings along the horizontal axis. Each element in this ``[4x5]`` tensor is represented by a green block.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/bentrevett/pytorch-sentiment-analysis/raw/2b666b3cba7d629a2f192c7d9c66fadcc9f0c363/assets/sentiment9.png\"/>\n",
        "</p>\n",
        "\n",
        "The ``avg_pool2d`` uses a filter of size ``embedded.shape[1] `` (i.e. the length of the sentence) by ``1``. This is shown in pink in the image below.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/bentrevett/pytorch-sentiment-analysis/raw/2b666b3cba7d629a2f192c7d9c66fadcc9f0c363/assets/sentiment10.png\"/>\n",
        "</p>\n",
        "We calculate the average value of all elements covered by the filter, then the filter then slides to the right, calculating the average over the next column of embedding values for each word in the sentence.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/bentrevett/pytorch-sentiment-analysis/raw/2b666b3cba7d629a2f192c7d9c66fadcc9f0c363/assets/sentiment11.png\"/>\n",
        "</p>\n",
        "\n",
        "Each filter position gives us a single value, the average of all covered elements. After the filter has covered all embedding dimensions we get a ``[1x5]`` tensor. This tensor is then passed through the linear layer to produce our prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-1aUJFanVXi"
      },
      "source": [
        "from torch.nn import functional as F\n",
        "import torch.nn as nn"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFajkTcPnQHH"
      },
      "source": [
        "class FastText(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, output_dim, pad_idx):\n",
        "    super(FastText, self).__init__()\n",
        "\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "    self.fc = nn.Linear(embedding_dim, output_dim)\n",
        "\n",
        "  def forward(self, text):\n",
        "    #text = [sent len, batch size]\n",
        "    embedded = self.embedding(text)\n",
        "    #embedded = [sent len, batch size, emb dim]\n",
        "    embedded = embedded.permute(1, 0, 2)\n",
        "    #embedded = [batch size, sent len, emb dim]\n",
        "    pooled = F.avg_pool2d(embedded, (embedded.shape[1], 1)).squeeze(1) \n",
        "    #pooled = [batch size, embedding_dim]\n",
        "    return self.fc(pooled)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzZfwDrwogmy"
      },
      "source": [
        "### Creating the instance of the `FastText` model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Uu8DaWxnQBD",
        "outputId": "96c2d4f8-0b1a-44d9-d14c-4ce95c88400b"
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "OUTPUT_DIM = 1\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "model = FastText(INPUT_DIM, EMBEDDING_DIM, OUTPUT_DIM, PAD_IDX)\n",
        "model"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FastText(\n",
              "  (embedding): Embedding(25002, 100, padding_idx=1)\n",
              "  (fc): Linear(in_features=100, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nme9fvTEo4Ax"
      },
      "source": [
        "### Checking number of trainable parameters in our `FastText` model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aiiGEWIoynQ",
        "outputId": "4bc68e10-6730-4473-a45a-69618e7cee83"
      },
      "source": [
        "def count_trainable_params(model):\n",
        "  return sum(p.numel() for p in model.parameters() if p.requires_grad==True)\n",
        "\n",
        "print(f'The model has {count_trainable_params(model):,} trainable parameters')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 2,500,301 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vEwk7HnpnAA"
      },
      "source": [
        "### Copying the pretrainned vectors to the `embedding` layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "id3ckW_op6NC"
      },
      "source": [
        "pretrained_embeddings = TEXT.vocab.vectors"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjCgbNXUnP96",
        "outputId": "4d19fddf-c535-42f6-e8ed-d3f02a853c72"
      },
      "source": [
        "model.embedding.weight.data.copy_(pretrained_embeddings)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.9269,  1.4873,  0.9007,  ...,  0.1233,  0.3499,  0.6173],\n",
              "        [ 0.7262,  0.0912, -0.3891,  ...,  0.0821,  0.4440, -0.7240],\n",
              "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
              "        ...,\n",
              "        [-0.0232, -0.1614,  0.2054,  ...,  0.5729,  1.6427,  0.1845],\n",
              "        [ 0.2418, -0.4951,  0.7971,  ..., -0.0517,  0.3518,  0.1536],\n",
              "        [-1.8498, -0.1302, -0.6559,  ..., -0.3399,  1.0973, -0.7170]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azZhNUH2p9wf"
      },
      "source": [
        "### Initialising `padding` and `unknown` weights to zeros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6O-CpUfnP6m",
        "outputId": "1a0aae4e-9190-4229-d6e0-34ef109d7e29"
      },
      "source": [
        "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "\n",
        "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "print(model.embedding.weight.data)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
            "        ...,\n",
            "        [-0.0232, -0.1614,  0.2054,  ...,  0.5729,  1.6427,  0.1845],\n",
            "        [ 0.2418, -0.4951,  0.7971,  ..., -0.0517,  0.3518,  0.1536],\n",
            "        [-1.8498, -0.1302, -0.6559,  ..., -0.3399,  1.0973, -0.7170]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-56py0kqQ33"
      },
      "source": [
        "### Trainning the Model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvZ8tazdnP3S"
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "criterion = nn.BCEWithLogitsLoss()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHcn1Z1sqjWz"
      },
      "source": [
        "### Pushing the Model and Loss functions to the `device`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2Z6ikftnPxD"
      },
      "source": [
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec9l2kWpqxkR"
      },
      "source": [
        "### The Accuracy Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIMdYGUCnPt1"
      },
      "source": [
        "def accuracy(y_preds, y_true):\n",
        "  #round predictions to the closest integer\n",
        "  rounded_preds = torch.round(torch.sigmoid(y_preds))\n",
        "  correct = (rounded_preds == y_true).float() #convert into float for division \n",
        "  acc = correct.sum() / len(correct)\n",
        "  return acc"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IY_xMz-q9ek"
      },
      "source": [
        "### Trainning and Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsll4uejnPqg"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    model.train()\n",
        "    for batch in iterator:\n",
        "        optimizer.zero_grad()\n",
        "        text = batch.text\n",
        "        predictions = model(text).squeeze(1)\n",
        "        loss = criterion(predictions, batch.label)\n",
        "        acc = accuracy(predictions, batch.label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            text = batch.text\n",
        "            predictions = model(text).squeeze(1)\n",
        "            loss = criterion(predictions, batch.label)\n",
        "            acc = accuracy(predictions, batch.label)\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNQwmcK7rJ9N"
      },
      "source": [
        "We'll also create a function to tell us how long an epoch takes to compare training times between models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75LHSaQznPnY"
      },
      "source": [
        "import time\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Vf6GEIMrZpo"
      },
      "source": [
        "### Running Training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vUqBteEnPkK",
        "outputId": "71f9992b-c539-4707-b1d7-2347a93251b9"
      },
      "source": [
        "N_EPOCHS = 5\n",
        "best_valid_loss = float('inf')\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    end_time = time.time()\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'best-model.pt')\n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.577 | Train Acc: 80.38%\n",
            "\t Val. Loss: 0.410 |  Val. Acc: 81.47%\n",
            "Epoch: 02 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.497 | Train Acc: 84.67%\n",
            "\t Val. Loss: 0.377 |  Val. Acc: 84.42%\n",
            "Epoch: 03 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.428 | Train Acc: 87.32%\n",
            "\t Val. Loss: 0.375 |  Val. Acc: 85.70%\n",
            "Epoch: 04 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.379 | Train Acc: 88.81%\n",
            "\t Val. Loss: 0.385 |  Val. Acc: 86.79%\n",
            "Epoch: 05 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.337 | Train Acc: 89.82%\n",
            "\t Val. Loss: 0.403 |  Val. Acc: 87.44%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3dCBF5Grdsx"
      },
      "source": [
        "### Evaluating the Best Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_K9pRDqHnPhF",
        "outputId": "449e84b2-2333-40b8-c5b9-22ac08ef4b31"
      },
      "source": [
        "model.load_state_dict(torch.load('best-model.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.383 | Test Acc: 85.41%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBe_riqwriwq"
      },
      "source": [
        "### Making predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3vm1WSAnPRh"
      },
      "source": [
        "import spacy\n",
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()\n",
        "def predict_sentiment(model, sentence):\n",
        "    model.eval()\n",
        "    tokenized = generate_bigrams([tok.text for tok in nlp.tokenizer(sentence)])\n",
        "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
        "    tensor = torch.LongTensor(indexed).to(device)\n",
        "    tensor = tensor.unsqueeze(1)\n",
        "    prediction = torch.sigmoid(model(tensor))\n",
        "    return prediction.item()"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1SPOTULr5N4"
      },
      "source": [
        "#### Negative sentiment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2fpPs-Sr7oq",
        "outputId": "5392e58c-bd26-478c-e0e7-ae3bd7625753"
      },
      "source": [
        "predict_sentiment(model, \"This film is terrible\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-IzIttkr8Mt"
      },
      "source": [
        "#### Positive Sentiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c4eJt-pr-t6",
        "outputId": "801e60d0-a212-4f23-dd73-95d2856909fb"
      },
      "source": [
        "predict_sentiment(model, \"This film is great\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.208332384873421e-19"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jm_gGb4bsFnb"
      },
      "source": [
        "### Next Steps\n",
        "* CNN in Sentiment Analyisis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AeTFJIesFHI"
      },
      "source": [
        "### Credits.\n",
        "* [bentrevett](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/3%20-%20Faster%20Sentiment%20Analysis.ipynb)"
      ]
    }
  ]
}