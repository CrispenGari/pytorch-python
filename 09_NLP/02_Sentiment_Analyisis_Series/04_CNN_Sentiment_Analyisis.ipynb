{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04_CNN_Sentiment_Analyisis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fwf45IZpS2SK"
      },
      "source": [
        "### CNN in text classification.\n",
        "``Covulutinal Neural Nets`` are not the best choice for precessing sequencial data as they are mainly meant for processing images. In this Notebook, however we are going to use ``CNN`` in text classification.\n",
        "\n",
        "From the previous Notebook we were doing text classification using the `FastText`. We are not interested in calculating `n-grams` this time as we did from the previous `FastText` sentiment analyis. Instead we are going to include batch dimension since `CNN` expect the ``batch_size``. We can achive this by adding `batch_first = True` in the TEXT field.\n",
        "\n",
        "### Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4JUTVEkSqOZ"
      },
      "source": [
        "import torch\n",
        "from torchtext.legacy import data, datasets\n",
        "import numpy as np\n",
        "import random"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dcsuiCeSz_d"
      },
      "source": [
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9LToR40Sz8w"
      },
      "source": [
        "TEXT = data.Field(tokenize=\"spacy\", tokenizer_language=\"en_core_web_sm\", batch_first=True)\n",
        "LABEL = data.LabelField(dtype=torch.float32)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-BB-UoiSz5y",
        "outputId": "4099fad5-751a-4124-ea7d-efd757895d8b"
      },
      "source": [
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\raclImdb_v1.tar.gz:   0%|          | 0.00/84.1M [00:00<?, ?B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:03<00:00, 27.3MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pESXaLJuSz3R"
      },
      "source": [
        "validation_data, test_data = test_data.split(random_state=random.seed(SEED))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y--7hdXOWcdh"
      },
      "source": [
        "### Checking how many examples for each set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWi8cEMrSz0Z",
        "outputId": "ecb46013-8af6-476f-9734-ee7e9dfcb193"
      },
      "source": [
        "print(f\"TRAINING: \\t {len(train_data)}\")\n",
        "print(f\"TESTING: \\t {len(test_data)}\")\n",
        "print(f\"VALIDATION: \\t {len(validation_data)}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAINING: \t 25000\n",
            "TESTING: \t 7500\n",
            "VALIDATION: \t 17500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzkBLPuLW6IT"
      },
      "source": [
        "### Building a Vocabulary Using pretrained Word Vectors.\n",
        "* We are going to use `glove.6B.100d` vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2eydYh5Szuq",
        "outputId": "ce305945-8638-49ce-eed7-e42f441bddba"
      },
      "source": [
        "MAX_VOCAB_SIZE = 25_000\n",
        "TEXT.build_vocab(\n",
        "    train_data,\n",
        "    max_size = MAX_VOCAB_SIZE,\n",
        "    vectors= \"glove.6B.100d\",\n",
        "    unk_init = torch.Tensor.normal_\n",
        ")\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:42, 5.30MB/s]                           \n",
            " 99%|█████████▉| 397350/400000 [00:13<00:00, 28484.01it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zd6d_lqjXw4Y"
      },
      "source": [
        "### Creating Iterators.\n",
        "We are going to use the `BucketIterator` to create our iterators."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62PzeuRdSzsN"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "train_iterators, validation_iterators, test_iterators = data.BucketIterator.splits(\n",
        "    (train_data, validation_data, test_data),\n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device\n",
        ")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Te86XTbCYa8B"
      },
      "source": [
        "### Building a `CNN` text classifier.\n",
        "\n",
        "#### How `CNN` are used for text.\n",
        "Images are typically 2 dimensional (we'll ignore the fact that there is a third \"colour\" dimension for now) whereas text is 1 dimensional. However, we know that the first step in almost all of our previous tutorials (and pretty much all NLP pipelines) is converting the words into word embeddings. This is how we can visualize our words in 2 dimensions, each word along one axis and the elements of vectors aross the other dimension. Consider the 2 dimensional representation of the embedded sentence below:\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/bentrevett/pytorch-sentiment-analysis/raw/2b666b3cba7d629a2f192c7d9c66fadcc9f0c363/assets/sentiment9.png\"/>\n",
        "</p>\n",
        "\n",
        "We can then use a filter that is **[``n`` ``x`` ``emb_dim``]**. This will cover **$n$** sequential words entirely, as their width will be emb_dim dimensions. Consider the image below, with our word vectors are represented in green. Here we have 4 words with 5 dimensional embeddings, creating a **``[4x5]``** ``imag`` tensor. A filter that covers two words at a time (i.e. bi-grams) will be **``[2x5]``** filter, shown in yellow, and each element of the filter with have a weight associated with it. The output of this filter (shown in red) will be a single real number that is the weighted sum of all elements covered by the filter.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/bentrevett/pytorch-sentiment-analysis/raw/2b666b3cba7d629a2f192c7d9c66fadcc9f0c363/assets/sentiment12.png\"/>\n",
        "</p>\n",
        "\n",
        "The filter then moves ``down`` the image (or across the sentence) to cover the next ``bi-gram`` and another output (weighted sum) is calculated and so on.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/bentrevett/pytorch-sentiment-analysis/raw/2b666b3cba7d629a2f192c7d9c66fadcc9f0c363/assets/sentiment13.png\"/>\n",
        "  <img src=\"https://github.com/bentrevett/pytorch-sentiment-analysis/raw/2b666b3cba7d629a2f192c7d9c66fadcc9f0c363/assets/sentiment14.png\"/>\n",
        "</p>\n",
        "\n",
        "In our case (and in the general case where the width of the filter equals the width of the \"image\"), our output will be a vector with number of elements equal to the height of the image (or lenth of the word) minus the height of the filter plus one, $4-2+1=3$ in this case.\n",
        "\n",
        "This example showed how to calculate the output of one filter. Our model (and pretty much all CNNs) will have lots of these filters. The idea is that each filter will learn a different feature to extract. In the above example, we are hoping each of the [2 x emb_dim] filters will be looking for the occurence of different bi-grams.\n",
        "\n",
        "In our model, we will also have different sizes of filters, heights of 3, 4 and 5, with 100 of each of them. The intuition is that we will be looking for the occurence of different tri-grams, 4-grams and 5-grams that are relevant for analysing sentiment of movie reviews.\n",
        "\n",
        "The next step in our model is to use pooling (specifically max pooling) on the output of the convolutional layers. This is similar to the FastText model where we performed the average over each of the word vectors, implemented by the F.avg_pool2d function, however instead of taking the average over a dimension, we are taking the maximum value over a dimension. Below an example of taking the maximum value (0.9) from the output of the convolutional layer on the example sentence (not shown is the activation function applied to the output of the convolutions).\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/bentrevett/pytorch-sentiment-analysis/raw/2b666b3cba7d629a2f192c7d9c66fadcc9f0c363/assets/sentiment15.png\"/>\n",
        "</p>\n",
        "\n",
        "The idea here is that the maximum value is the \"most important\" feature for determining the sentiment of the review, which corresponds to the \"most important\" ``n-gram`` within the review. \n",
        "\n",
        "### How do we know what the \"most important\" ``n-gram`` is? \n",
        "Luckily, we don't have to! Through backpropagation, the weights of the filters are changed so that whenever certain n-grams that are highly indicative of the sentiment are seen, the output of the filter is a \"high\" value. This \"high\" value then passes through the max pooling layer if it is the maximum value in the output.\n",
        "\n",
        "As our model has 100 filters of 3 different sizes, that means we have 300 different n-grams the model thinks are important. We concatenate these together into a single vector and pass them through a linear layer to predict the sentiment. We can think of the weights of this linear layer as \"weighting up the evidence\" from each of the 300 ``n-grams`` and making a final decision.\n",
        "\n",
        "### Model Implementation.\n",
        "We implement the convolutional layers with ``nn.Conv2d``. The ``in_channels`` argument is the number of \"channels\" in your image going into the convolutional layer. In actual images this is usually ``3`` (one channel for each of the red, blue and green channels), however when using text we only have a **single** channel, the text itself. The ``out_channels`` is the number of filters and the ``kernel_size`` is the size of the filters. Each of our ``kernel_sizes`` is going to be ``[n x emb_dim]`` where $n$ is the size of the n-grams.\n",
        "\n",
        "In PyTorch, RNNs want the input with the batch dimension second, whereas ``CNNs`` want the batch dimension first - we do not have to permute the data here as we have already set ``batch_first = True`` in our ``TEXT`` field. We then pass the sentence through an embedding layer to get our embeddings. The second dimension of the input into a ``nn.Conv2d`` layer must be the channel dimension. As text technically does not have a channel dimension, we ``unsqueeze`` our tensor to create one. This matches with our ``in_channels=1`` in the initialization of our convolutional layers.\n",
        "\n",
        "We then pass the tensors through the convolutional and pooling layers, using the ReLU activation function after the convolutional layers. Another nice feature of the pooling layers is that they handle sentences of different lengths. The size of the output of the convolutional layer is dependent on the size of the input to it, and different batches contain sentences of different lengths. Without the max pooling layer the input to our linear layer would depend on the size of the input sentence (not what we want). One option to rectify this would be to trim/pad all sentences to the same length, however with the max pooling layer we always know the input to the linear layer will be the total number of filters. Note: there an exception to this if your sentence(s) are shorter than the largest filter used.\n",
        " \n",
        "Note: **You will then have to pad your sentences to the length of the largest filter. In the IMDb data there are no reviews only 5 words long so we don't have to worry about that, but you will if you are using your own data.**\n",
        "\n",
        "Finally, we perform dropout on the concatenated filter outputs and then pass them through a linear layer to make our predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aTJeAEtSzpS"
      },
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn import  functional as F"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbJeC57rhJLt"
      },
      "source": [
        "### CNN with different filters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qV7rWdAOSzjx"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_size, n_filters, filter_sizes, output_size, \n",
        "                 dropout, pad_idx):\n",
        "    super(CNN, self).__init__()\n",
        "\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim=embedding_size, padding_idx=pad_idx)\n",
        "    self.conv_0 = nn.Conv2d(in_channels = 1, \n",
        "                        out_channels = n_filters, \n",
        "                        kernel_size = (filter_sizes[0], embedding_size))\n",
        "    self.conv_1 = nn.Conv2d(in_channels = 1, \n",
        "                        out_channels = n_filters, \n",
        "                        kernel_size = (filter_sizes[1], embedding_size))\n",
        "    self.conv_2 = nn.Conv2d(in_channels = 1, \n",
        "                        out_channels = n_filters, \n",
        "                        kernel_size = (filter_sizes[2], embedding_size))\n",
        "    self.fc = nn.Linear(len(filter_sizes) * n_filters, output_size)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    #text = [batch size, sent len]\n",
        "    embedded = self.embedding(text)  \n",
        "    #embedded = [batch size, sent len, emb dim\n",
        "    embedded = embedding.unsqueeze(1)\n",
        "\n",
        "    #embedded = [batch size, 1, sent len, emb dim]\n",
        "    conved_0 = F.relu(self.conv_0(embedded).squeeze(3))\n",
        "    conved_1 = F.relu(self.conv_1(embedded).squeeze(3))\n",
        "    conved_2 = F.relu(self.conv_2(embedded).squeeze(3))\n",
        "        \n",
        "    # conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
        "    pooled_0 = F.max_pool1d(conved_0, conved_0.shape[2]).squeeze(2)\n",
        "    pooled_1 = F.max_pool1d(conved_1, conved_1.shape[2]).squeeze(2)\n",
        "    pooled_2 = F.max_pool1d(conved_2, conved_2.shape[2]).squeeze(2)\n",
        "\n",
        "    #pooled_n = [batch size, n_filters]\n",
        "    cat = self.dropout(torch.cat((pooled_0, pooled_1, pooled_2), dim = 1))\n",
        "    #cat = [batch size, n_filters * len(filter_sizes)]\n",
        "    return self.fc(cat)\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOWyvqV-hIJn"
      },
      "source": [
        "### A generic `CNN` that takes any number of filters.\n",
        "\n",
        "We do this by placing all of our convolutional layers in a ``nn.ModuleList``.\n",
        "\n",
        "### What is ``nn.ModuleList``?\n",
        "Is a function used to hold a list of PyTorch ``nn.Modules.`` If we simply used a standard Python list, the modules within the list cannot be \"seen\" by any modules outside the list which will cause us some errors.\n",
        "\n",
        "We can now pass an arbitrary sized list of filter sizes and the list comprehension will create a convolutional layer for each of them. Then, in the forward method we iterate through the list applying each convolutional layer to get a list of convolutional outputs, which we also feed through the max pooling in a list comprehension before concatenating together and passing through the dropout and linear layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISVFgfT9h5td"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_size, n_filters, filter_sizes, output_size, \n",
        "            dropout, pad_idx):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_size, padding_idx = pad_idx)\n",
        "    self.convs = nn.ModuleList([\n",
        "                                nn.Conv2d(in_channels = 1, \n",
        "                                          out_channels = n_filters, \n",
        "                                          kernel_size = (fs, embedding_size)) \n",
        "                                for fs in filter_sizes\n",
        "                                ])\n",
        "    self.fc = nn.Linear(len(filter_sizes) * n_filters, output_size)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, text):  \n",
        "    #text = [batch size, sent len]\n",
        "    embedded = self.embedding(text)    \n",
        "    #embedded = [batch size, sent len, emb dim]\n",
        "    embedded = embedded.unsqueeze(1)\n",
        "    #embedded = [batch size, 1, sent len, emb dim]\n",
        "\n",
        "    conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
        "    #conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
        "\n",
        "    pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
        "    #pooled_n = [batch size, n_filters]\n",
        "    cat = self.dropout(torch.cat(pooled, dim = 1))\n",
        "    #cat = [batch size, n_filters * len(filter_sizes)]  \n",
        "    return self.fc(cat)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7BFxqUfjQHN"
      },
      "source": [
        "### Conv1d\n",
        "We can also implement the above model using ``1-dimensional `` convolutional layers, where the embedding dimension is the \"depth\" of the filter and the number of tokens in the sentence is the width."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3x3RJRGjXpb"
      },
      "source": [
        "class CNN1d(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
        "            dropout, pad_idx):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
        "    self.convs = nn.ModuleList([\n",
        "                                nn.Conv1d(in_channels = embedding_dim, \n",
        "                                          out_channels = n_filters, \n",
        "                                          kernel_size = fs)\n",
        "                                for fs in filter_sizes\n",
        "                                ])\n",
        "    self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "  def forward(self, text):\n",
        "    #text = [batch size, sent len]\n",
        "    embedded = self.embedding(text)  \n",
        "    #embedded = [batch size, sent len, emb dim]\n",
        "    embedded = embedded.permute(0, 2, 1)\n",
        "    #embedded = [batch size, emb dim, sent len]\n",
        "    conved = [F.relu(conv(embedded)) for conv in self.convs]\n",
        "    #conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
        "    pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
        "    #pooled_n = [batch size, n_filters]\n",
        "    cat = self.dropout(torch.cat(pooled, dim = 1))\n",
        "    #cat = [batch size, n_filters * len(filter_sizes)]\n",
        "    return self.fc(cat)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vdwBl9nfFb2"
      },
      "source": [
        "\n",
        "### Hyper Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1_VZx0KkHUh"
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "N_FILTERS = 100\n",
        "FILTER_SIZES = [3, 4, 5]\n",
        "OUTPUT_DIM = 1\n",
        "DROPOUT = 0.5\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPPFBlj5kMfx"
      },
      "source": [
        "### CNN `2D` instance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eW37c4OWSzg5"
      },
      "source": [
        "conv_2d_model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Lx-r3_4k-ix"
      },
      "source": [
        "### CNN `1D` instance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mGrNJXuk9pd"
      },
      "source": [
        "conv_1d_model = CNN1d(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9tPO973kaZ-"
      },
      "source": [
        "### Checking Trainable Parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODpLvxcOddMW",
        "outputId": "ff717ab9-bcce-4647-e90d-c13a1a57ada3"
      },
      "source": [
        "def count_trainable_params(model):\n",
        "  return sum([p.numel() for p in model.parameters() if p.requires_grad])\n",
        "\n",
        "print(f'The model (CNN2D) has  {count_trainable_params(conv_2d_model):,} trainable parameters')\n",
        "print(f'The model (CNN1D) has  {count_trainable_params(conv_1d_model):,} trainable parameters')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model (CNN2D) has  2,620,801 trainable parameters\n",
            "The model (CNN1D) has  2,620,801 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s25k31WIddKF"
      },
      "source": [
        "### Trainning the Model.\n",
        "First let's train the `Conv2d` model first and then we will move on and train the `Conv1D`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4z2zCt4ltTW"
      },
      "source": [
        "### Loading pretrained embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjrTN_0Bl9_s"
      },
      "source": [
        "pretrained_embeddings = TEXT.vocab.vectors"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ieWluCHddGq",
        "outputId": "817bd30b-c24c-449c-f41c-cc9d1498ee32"
      },
      "source": [
        "conv_2d_model.embedding.weight.data.copy_(pretrained_embeddings)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.9269,  1.4873,  0.9007,  ...,  0.1233,  0.3499,  0.6173],\n",
              "        [ 0.7262,  0.0912, -0.3891,  ...,  0.0821,  0.4440, -0.7240],\n",
              "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
              "        ...,\n",
              "        [ 0.6612,  0.4606,  0.7589,  ...,  1.0253, -0.7948,  0.7347],\n",
              "        [ 0.3369, -1.2521, -0.7555,  ..., -1.7327, -0.9087,  0.3905],\n",
              "        [ 1.0300,  0.0859,  1.1354,  ..., -0.4458,  2.0626, -3.2186]],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehv4lLfPmTUz"
      },
      "source": [
        "### Zero the initial weights of the ``unknown`` and ``padding `` tokens.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "RsjA0PbNddD-",
        "outputId": "4aa7bfff-52e5-42ec-c411-5fee1547ab9b"
      },
      "source": [
        "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "for i in range(2):\n",
        "  conv_2d_model.embedding.weight.data[i] = torch.zeros(EMBEDDING_DIM)\n",
        "\n",
        "\"\"\" same as doing it this way\n",
        "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "\"\"\""
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' same as doing it this way\\nUNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\\nmodel.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\\nmodel.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_72oEim1ddAt",
        "outputId": "85285b40-0c83-48ac-ca3d-85f6265953ca"
      },
      "source": [
        "print(conv_2d_model.embedding.weight.data)"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
            "        ...,\n",
            "        [ 0.6612,  0.4606,  0.7589,  ...,  1.0253, -0.7948,  0.7347],\n",
            "        [ 0.3369, -1.2521, -0.7555,  ..., -1.7327, -0.9087,  0.3905],\n",
            "        [ 1.0300,  0.0859,  1.1354,  ..., -0.4458,  2.0626, -3.2186]],\n",
            "       device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ym0eqMjRnVO7"
      },
      "source": [
        "### Trainning the `Conv2D` model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5yNeur4dc9_"
      },
      "source": [
        "optimizer = torch.optim.Adam(conv_2d_model.parameters())\n",
        "criterion = nn.BCEWithLogitsLoss()"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2TxBsbhnoCb"
      },
      "source": [
        "### Pushing loss function and model to the device."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOYJRMlCdc64"
      },
      "source": [
        "\n",
        "conv_2d_model = conv_2d_model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYRukkBZnzY8"
      },
      "source": [
        "### Accuracy function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1bzBZNudc4J"
      },
      "source": [
        "def accuracy(y_preds, y_true):\n",
        "  #round predictions to the closest integer\n",
        "  rounded_preds = torch.round(torch.sigmoid(y_preds))\n",
        "  correct = (rounded_preds == y_true).float() #convert into float for division \n",
        "  acc = correct.sum() / len(correct)\n",
        "  return acc"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4nOKDCun9-v"
      },
      "source": [
        "### Training and Evaluation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-BzyAuQdc1A"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    model.train()\n",
        "    for batch in iterator:\n",
        "        optimizer.zero_grad()\n",
        "        text = batch.text\n",
        "        predictions = model(text).squeeze(1)\n",
        "        loss = criterion(predictions, batch.label)\n",
        "        acc = accuracy(predictions, batch.label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            text = batch.text\n",
        "            predictions = model(text).squeeze(1)\n",
        "            loss = criterion(predictions, batch.label)\n",
        "            acc = accuracy(predictions, batch.label)\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vy6axdtfoSbZ"
      },
      "source": [
        "\n",
        "We'll also create a function to tell us how long an epoch takes to compare training times between models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIMciQdFdcyC"
      },
      "source": [
        "import time\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFyDOw26dcsZ",
        "outputId": "87ed3f96-c5de-49ee-b986-cae177221b5a"
      },
      "source": [
        "N_EPOCHS = 5\n",
        "best_valid_loss = float('inf')\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss, train_acc = train(conv_2d_model, train_iterators, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(conv_2d_model, validation_iterators, criterion)\n",
        "    end_time = time.time()\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(conv_2d_model.state_dict(), 'best-conv-2d-model.pt')\n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 31s\n",
            "\tTrain Loss: 0.440 | Train Acc: 79.64%\n",
            "\t Val. Loss: 0.368 |  Val. Acc: 83.22%\n",
            "Epoch: 02 | Epoch Time: 0m 30s\n",
            "\tTrain Loss: 0.263 | Train Acc: 89.08%\n",
            "\t Val. Loss: 0.351 |  Val. Acc: 84.52%\n",
            "Epoch: 03 | Epoch Time: 0m 30s\n",
            "\tTrain Loss: 0.195 | Train Acc: 92.40%\n",
            "\t Val. Loss: 0.357 |  Val. Acc: 84.89%\n",
            "Epoch: 04 | Epoch Time: 0m 30s\n",
            "\tTrain Loss: 0.148 | Train Acc: 94.36%\n",
            "\t Val. Loss: 0.359 |  Val. Acc: 85.26%\n",
            "Epoch: 05 | Epoch Time: 0m 30s\n",
            "\tTrain Loss: 0.114 | Train Acc: 95.77%\n",
            "\t Val. Loss: 0.376 |  Val. Acc: 85.62%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgdF0UFqCjK"
      },
      "source": [
        "### Evaluating the `Conv2d` model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyXABGbfqCQy",
        "outputId": "61b9c22f-fc4b-4e06-fde0-07c2d6739ba4"
      },
      "source": [
        "conv_2d_model.load_state_dict(torch.load('best-conv-2d-model.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(conv_2d_model, test_iterators, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.355 | Test Acc: 84.59%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uehSLtvo67q"
      },
      "source": [
        "### Training the `conv1D` model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4PHSLzedcpl",
        "outputId": "a3d6c594-15ec-43b5-8a0c-a0114e084fde"
      },
      "source": [
        "conv_1d_model.embedding.weight.data.copy_(pretrained_embeddings)\n",
        "for i in range(2):\n",
        "  conv_1d_model.embedding.weight.data[i] = torch.zeros(EMBEDDING_DIM)\n",
        "\n",
        "print(conv_1d_model.embedding.weight.data)"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
            "        ...,\n",
            "        [ 0.6612,  0.4606,  0.7589,  ...,  1.0253, -0.7948,  0.7347],\n",
            "        [ 0.3369, -1.2521, -0.7555,  ..., -1.7327, -0.9087,  0.3905],\n",
            "        [ 1.0300,  0.0859,  1.1354,  ..., -0.4458,  2.0626, -3.2186]],\n",
            "       device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJZH9Q2edcm5"
      },
      "source": [
        "optimizer = torch.optim.Adam(conv_1d_model.parameters())\n",
        "criterion = nn.BCEWithLogitsLoss()"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDq3D_CGdcj2"
      },
      "source": [
        "conv_1d_model = conv_1d_model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KfO5fmgdcgj",
        "outputId": "74fe363e-fc01-481c-e667-76cb63b6ba79"
      },
      "source": [
        "N_EPOCHS = 5\n",
        "best_valid_loss = float('inf')\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss, train_acc = train(conv_1d_model, train_iterators, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(conv_1d_model, validation_iterators, criterion)\n",
        "    end_time = time.time()\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(conv_1d_model.state_dict(), 'best-conv-1d-model.pt')\n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.256 | Train Acc: 89.44%\n",
            "\t Val. Loss: 0.374 |  Val. Acc: 83.80%\n",
            "Epoch: 02 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.172 | Train Acc: 93.51%\n",
            "\t Val. Loss: 0.371 |  Val. Acc: 84.54%\n",
            "Epoch: 03 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.134 | Train Acc: 95.09%\n",
            "\t Val. Loss: 0.384 |  Val. Acc: 84.83%\n",
            "Epoch: 04 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.107 | Train Acc: 96.13%\n",
            "\t Val. Loss: 0.413 |  Val. Acc: 84.23%\n",
            "Epoch: 05 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.085 | Train Acc: 97.03%\n",
            "\t Val. Loss: 0.405 |  Val. Acc: 85.43%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fM2QUqV0qmx2"
      },
      "source": [
        "### Evaluation the `conv1d` model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IoBHTnKql4-",
        "outputId": "fb1b00ec-676c-43ae-a737-3885dcded748"
      },
      "source": [
        "conv_1d_model.load_state_dict(torch.load('best-conv-1d-model.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(conv_1d_model, test_iterators, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.370 | Test Acc: 84.96%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gf46QNedrCls"
      },
      "source": [
        "### User Input.\n",
        "\n",
        "**Note:** As mentioned in the implementation details, the input sentence has to be at least as long as the largest filter height used. We modify our ``predict_sentiment`` function to also accept a minimum length argument. If the tokenized input sentence is less than ``min_len`` tokens, we append padding tokens ``(<pad>)`` to make it ``min_len`` tokens.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yzi_Q-YJdcae"
      },
      "source": [
        "import spacy\n",
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()\n",
        "\n",
        "def predict_sentiment(model, sentence, min_len = 5):\n",
        "    model.eval()\n",
        "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
        "    if len(tokenized) < min_len:\n",
        "        tokenized += ['<pad>'] * (min_len - len(tokenized))\n",
        "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
        "    tensor = torch.LongTensor(indexed).to(device)\n",
        "    tensor = tensor.unsqueeze(0)\n",
        "    prediction = torch.sigmoid(model(tensor))\n",
        "    return prediction.item()"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_-jqoxysjNm"
      },
      "source": [
        "### Negative Review."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faf4wXjbstQh",
        "outputId": "d6fcb266-6e2b-4d40-9312-d74eab7f1aab"
      },
      "source": [
        "predict_sentiment(conv_2d_model, \"This film is terrible\")"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0939841940999031"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czs5cXIMsyoh",
        "outputId": "540a6bd0-96d0-4eef-abd8-fcccd8dd0538"
      },
      "source": [
        "predict_sentiment(conv_1d_model, \"This film is terrible\")"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.25023776292800903"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGYjo5rYsctz"
      },
      "source": [
        "## Positive Review"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHpc5g7cs90i",
        "outputId": "1f4c1c22-56c1-4205-c965-7daa34517881"
      },
      "source": [
        "predict_sentiment(conv_2d_model, \"This film is great\")"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9003241062164307"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JA6E_Yxms9eq",
        "outputId": "533a4d22-d525-4f27-d908-9bedde231b9e"
      },
      "source": [
        "predict_sentiment(conv_1d_model, \"This film is great\")"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9770062565803528"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WR5OCOcPtEmF",
        "outputId": "c899af13-86b1-48d5-cf76-026958a4152f"
      },
      "source": [
        "print(LABEL.vocab.stoi)"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "defaultdict(None, {'neg': 0, 'pos': 1})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0UNzn3XtVBK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBBsmcOKujAa"
      },
      "source": [
        "### Credits\n",
        "* [bentrevett](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/4%20-%20Convolutional%20Sentiment%20Analysis.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPeV0etxus4g"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}