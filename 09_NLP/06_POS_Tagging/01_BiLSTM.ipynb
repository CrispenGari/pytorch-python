{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_BiLSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaFpM1xf-9JC"
      },
      "source": [
        "---\n",
        "Author: **`Crispen Gari`**\n",
        "\n",
        "Date: **`2020-09-08`**\n",
        "\n",
        "Topic: **`Part of Speech Tagging (PoS)Tagging`**\n",
        "\n",
        "Library: **`Pytorch`**\n",
        "\n",
        "Language: **`Python`**\n",
        "\n",
        "Main: **`Natural Language Processing (NLP)`**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZ4Dk-g77jhQ"
      },
      "source": [
        "### BiLSTM for POS Tagging\n",
        "\n",
        "In this notebook we are going to create a model that will do what we call the **P**art **o**f **S**peech Tagging (PoS) tagging. We will take in a sequence of tokens, or a sentence and the the model will the predict the POS tag for each word. This can also be used for named entity recognition (NER), where the output for each token will be what type of entity, if any, the token is.\n",
        "\n",
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6Bjyb8nW7jXv",
        "outputId": "10cece13-33fc-48d5-8cb0-687e50eb64ee"
      },
      "source": [
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from torch import nn\n",
        "\n",
        "from torchtext.legacy import data, datasets\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "import time, os, random\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "torch.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.9.0+cu102'"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "he6GA9b68yON"
      },
      "source": [
        "### Seeds and Device"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-7jIl3q7WZb",
        "outputId": "5b85aa1f-6d66-442c-e8f6-736f7424c10e"
      },
      "source": [
        "SEED = 42\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEKXakFZDQjg"
      },
      "source": [
        "### Helper function that will visualise data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iJQQNQsDWiP"
      },
      "source": [
        "def tabulate(column_names, data, title=\"VISUALIZING SETS EXAMPLES\"):\n",
        "  table = PrettyTable(column_names)\n",
        "  table.title= title\n",
        "  for row in data:\n",
        "    table.add_row(row)\n",
        "  print(table)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lv7M-tbb9jWB"
      },
      "source": [
        "### Data preparation\n",
        "\n",
        "The dataset that we are working with has two tags:\n",
        "1. [universal dependency (UD) tags](https://universaldependencies.org/u/pos/)\n",
        "2. [Penn Treebank (PTB) tags](https://www.sketchengine.eu/penn-treebank-tagset/)\n",
        "\n",
        "In this example we will be working with UD tags but for demostration purposes we are also going to load the PTB tags.\n",
        "\n",
        "We will have unknown tokens in the dataset which are tokens that are not found in the vocabulary. However, we wont have unknown tags in the dataset so we will change the default value of unknown in torch text from `<unk>` to `None`\n",
        "\n",
        "### Fields"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYCLuyOb7jN-"
      },
      "source": [
        "TEXT = data.Field(lower=True)\n",
        "UD_TAGS = data.Field(unk_token = None)\n",
        "PTB_TAGS = data.Field(unk_token = None)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvUWgn6IAmmI"
      },
      "source": [
        "We then define the field which handles passing our fields to the dataset. Note that the order matters, if we were loading PTB tags we will do it as follows:\n",
        "\n",
        "```py\n",
        "fields = ((\"text\", TEXT), (None, None), (\"ptbtags\", PTB_TAGS))\n",
        "```\n",
        "\n",
        "The tuple (None, None) tells torchtext that we must not load those fields"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xXi1dL_7jK0"
      },
      "source": [
        "fields = (\n",
        "    (\"text\", TEXT),\n",
        "    (\"udtags\", UD_TAGS),\n",
        "    (\"ptbtags\", PTB_TAGS)\n",
        ")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrYWlx76BiHU"
      },
      "source": [
        "Now we will load our data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-sCA5Xe7jH9"
      },
      "source": [
        "train_data, valid_data, test_data = datasets.UDPOS.splits(fields)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJV4nsxKC-g1"
      },
      "source": [
        "Checking examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aVnSlcQ7jFD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69b96b8d-85e6-43e1-e12a-02f40269dba7"
      },
      "source": [
        "column_names = [\"SUBSET\", \"EXAMPLE(s)\"]\n",
        "row_data = [\n",
        "        [\"training\", f\"{len(train_data):,}\"],\n",
        "        ['validation', f\"{len(valid_data):,}\"],\n",
        "        ['test', f\"{len(test_data):,}\"]\n",
        "]\n",
        "tabulate(column_names, row_data)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------+\n",
            "|  VISUALIZING SETS EXAMPLES  |\n",
            "+--------------+--------------+\n",
            "|    SUBSET    |  EXAMPLE(s)  |\n",
            "+--------------+--------------+\n",
            "|   training   |    12,543    |\n",
            "|  validation  |    2,002     |\n",
            "|     test     |    2,077     |\n",
            "+--------------+--------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQktN050D9sT"
      },
      "source": [
        "Printing examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0F-gQLT7jBu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9a9afd4-d290-4574-9651-5300a68a7089"
      },
      "source": [
        "print(vars(train_data.examples[0]))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': ['al', '-', 'zaman', ':', 'american', 'forces', 'killed', 'shaikh', 'abdullah', 'al', '-', 'ani', ',', 'the', 'preacher', 'at', 'the', 'mosque', 'in', 'the', 'town', 'of', 'qaim', ',', 'near', 'the', 'syrian', 'border', '.'], 'udtags': ['PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'ADJ', 'NOUN', 'VERB', 'PROPN', 'PROPN', 'PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'PROPN', 'PUNCT', 'ADP', 'DET', 'ADJ', 'NOUN', 'PUNCT'], 'ptbtags': ['NNP', 'HYPH', 'NNP', ':', 'JJ', 'NNS', 'VBD', 'NNP', 'NNP', 'NNP', 'HYPH', 'NNP', ',', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'NNP', ',', 'IN', 'DT', 'JJ', 'NN', '.']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6AGhLOJ7i-z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78945f42-7309-469d-cd16-b7a369177416"
      },
      "source": [
        "print(vars(train_data.examples[0]).get(\"text\"))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['al', '-', 'zaman', ':', 'american', 'forces', 'killed', 'shaikh', 'abdullah', 'al', '-', 'ani', ',', 'the', 'preacher', 'at', 'the', 'mosque', 'in', 'the', 'town', 'of', 'qaim', ',', 'near', 'the', 'syrian', 'border', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyvw_qk17i7o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "daf28945-3657-49b5-8b70-19ac96de331d"
      },
      "source": [
        "print(vars(train_data.examples[0]).get(\"udtags\"))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'ADJ', 'NOUN', 'VERB', 'PROPN', 'PROPN', 'PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'PROPN', 'PUNCT', 'ADP', 'DET', 'ADJ', 'NOUN', 'PUNCT']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "komUUHd-7i4n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a2300b7-482d-46d6-a108-e9f7b064c6bd"
      },
      "source": [
        "print(vars(train_data.examples[0]).get(\"ptbtags\"))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NNP', 'HYPH', 'NNP', ':', 'JJ', 'NNS', 'VBD', 'NNP', 'NNP', 'NNP', 'HYPH', 'NNP', ',', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'NNP', ',', 'IN', 'DT', 'JJ', 'NN', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQedH9lMETga"
      },
      "source": [
        "### Vocabulary mapping\n",
        "Next we will build a maping of tokens to integers. We are going to set the `min_freq` to 2 so that the tokens that appears less that 2 times in the corpus will be automatically converted to `<unk>`\n",
        "\n",
        "WE are also going to load the Glove pretrainded word embedding. Using pretrained word vectors lead to impovement in the performance of the model. In our case we are going to use the `glove.6B.100d` which was trained with about 6 billion words and each word is `100d` vector of numbers.\n",
        "\n",
        "_`unk_init`_ is used to initialize the token embeddings which are not in the pre-trained embedding vocabulary. By default this sets those embeddings to zeros, however it is better to not have them all initialized to the same value, so we initialize them from a **Normal/Gaussian** distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6iZKN047i14"
      },
      "source": [
        "MIN_FREQ = 2\n",
        "TEXT.build_vocab(train_data, min_freq=MIN_FREQ,\n",
        "                 vectors = \"glove.6B.100d\",\n",
        "                 unk_init = torch.Tensor.normal_\n",
        "                 )\n",
        "\n",
        "UD_TAGS.build_vocab(train_data)\n",
        "PTB_TAGS.build_vocab(train_data)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2wR16IKFvkF"
      },
      "source": [
        "Let's check how many tokens are in our vobabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qdd8qs87iyU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d495eda-de78-499e-ae0e-be9513a6377d"
      },
      "source": [
        "column_names = [\"FIELD\", \"TOKEN(s)\"]\n",
        "row_data = [\n",
        "        [\"TEXT\", f\"{len(TEXT.vocab):,}\"],\n",
        "        ['UD_TAGS', f\"{len(UD_TAGS.vocab):,}\"],\n",
        "        ['PTB_TAGS', f\"{len(PTB_TAGS.vocab):,}\"]\n",
        "]\n",
        "tabulate(column_names, row_data, title=\"VOCABULARY SIZES\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------+\n",
            "|   VOCABULARY SIZES  |\n",
            "+----------+----------+\n",
            "|  FIELD   | TOKEN(s) |\n",
            "+----------+----------+\n",
            "|   TEXT   |  8,866   |\n",
            "| UD_TAGS  |    18    |\n",
            "| PTB_TAGS |    51    |\n",
            "+----------+----------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Z9yBfhhGSfa"
      },
      "source": [
        "Checking the most common words in the vocabulary..\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKYXNhg07iwJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18c70865-34d4-4ebc-bc86-8bc8a21c0a42"
      },
      "source": [
        "print(TEXT.vocab.freqs.most_common(10))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('the', 9076), ('.', 8640), (',', 7021), ('to', 5137), ('and', 5002), ('a', 3782), ('of', 3622), ('i', 3379), ('in', 3112), ('is', 2239)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qu1sREX6GnpB"
      },
      "source": [
        "We can check the vocabularies of our tags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3mZMLHEGnKB",
        "outputId": "87fa13ac-48c9-4abc-f590-054bb19e8320"
      },
      "source": [
        "print(UD_TAGS.vocab.itos)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<pad>', 'NOUN', 'PUNCT', 'VERB', 'PRON', 'ADP', 'DET', 'PROPN', 'ADJ', 'AUX', 'ADV', 'CCONJ', 'PART', 'NUM', 'SCONJ', 'X', 'INTJ', 'SYM']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tKO6CX87isb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "810205a2-8566-4900-94f6-813a62a20ba5"
      },
      "source": [
        "print(PTB_TAGS.vocab.itos)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<pad>', 'NN', 'IN', 'DT', 'NNP', 'PRP', 'JJ', 'RB', '.', 'VB', 'NNS', ',', 'CC', 'VBD', 'VBP', 'VBZ', 'CD', 'VBN', 'VBG', 'MD', 'TO', 'PRP$', '-RRB-', '-LRB-', 'WDT', 'WRB', ':', '``', \"''\", 'WP', 'RP', 'UH', 'POS', 'HYPH', 'JJR', 'NNPS', 'JJS', 'EX', 'NFP', 'GW', 'ADD', 'RBR', '$', 'PDT', 'RBS', 'SYM', 'LS', 'FW', 'AFX', 'WP$', 'XX']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHAqjoDrG2Mb"
      },
      "source": [
        "Checking most common tags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiATfj2R7iqf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfc1f254-c0e4-4591-d706-c16a45f627d7"
      },
      "source": [
        "print(UD_TAGS.vocab.freqs.most_common(10))\n",
        "print(PTB_TAGS.vocab.freqs.most_common(10))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('NOUN', 34781), ('PUNCT', 23679), ('VERB', 23081), ('PRON', 18577), ('ADP', 17638), ('DET', 16285), ('PROPN', 12946), ('ADJ', 12477), ('AUX', 12343), ('ADV', 10548)]\n",
            "[('NN', 26915), ('IN', 20724), ('DT', 16817), ('NNP', 12449), ('PRP', 12193), ('JJ', 11591), ('RB', 10831), ('.', 10317), ('VB', 9476), ('NNS', 8438)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETWKyw4vHO_q"
      },
      "source": [
        "We can also check tag percentages that are in our training data set as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Zh4KHhv7imo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88f7fa43-99f0-40e6-fcc6-92e4de4d6cbc"
      },
      "source": [
        "def tabulate_percentage(column_names, data, title=\"TAGS STATISTICS\"):\n",
        "  table = PrettyTable(column_names)\n",
        "  table.title= title\n",
        "  table.align[column_names[0]] = 'l'\n",
        "  table.align[column_names[1]] = 'r'\n",
        "  table.align[column_names[2]] = 'r'\n",
        "  for row in data:\n",
        "    table.add_row(row)\n",
        "  print(table)\n",
        "\n",
        "def tag_percentage(tag_counts):\n",
        "  total_count = sum([count for tag, count in tag_counts])\n",
        "  tag_counts_percentages = [\n",
        "      (tag, count, count/total_count) for tag, count in tag_counts\n",
        "  ]\n",
        "  return tag_counts_percentages\n",
        "\n",
        "column_names = [\"Tag\", \"Count\", \"Percentage\"]\n",
        "row_data = []\n",
        "\n",
        "for tag, count, percent in tag_percentage(UD_TAGS.vocab.freqs.most_common()):\n",
        "  row_data.append([\n",
        "    tag, f\"{count:,}\", f\"{percent * 100:3.1f}%\"\n",
        "  ])\n",
        "\n",
        "tabulate_percentage(column_names, row_data )\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------+\n",
            "|       TAGS STATISTICS       |\n",
            "+-------+--------+------------+\n",
            "| Tag   |  Count | Percentage |\n",
            "+-------+--------+------------+\n",
            "| NOUN  | 34,781 |      17.0% |\n",
            "| PUNCT | 23,679 |      11.6% |\n",
            "| VERB  | 23,081 |      11.3% |\n",
            "| PRON  | 18,577 |       9.1% |\n",
            "| ADP   | 17,638 |       8.6% |\n",
            "| DET   | 16,285 |       8.0% |\n",
            "| PROPN | 12,946 |       6.3% |\n",
            "| ADJ   | 12,477 |       6.1% |\n",
            "| AUX   | 12,343 |       6.0% |\n",
            "| ADV   | 10,548 |       5.2% |\n",
            "| CCONJ |  6,707 |       3.3% |\n",
            "| PART  |  5,567 |       2.7% |\n",
            "| NUM   |  3,999 |       2.0% |\n",
            "| SCONJ |  3,843 |       1.9% |\n",
            "| X     |    847 |       0.4% |\n",
            "| INTJ  |    688 |       0.3% |\n",
            "| SYM   |    599 |       0.3% |\n",
            "+-------+--------+------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3dfLKtp7ijO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4351be75-03c9-463b-a40f-7522c4b37d2d"
      },
      "source": [
        "for tag, count, percent in tag_percentage(PTB_TAGS.vocab.freqs.most_common()):\n",
        "  row_data.append([\n",
        "    tag, f\"{count:,}\", f\"{percent * 100:3.1f}%\"\n",
        "  ])\n",
        "\n",
        "tabulate_percentage(column_names, row_data )"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------+\n",
            "|       TAGS STATISTICS       |\n",
            "+-------+--------+------------+\n",
            "| Tag   |  Count | Percentage |\n",
            "+-------+--------+------------+\n",
            "| NOUN  | 34,781 |      17.0% |\n",
            "| PUNCT | 23,679 |      11.6% |\n",
            "| VERB  | 23,081 |      11.3% |\n",
            "| PRON  | 18,577 |       9.1% |\n",
            "| ADP   | 17,638 |       8.6% |\n",
            "| DET   | 16,285 |       8.0% |\n",
            "| PROPN | 12,946 |       6.3% |\n",
            "| ADJ   | 12,477 |       6.1% |\n",
            "| AUX   | 12,343 |       6.0% |\n",
            "| ADV   | 10,548 |       5.2% |\n",
            "| CCONJ |  6,707 |       3.3% |\n",
            "| PART  |  5,567 |       2.7% |\n",
            "| NUM   |  3,999 |       2.0% |\n",
            "| SCONJ |  3,843 |       1.9% |\n",
            "| X     |    847 |       0.4% |\n",
            "| INTJ  |    688 |       0.3% |\n",
            "| SYM   |    599 |       0.3% |\n",
            "| NN    | 26,915 |      13.2% |\n",
            "| IN    | 20,724 |      10.1% |\n",
            "| DT    | 16,817 |       8.2% |\n",
            "| NNP   | 12,449 |       6.1% |\n",
            "| PRP   | 12,193 |       6.0% |\n",
            "| JJ    | 11,591 |       5.7% |\n",
            "| RB    | 10,831 |       5.3% |\n",
            "| .     | 10,317 |       5.0% |\n",
            "| VB    |  9,476 |       4.6% |\n",
            "| NNS   |  8,438 |       4.1% |\n",
            "| ,     |  8,062 |       3.9% |\n",
            "| CC    |  6,706 |       3.3% |\n",
            "| VBD   |  5,402 |       2.6% |\n",
            "| VBP   |  5,374 |       2.6% |\n",
            "| VBZ   |  4,578 |       2.2% |\n",
            "| CD    |  3,998 |       2.0% |\n",
            "| VBN   |  3,967 |       1.9% |\n",
            "| VBG   |  3,330 |       1.6% |\n",
            "| MD    |  3,294 |       1.6% |\n",
            "| TO    |  3,286 |       1.6% |\n",
            "| PRP$  |  3,068 |       1.5% |\n",
            "| -RRB- |  1,008 |       0.5% |\n",
            "| -LRB- |    973 |       0.5% |\n",
            "| WDT   |    948 |       0.5% |\n",
            "| WRB   |    869 |       0.4% |\n",
            "| :     |    866 |       0.4% |\n",
            "| ``    |    813 |       0.4% |\n",
            "| ''    |    785 |       0.4% |\n",
            "| WP    |    760 |       0.4% |\n",
            "| RP    |    755 |       0.4% |\n",
            "| UH    |    689 |       0.3% |\n",
            "| POS   |    684 |       0.3% |\n",
            "| HYPH  |    664 |       0.3% |\n",
            "| JJR   |    503 |       0.2% |\n",
            "| NNPS  |    498 |       0.2% |\n",
            "| JJS   |    383 |       0.2% |\n",
            "| EX    |    359 |       0.2% |\n",
            "| NFP   |    338 |       0.2% |\n",
            "| GW    |    294 |       0.1% |\n",
            "| ADD   |    292 |       0.1% |\n",
            "| RBR   |    276 |       0.1% |\n",
            "| $     |    258 |       0.1% |\n",
            "| PDT   |    175 |       0.1% |\n",
            "| RBS   |    169 |       0.1% |\n",
            "| SYM   |    156 |       0.1% |\n",
            "| LS    |    117 |       0.1% |\n",
            "| FW    |     93 |       0.0% |\n",
            "| AFX   |     48 |       0.0% |\n",
            "| WP$   |     15 |       0.0% |\n",
            "| XX    |      1 |       0.0% |\n",
            "+-------+--------+------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kgkoj6r0LgVG"
      },
      "source": [
        "### Creating an iterator\n",
        "\n",
        "We are going to use the `BucketIterator` to create iterators for our different sets, train, test and validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owBRob2J7iUo"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size = BATCH_SIZE,\n",
        "    device=device\n",
        ")"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xj5-U0T-MHEp"
      },
      "source": [
        "### Model building\n",
        "\n",
        "We are going to build a BiDirectional LSTM (BiLSTM) model. The following image shows the simple vession  of a model with a single LSTM layer without cell state.\n",
        "\n",
        "![img](https://camo.githubusercontent.com/192cdf13224f82ea92cf3635554437271ec33e64/68747470733a2f2f6769746875622e636f6d2f62656e747265766574742f7079746f7263682d706f732d74616767696e672f626c6f622f6d61737465722f6173736574732f706f732d6269646972656374696f6e616c2d6c73746d2e706e673f7261773d31)\n",
        "\n",
        "The model takes in a sequence of tokens, $X = \\{x_1, x_2,...,x_T\\}$, passes them through an embedding layer, $e$, to get the token embeddings, $e(X) = \\{e(x_1), e(x_2), ..., e(x_T)\\}$.\n",
        "\n",
        "The hidden state of an LSTM model from the previous time step can be denoted by:\n",
        "\n",
        "$$h^{\\rightarrow}_t = \\text{LSTM}^{\\rightarrow}(e(x^{\\rightarrow}_t), h^{\\rightarrow}_{t-1}, c^{\\rightarrow}_{t-1})$$$$h^{\\leftarrow}_t=\\text{LSTM}^{\\leftarrow}(e(x^{\\leftarrow}_t), h^{\\leftarrow}_{t-1}, c^{\\leftarrow}_{t-1})$$\n",
        "\n",
        "After the whole sequence has been processed, the hidden and cell states are then passed to the next layer of the LSTM.\n",
        "\n",
        "The initial states are tesnsors of zeros.\n",
        "\n",
        "We then concatenate both the forward and backward hidden states from the final layer of the LSTM, $H = \\{h_1, h_2, ... h_T\\}$, where $h_1 = [h^{\\rightarrow}_1;h^{\\leftarrow}_T]$, $h_2 = [h^{\\rightarrow}_2;h^{\\leftarrow}_{T-1}]$, etc. and pass them through a linear layer, $f$, which is used to make the prediction of which tag applies to this token, $\\hat{y}_t = f(h_t)$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhCmBZhN7iRn"
      },
      "source": [
        "class BiLSTMPOSTagger(nn.Module):\n",
        "  def __init__(self,\n",
        "               input_dim,\n",
        "               embedding_dim,\n",
        "               hidden_dim,\n",
        "               output_dim,\n",
        "               pad_idx,\n",
        "               n_layers =2,\n",
        "               bidirectional=True,\n",
        "               dropout=.5,\n",
        "               ):\n",
        "    super(BiLSTMPOSTagger, self).__init__()\n",
        "    self.embedding = nn.Embedding(input_dim, embedding_dim,\n",
        "                                  padding_idx=pad_idx)\n",
        "    self.lstm = nn.LSTM(embedding_dim,\n",
        "                        hidden_dim,\n",
        "                        num_layers=n_layers,\n",
        "                        bidirectional = bidirectional,\n",
        "                        dropout = dropout if n_layers > 1 else 0\n",
        "                        )\n",
        "    self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim,\n",
        "                        output_dim )\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, text):\n",
        "    # text = [sent len, batch size]\n",
        "    embedded = self.dropout(self.embedding(text))\n",
        "    \n",
        "    # embedded = [sent len, batch size, emb dim]\n",
        "    outputs, (h_0, c_0) = self.lstm(embedded)\n",
        "    \"\"\"\n",
        "    outputs holds the backward and forward hidden states \n",
        "    in the final layer\n",
        "    \n",
        "    hidden and cell are the backward and forward hidden\n",
        "    and cell states at the final time-step\n",
        "    \n",
        "    output = [sent len, batch size, hid dim * n directions]\n",
        "    hidden/cell = [n layers * n directions, batch size, hid dim]\n",
        "    \n",
        "    we use our outputs to make a prediction of what the tag should be\n",
        "    \"\"\"\n",
        "    out = self.fc(self.dropout(outputs))\n",
        "    # out [sent len, batch size, output dim]\n",
        "    return out"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0vJuKlcPSQC"
      },
      "source": [
        "### Model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RV3UVhrW7iOK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9a32165-2aa6-47c1-a2d9-850136e8eb86"
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 128\n",
        "OUTPUT_DIM = len(UD_TAGS.vocab)\n",
        "\n",
        "N_LAYERS = 2\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT = 0.25\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "model = BiLSTMPOSTagger(\n",
        "    INPUT_DIM, \n",
        "    EMBEDDING_DIM, \n",
        "    HIDDEN_DIM, \n",
        "    OUTPUT_DIM, \n",
        "    PAD_IDX,\n",
        "    n_layers= N_LAYERS, \n",
        "    bidirectional = BIDIRECTIONAL, \n",
        "    dropout= DROPOUT, \n",
        ")\n",
        "model"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BiLSTMPOSTagger(\n",
              "  (embedding): Embedding(8866, 100, padding_idx=1)\n",
              "  (lstm): LSTM(100, 128, num_layers=2, dropout=0.25, bidirectional=True)\n",
              "  (fc): Linear(in_features=256, out_features=18, bias=True)\n",
              "  (dropout): Dropout(p=0.25, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDyKcMGjQKVZ"
      },
      "source": [
        "We are going then to initialize the model weights using Normal distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAgJSinq7iLW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f77c83a-dda5-4a05-bba5-61ff1f2b2870"
      },
      "source": [
        "def init_weights(m):\n",
        "  for name, param in m.named_parameters():\n",
        "    nn.init.normal_(param.data, mean=0, std=0.1)\n",
        "\n",
        "model.apply(init_weights)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BiLSTMPOSTagger(\n",
              "  (embedding): Embedding(8866, 100, padding_idx=1)\n",
              "  (lstm): LSTM(100, 128, num_layers=2, dropout=0.25, bidirectional=True)\n",
              "  (fc): Linear(in_features=256, out_features=18, bias=True)\n",
              "  (dropout): Dropout(p=0.25, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Si6Ar3pQiUz"
      },
      "source": [
        "Next we are going to count moedl parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZAND-q-7iIc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ae26baa-8f6f-4b62-f879-473da4a50419"
      },
      "source": [
        "def count_trainable_params(model):\n",
        "  return sum(p.numel() for p in model.parameters()), sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "n_params, trainable_params = count_trainable_params(model)\n",
        "print(f\"Total number of paramaters: {n_params:,}\\nTotal tainable parameters: {trainable_params:,}\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of paramaters: 1,522,010\n",
            "Total tainable parameters: 1,522,010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OI0k3cD2Q_DC"
      },
      "source": [
        "We will then initialize the model embedding layer with pretained word vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NN030Tq7iFb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "507e87b0-5a3d-44c0-c4c1-dd933d1fef4e"
      },
      "source": [
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "model.embedding.weight.data.copy_(pretrained_embeddings)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.9269,  1.4873,  0.9007,  ...,  0.1233,  0.3499,  0.6173],\n",
              "        [ 0.7262,  0.0912, -0.3891,  ...,  0.0821,  0.4440, -0.7240],\n",
              "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
              "        ...,\n",
              "        [-0.6808,  0.5419, -1.5231,  ..., -1.1103, -0.5245, -0.9152],\n",
              "        [-0.5972,  0.0471, -0.2406,  ..., -0.9446, -0.1126, -0.2260],\n",
              "        [-1.8684, -1.3026, -0.8013,  ...,  0.2404,  0.4319, -1.3682]])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M46pzlraRhTC"
      },
      "source": [
        "We will then initialize the padding tokens to zeros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcVG7Zpw7iCJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fdbe412-3a09-4c5d-fbd8-f84faabbb91f"
      },
      "source": [
        "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "model.embedding.weight.data"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.9269,  1.4873,  0.9007,  ...,  0.1233,  0.3499,  0.6173],\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
              "        ...,\n",
              "        [-0.6808,  0.5419, -1.5231,  ..., -1.1103, -0.5245, -0.9152],\n",
              "        [-0.5972,  0.0471, -0.2406,  ..., -0.9446, -0.1126, -0.2260],\n",
              "        [-1.8684, -1.3026, -0.8013,  ...,  0.2404,  0.4319, -1.3682]])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veagGKatRujE"
      },
      "source": [
        "### Optimizer\n",
        "\n",
        "We are going to use the Adam optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRP9HKkQRsit"
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters())"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTjxB87lR61M"
      },
      "source": [
        "### Criterion/Loss Function\n",
        "\n",
        "Next we are going to define our loss function.\n",
        "\n",
        "Even though we have no <unk> tokens within our tag vocab, we still have <pad> tokens. This is because all sentences within a batch need to be the same size. However, we don't want to calculate the loss when the target is a <pad> token as we aren't training our model to recognize padding tokens.\n",
        "\n",
        "We handle this by setting the ignore_index in our loss function to the index of the padding token in our tag vocabulary.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q19NDJqkR6XA"
      },
      "source": [
        "TAG_PAD_IDX = UD_TAGS.vocab.stoi[UD_TAGS.pad_token]\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kh0Ep_6USMr1"
      },
      "source": [
        "### Model and criterion to GPU if available"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwXiC4hYSMJT"
      },
      "source": [
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oY9ma1jPTsdY"
      },
      "source": [
        "We dont want to caculate the accuracy over `<pad>` tokens as we are not interested in predicting them. So we are going to create a function that calculate the accuracy of non-padded tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6fgUyi7Tg1W"
      },
      "source": [
        "def categorical_accuracy(preds, y, tag_pad_idx):\n",
        "  max_preds = preds.argmax(dim = 1, keepdim = True)\n",
        "  non_pad_elements = (y != tag_pad_idx).nonzero()\n",
        "  correct = max_preds[non_pad_elements].squeeze(1).eq(y[non_pad_elements])\n",
        "  return correct.sum() / y[non_pad_elements].shape[0]"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2vKmsc_UUxo"
      },
      "source": [
        "### Training and evaluation functions\n",
        "\n",
        "In the training function we will put the model in train mode and then iterate over our iteraror which returns a batch of examples and for each batch we do the following:\n",
        "\n",
        "* we zero the gradients over the parameters from the last gradient calculation\n",
        "* insert the batch of text into the model to get predictions\n",
        "* as PyTorch loss functions cannot handle 3-dimensional predictions we reshape our predictions\n",
        "* calculate the loss and accuracy between the predicted tags and actual tags\n",
        "* call backward to calculate the gradients of the parameters w.r.t. the loss\n",
        "* take an optimizer step to update the parameters\n",
        "* add to the running total of loss and accuracy\n",
        "\n",
        "In the evaluation functio we pretty much do the same. We are going to put the model in the evaluation mode and wrap our iteration loop in the `torch.no_grad` to ensure that we don't calculate the gradients, we also don't use the optimizer in there."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vupm9OdyUUAa"
      },
      "source": [
        "def train(model, iterator,  optimizer, criterion, tag_pad_idx):\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "  model.train()\n",
        "  for batch in iterator:\n",
        "    text = batch.text # text = [sent len, batch size]\n",
        "    tags = batch.udtags # tags = [sent len, batch size]\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    predictions = model(text)\n",
        "    # predictions = [sent len, batch size, output dim]\n",
        "    predictions = predictions.view(-1, predictions.shape[-1])\n",
        "    # predictions = [sent len * batch size, output dim]\n",
        "    tags = tags.view(-1) # tags = [sent len * batch size]\n",
        "    loss = criterion(predictions, tags)\n",
        "    acc = categorical_accuracy(predictions, tags, tag_pad_idx)\n",
        "    \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    epoch_loss += loss.item()\n",
        "    epoch_acc += acc.item()\n",
        "\n",
        "  return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "\n",
        "def evaluate(model, iterator, criterion, tag_pad_idx):\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch in iterator:\n",
        "      text = batch.text # text = [sent len, batch size]\n",
        "      tags = batch.udtags # tags = [sent len, batch size]\n",
        "      predictions = model(text)\n",
        "      # predictions = [sent len, batch size, output dim]\n",
        "      predictions = predictions.view(-1, predictions.shape[-1])\n",
        "      # predictions = [sent len * batch size, output dim]\n",
        "      tags = tags.view(-1) # tags = [sent len * batch size]\n",
        "      loss = criterion(predictions, tags)\n",
        "      acc = categorical_accuracy(predictions, tags, tag_pad_idx)\n",
        "      \n",
        "      epoch_loss += loss.item()\n",
        "      epoch_acc += acc.item()\n",
        "  return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF-6wpeGXT1t"
      },
      "source": [
        "### Training loop\n",
        "\n",
        "We are going to have helper functions that will helps us to visualizing our trainig epoch\n",
        "\n",
        "1. Time to string function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpUeT8DyW2bS"
      },
      "source": [
        "def hms_string(sec_elapsed):\n",
        "    h = int(sec_elapsed / (60 * 60))\n",
        "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
        "    s = sec_elapsed % 60\n",
        "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00GbYksPXgmz"
      },
      "source": [
        "2. visualize training epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHnAJ34CXgYC"
      },
      "source": [
        "def visualize_training(start, end, train_loss, train_accuracy, val_loss, val_accuracy, title):\n",
        "  data = [\n",
        "       [\"Training\", f'{train_loss:.3f}', f'{train_accuracy:.3f}', f\"{hms_string(end - start)}\" ],\n",
        "       [\"Validation\", f'{val_loss:.3f}', f'{val_accuracy:.3f}', \"\" ],       \n",
        "  ]\n",
        "  table = PrettyTable([\"CATEGORY\", \"LOSS\", \"ACCURACY\", \"ETA\"])\n",
        "  table.align[\"CATEGORY\"] = 'l'\n",
        "  table.align[\"LOSS\"] = 'r'\n",
        "  table.align[\"ACCURACY\"] = 'r'\n",
        "  table.align[\"ETA\"] = 'r'\n",
        "  table.title = title\n",
        "  for row in data:\n",
        "    table.add_row(row)\n",
        "  print(table)\n",
        "  "
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T04toMT7XgWH",
        "outputId": "52a789e4-735f-4bac-f9c7-0386606ba90e"
      },
      "source": [
        "\n",
        "N_EPOCHS = 10\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "  start = time.time()\n",
        "  train_loss, train_acc = train(model, train_iterator, optimizer,\n",
        "                                criterion, TAG_PAD_IDX)\n",
        "  valid_loss, valid_acc = evaluate(model,\n",
        "                                   valid_iterator,\n",
        "                                   criterion, TAG_PAD_IDX)\n",
        "  \n",
        "  title = f\"EPOCH: {epoch+1:02}/{N_EPOCHS:02} {'saving best model...' if valid_loss < best_valid_loss else 'not saving...'}\"\n",
        "  if valid_loss < best_valid_loss:\n",
        "      best_valid_loss = valid_loss\n",
        "      torch.save(model.state_dict(), 'best-model.pt')\n",
        "  end = time.time()\n",
        "  visualize_training(start, end, train_loss, train_acc,\n",
        "                     valid_loss, valid_acc, title)\n",
        "  "
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------------------+\n",
            "|     EPOCH: 01/10 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.347 |    0.581 | 0:00:07.01 |\n",
            "| Validation | 0.665 |    0.803 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 02/10 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.476 |    0.852 | 0:00:06.68 |\n",
            "| Validation | 0.494 |    0.838 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 03/10 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.348 |    0.890 | 0:00:06.68 |\n",
            "| Validation | 0.439 |    0.871 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 04/10 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.288 |    0.909 | 0:00:06.68 |\n",
            "| Validation | 0.400 |    0.879 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 05/10 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.251 |    0.921 | 0:00:06.77 |\n",
            "| Validation | 0.383 |    0.886 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|         EPOCH: 06/10 not saving...         |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.225 |    0.928 | 0:00:06.64 |\n",
            "| Validation | 0.386 |    0.876 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 07/10 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.205 |    0.934 | 0:00:06.72 |\n",
            "| Validation | 0.377 |    0.887 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 08/10 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.189 |    0.939 | 0:00:06.75 |\n",
            "| Validation | 0.357 |    0.893 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 09/10 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.177 |    0.943 | 0:00:06.69 |\n",
            "| Validation | 0.353 |    0.893 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 10/10 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.166 |    0.947 | 0:00:06.72 |\n",
            "| Validation | 0.349 |    0.893 |            |\n",
            "+------------+-------+----------+------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UrjKsZFYXSt"
      },
      "source": [
        "### Evaluating the best model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zujOKg0XgRw",
        "outputId": "0c395208-1eb9-4471-c6d3-224d1cd56bd3"
      },
      "source": [
        "def visualize_test(start, end, test_loss,\n",
        "                       test_accuracy, title):\n",
        "  data = [\n",
        "       [\"test\", f'{test_loss:.3f}', f'{test_accuracy:.3f}', f\"{hms_string(end - start)}\" ],       \n",
        "  ]\n",
        "  table = PrettyTable([\"CATEGORY\", \"LOSS\", \"ACCURACY\", \"ETA\"])\n",
        "  table.align[\"CATEGORY\"] = 'l'\n",
        "  table.align[\"LOSS\"] = 'r'\n",
        "  table.align[\"ACCURACY\"] = 'r'\n",
        "  table.align[\"ETA\"] = 'r'\n",
        "  table.title = title\n",
        "  for row in data:\n",
        "    table.add_row(row)\n",
        "  print(table)\n",
        "  \n",
        "\n",
        "model.load_state_dict(torch.load('best-model.pt'))\n",
        "\n",
        "start = time.time()\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion, tag_pad_idx=TAG_PAD_IDX)\n",
        "end = time.time()\n",
        "\n",
        "visualize_test(start, end, test_loss, test_acc, \"MODEL EVALUATION SUMMARY\")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------+\n",
            "|         MODEL EVALUATION SUMMARY         |\n",
            "+----------+-------+----------+------------+\n",
            "| CATEGORY |  LOSS | ACCURACY |        ETA |\n",
            "+----------+-------+----------+------------+\n",
            "| test     | 0.363 |    0.884 | 0:00:00.17 |\n",
            "+----------+-------+----------+------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efX2Tmh-f6y1"
      },
      "source": [
        "### Model inference\n",
        "\n",
        "Now we are ready to create out `tag_sentence` function that will:\n",
        "\n",
        "* put the model into evaluation mode\n",
        "* tokenize the sentence with spaCy if it is not a list\n",
        "* lowercase the tokens if the Field did\n",
        "numericalize the tokens using the vocabulary\n",
        "* find out which tokens are not in the vocabulary, i.e. are ``<unk>`` tokens\n",
        "convert the numericalized tokens into a tensor and add a batch dimension\n",
        "* feed the tensor into the model\n",
        "* get the predictions over the sentence\n",
        "* convert the predictions into readable tags\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtO7KhE0f6oU"
      },
      "source": [
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()\n",
        "def tag_sentence(model, device, sentence, text_field, tag_field):\n",
        "  model.eval()\n",
        "\n",
        "  if isinstance(sentence, str):\n",
        "    tokens = [token.text for token in nlp.tokenizer(sentence)]\n",
        "  else:\n",
        "    tokens = [token for token in sentence]\n",
        "\n",
        "  if text_field.lower:\n",
        "    tokens = [t.lower() for t in tokens]\n",
        "\n",
        "  numericalized_tokens = [text_field.vocab.stoi[t] for t in tokens]\n",
        "  unk_idx = text_field.vocab.stoi[text_field.unk_token]\n",
        "  unks = [t for t, n in zip(tokens, numericalized_tokens) if n == unk_idx]\n",
        "  token_tensor = torch.LongTensor(numericalized_tokens)\n",
        "  token_tensor = token_tensor.unsqueeze(-1).to(device)\n",
        "  predictions = model(token_tensor)\n",
        "  top_predictions = predictions.argmax(-1)\n",
        "  predicted_tags = [tag_field.vocab.itos[t.item()] for t in top_predictions]\n",
        "  return tokens, predicted_tags, unks\n"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngKMHE_Vh2Ll"
      },
      "source": [
        "Taking a single example on our train data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKPnYqBgf6lt",
        "outputId": "b439ac26-5e64-4a2f-f87b-65920df929a8"
      },
      "source": [
        "example_index = 1\n",
        "\n",
        "sentence = vars(train_data.examples[example_index])['text']\n",
        "actual_tags = vars(train_data.examples[example_index])['udtags']\n",
        "print(sentence)\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[', 'this', 'killing', 'of', 'a', 'respected', 'cleric', 'will', 'be', 'causing', 'us', 'trouble', 'for', 'years', 'to', 'come', '.', ']']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_x1eczyf6jC",
        "outputId": "56b14287-640c-4ed9-92b5-4a534affc0a4"
      },
      "source": [
        "tokens, pred_tags, unks = tag_sentence(model, \n",
        "                                       device, \n",
        "                                       sentence, \n",
        "                                       TEXT, \n",
        "                                       UD_TAGS)\n",
        "\n",
        "print(unks) # 'respected', 'cleric' have unkown tags"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['respected', 'cleric']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQWdK7UpimU9"
      },
      "source": [
        "We can check how correct the nodel is right now:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9Az4sRQf6fc",
        "outputId": "63a2b43c-45db-48ce-f1d7-ed6dbb3c0c8a"
      },
      "source": [
        "print(\"Pred. Tag\\tActual Tag\\tCorrect?\\tToken\\n\")\n",
        "\n",
        "for token, pred_tag, actual_tag in zip(tokens, pred_tags, actual_tags):\n",
        "    correct = '' if pred_tag == actual_tag else ''\n",
        "    print(f\"{pred_tag}\\t\\t{actual_tag}\\t\\t{correct}\\t\\t{token}\")"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pred. Tag\tActual Tag\tCorrect?\tToken\n",
            "\n",
            "PUNCT\t\tPUNCT\t\t\t\t[\n",
            "DET\t\tDET\t\t\t\tthis\n",
            "NOUN\t\tNOUN\t\t\t\tkilling\n",
            "ADP\t\tADP\t\t\t\tof\n",
            "DET\t\tDET\t\t\t\ta\n",
            "NOUN\t\tADJ\t\t\t\trespected\n",
            "NOUN\t\tNOUN\t\t\t\tcleric\n",
            "AUX\t\tAUX\t\t\t\twill\n",
            "AUX\t\tAUX\t\t\t\tbe\n",
            "VERB\t\tVERB\t\t\t\tcausing\n",
            "PRON\t\tPRON\t\t\t\tus\n",
            "NOUN\t\tNOUN\t\t\t\ttrouble\n",
            "ADP\t\tADP\t\t\t\tfor\n",
            "NOUN\t\tNOUN\t\t\t\tyears\n",
            "PART\t\tPART\t\t\t\tto\n",
            "VERB\t\tVERB\t\t\t\tcome\n",
            "PUNCT\t\tPUNCT\t\t\t\t.\n",
            "PUNCT\t\tPUNCT\t\t\t\t]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxCFTP0vi9lk"
      },
      "source": [
        "Next we will make our own sentence and test this out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5TkzXAuf6c8",
        "outputId": "c33152f5-d770-4736-efa4-f24e27352317"
      },
      "source": [
        "sentence = 'The Queen will deliver a speech about the conflict in North Korea at 1pm tomorrow.'\n",
        "\n",
        "tokens, tags, unks = tag_sentence(model, \n",
        "                                  device, \n",
        "                                  sentence, \n",
        "                                  TEXT, \n",
        "                                  UD_TAGS)\n",
        "\n",
        "print(unks) # we cont have unknowns here."
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01D4EBF7f6Zq",
        "outputId": "28651828-383e-4c9d-8e44-c3c8a89e933e"
      },
      "source": [
        "print(\"Pred. Tag\\tToken\\n\")\n",
        "for token, tag in zip(tokens, tags):\n",
        "    print(f\"{tag}\\t\\t{token}\")"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pred. Tag\tToken\n",
            "\n",
            "DET\t\tthe\n",
            "NOUN\t\tqueen\n",
            "AUX\t\twill\n",
            "VERB\t\tdeliver\n",
            "DET\t\ta\n",
            "NOUN\t\tspeech\n",
            "ADP\t\tabout\n",
            "DET\t\tthe\n",
            "NOUN\t\tconflict\n",
            "ADP\t\tin\n",
            "PROPN\t\tnorth\n",
            "PROPN\t\tkorea\n",
            "ADP\t\tat\n",
            "NUM\t\t1\n",
            "NOUN\t\tpm\n",
            "NOUN\t\ttomorrow\n",
            "PUNCT\t\t.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFiD--CwjRwd"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "We have implemented out POS tagging in pytorch using UD Tags. In the following notebook we want to change to make use of the `PTB`. Basically in the following notebook we are justgoing to use this notebook as our base, and change just a few things.\n",
        "\n",
        "### Credits\n",
        "\n",
        "* [bentrevett](https://github.com/bentrevett/pytorch-pos-tagging/blob/master/1_bilstm.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXM-sH7GjvG2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}