{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02_BiLSTM_PTB-tags.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaFpM1xf-9JC"
      },
      "source": [
        "---\n",
        "Author: **`Crispen Gari`**\n",
        "\n",
        "Date: **`2020-09-08`**\n",
        "\n",
        "Topic: **`Part of Speech Tagging (PoS)Tagging`**\n",
        "\n",
        "Library: **`Pytorch`**\n",
        "\n",
        "Language: **`Python`**\n",
        "\n",
        "Main: **`Natural Language Processing (NLP)`**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZ4Dk-g77jhQ"
      },
      "source": [
        "### BiLSTM for POS Tagging\n",
        "\n",
        "In this notebook we are going to clone the previous notebook and make use of it as a base notebook to the task that we are going to perform today of creating a POS tagger using Bi-Directional LSTM. WE are only going to change the tags type that we will be working with. We are going to predict ``PTB`` tags instead of ``UD`` tags since we have done that in the previous notebook.\n",
        "\n",
        "The rest of the notebook will just remain the same with the cloned notebook where there's a change i will highlight.\n",
        "\n",
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6Bjyb8nW7jXv",
        "outputId": "cdd591de-2adb-4c53-c798-70c203be0bb1"
      },
      "source": [
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from torch import nn\n",
        "\n",
        "from torchtext.legacy import data, datasets\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "import time, os, random\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "torch.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.9.0+cu102'"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "he6GA9b68yON"
      },
      "source": [
        "### Seeds and Device"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-7jIl3q7WZb",
        "outputId": "c8f644f0-edc4-467a-8615-40e8ffd02eda"
      },
      "source": [
        "SEED = 42\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEKXakFZDQjg"
      },
      "source": [
        "### Helper function that will visualise data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iJQQNQsDWiP"
      },
      "source": [
        "def tabulate(column_names, data, title=\"VISUALIZING SETS EXAMPLES\"):\n",
        "  table = PrettyTable(column_names)\n",
        "  table.title= title\n",
        "  for row in data:\n",
        "    table.add_row(row)\n",
        "  print(table)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lv7M-tbb9jWB"
      },
      "source": [
        "### Data preparation\n",
        "\n",
        "This time around we will be working with PTB.\n",
        "### Fields"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYCLuyOb7jN-"
      },
      "source": [
        "TEXT = data.Field(lower=True)\n",
        "UD_TAGS = data.Field(unk_token = None)\n",
        "PTB_TAGS = data.Field(unk_token = None)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xXi1dL_7jK0"
      },
      "source": [
        "fields = ((\"text\", TEXT), (None, None), (\"ptbtags\", PTB_TAGS))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrYWlx76BiHU"
      },
      "source": [
        "Now we will load our data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-sCA5Xe7jH9"
      },
      "source": [
        "train_data, valid_data, test_data = datasets.UDPOS.splits(fields)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJV4nsxKC-g1"
      },
      "source": [
        "Checking examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aVnSlcQ7jFD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a62e0b7-5b62-4a5e-f7dc-a4aa2e6c0fb5"
      },
      "source": [
        "column_names = [\"SUBSET\", \"EXAMPLE(s)\"]\n",
        "row_data = [\n",
        "        [\"training\", f\"{len(train_data):,}\"],\n",
        "        ['validation', f\"{len(valid_data):,}\"],\n",
        "        ['test', f\"{len(test_data):,}\"]\n",
        "]\n",
        "tabulate(column_names, row_data)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------+\n",
            "|  VISUALIZING SETS EXAMPLES  |\n",
            "+--------------+--------------+\n",
            "|    SUBSET    |  EXAMPLE(s)  |\n",
            "+--------------+--------------+\n",
            "|   training   |    12,543    |\n",
            "|  validation  |    2,002     |\n",
            "|     test     |    2,077     |\n",
            "+--------------+--------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQktN050D9sT"
      },
      "source": [
        "Printing examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0F-gQLT7jBu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc12eedb-b589-4d04-984e-9c64798baed1"
      },
      "source": [
        "print(vars(train_data.examples[0]))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': ['al', '-', 'zaman', ':', 'american', 'forces', 'killed', 'shaikh', 'abdullah', 'al', '-', 'ani', ',', 'the', 'preacher', 'at', 'the', 'mosque', 'in', 'the', 'town', 'of', 'qaim', ',', 'near', 'the', 'syrian', 'border', '.'], 'ptbtags': ['NNP', 'HYPH', 'NNP', ':', 'JJ', 'NNS', 'VBD', 'NNP', 'NNP', 'NNP', 'HYPH', 'NNP', ',', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'NNP', ',', 'IN', 'DT', 'JJ', 'NN', '.']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6AGhLOJ7i-z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0d992a6-a5a6-47b8-eee6-dc95a50c5d24"
      },
      "source": [
        "print(vars(train_data.examples[0]).get(\"text\"))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['al', '-', 'zaman', ':', 'american', 'forces', 'killed', 'shaikh', 'abdullah', 'al', '-', 'ani', ',', 'the', 'preacher', 'at', 'the', 'mosque', 'in', 'the', 'town', 'of', 'qaim', ',', 'near', 'the', 'syrian', 'border', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "komUUHd-7i4n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75028aef-a2bc-47ac-f080-f9d4495abf4e"
      },
      "source": [
        "print(vars(train_data.examples[0]).get(\"ptbtags\"))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NNP', 'HYPH', 'NNP', ':', 'JJ', 'NNS', 'VBD', 'NNP', 'NNP', 'NNP', 'HYPH', 'NNP', ',', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'NNP', ',', 'IN', 'DT', 'JJ', 'NN', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQedH9lMETga"
      },
      "source": [
        "### Vocabulary mapping\n",
        "Next we will build a maping of tokens to integers. We are going to set the `min_freq` to 2 so that the tokens that appears less that 2 times in the corpus will be automatically converted to `<unk>`\n",
        "\n",
        "WE are also going to load the Glove pretrainded word embedding. Using pretrained word vectors lead to impovement in the performance of the model. In our case we are going to use the `glove.6B.100d` which was trained with about 6 billion words and each word is `100d` vector of numbers.\n",
        "\n",
        "_`unk_init`_ is used to initialize the token embeddings which are not in the pre-trained embedding vocabulary. By default this sets those embeddings to zeros, however it is better to not have them all initialized to the same value, so we initialize them from a **Normal/Gaussian** distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6iZKN047i14"
      },
      "source": [
        "MIN_FREQ = 2\n",
        "TEXT.build_vocab(train_data, min_freq=MIN_FREQ,\n",
        "                 vectors = \"glove.6B.100d\",\n",
        "                 unk_init = torch.Tensor.normal_\n",
        "                 )\n",
        "\n",
        "UD_TAGS.build_vocab(train_data)\n",
        "PTB_TAGS.build_vocab(train_data)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2wR16IKFvkF"
      },
      "source": [
        "Let's check how many tokens are in our vobabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qdd8qs87iyU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57d486bc-68cc-46e2-91bb-4aeb57b1807a"
      },
      "source": [
        "column_names = [\"FIELD\", \"TOKEN(s)\"]\n",
        "row_data = [\n",
        "        [\"TEXT\", f\"{len(TEXT.vocab):,}\"],\n",
        "        ['PTB_TAGS', f\"{len(PTB_TAGS.vocab):,}\"]\n",
        "]\n",
        "tabulate(column_names, row_data, title=\"VOCABULARY SIZES\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------+\n",
            "|   VOCABULARY SIZES  |\n",
            "+----------+----------+\n",
            "|  FIELD   | TOKEN(s) |\n",
            "+----------+----------+\n",
            "|   TEXT   |  8,866   |\n",
            "| PTB_TAGS |    51    |\n",
            "+----------+----------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Z9yBfhhGSfa"
      },
      "source": [
        "Checking the most common words in the vocabulary..\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKYXNhg07iwJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b70bf40-d281-4cd4-b2e2-90d56d4a099d"
      },
      "source": [
        "print(TEXT.vocab.freqs.most_common(10))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('the', 9076), ('.', 8640), (',', 7021), ('to', 5137), ('and', 5002), ('a', 3782), ('of', 3622), ('i', 3379), ('in', 3112), ('is', 2239)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qu1sREX6GnpB"
      },
      "source": [
        "We can check the vocabularies of our tags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tKO6CX87isb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17bb81a7-7021-4381-e689-b5c956b422b8"
      },
      "source": [
        "print(PTB_TAGS.vocab.itos)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<pad>', 'NN', 'IN', 'DT', 'NNP', 'PRP', 'JJ', 'RB', '.', 'VB', 'NNS', ',', 'CC', 'VBD', 'VBP', 'VBZ', 'CD', 'VBN', 'VBG', 'MD', 'TO', 'PRP$', '-RRB-', '-LRB-', 'WDT', 'WRB', ':', '``', \"''\", 'WP', 'RP', 'UH', 'POS', 'HYPH', 'JJR', 'NNPS', 'JJS', 'EX', 'NFP', 'GW', 'ADD', 'RBR', '$', 'PDT', 'RBS', 'SYM', 'LS', 'FW', 'AFX', 'WP$', 'XX']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHAqjoDrG2Mb"
      },
      "source": [
        "Checking most common tags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiATfj2R7iqf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9248a63d-4f00-4ab6-dd36-8611fd735767"
      },
      "source": [
        "\n",
        "print(PTB_TAGS.vocab.freqs.most_common(10))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('NN', 26915), ('IN', 20724), ('DT', 16817), ('NNP', 12449), ('PRP', 12193), ('JJ', 11591), ('RB', 10831), ('.', 10317), ('VB', 9476), ('NNS', 8438)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETWKyw4vHO_q"
      },
      "source": [
        "We can also check tag percentages that are in our training data set as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Zh4KHhv7imo"
      },
      "source": [
        "def tabulate_percentage(column_names, data, title=\"TAGS STATISTICS\"):\n",
        "  table = PrettyTable(column_names)\n",
        "  table.title= title\n",
        "  table.align[column_names[0]] = 'l'\n",
        "  table.align[column_names[1]] = 'r'\n",
        "  table.align[column_names[2]] = 'r'\n",
        "  for row in data:\n",
        "    table.add_row(row)\n",
        "  print(table)\n",
        "\n",
        "def tag_percentage(tag_counts):\n",
        "  total_count = sum([count for tag, count in tag_counts])\n",
        "  tag_counts_percentages = [\n",
        "      (tag, count, count/total_count) for tag, count in tag_counts\n",
        "  ]\n",
        "  return tag_counts_percentages\n",
        "\n",
        "column_names = [\"Tag\", \"Count\", \"Percentage\"]\n",
        "row_data = []\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3dfLKtp7ijO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e23d99c6-47c7-482a-bf43-dccb37ee3f55"
      },
      "source": [
        "for tag, count, percent in tag_percentage(PTB_TAGS.vocab.freqs.most_common()):\n",
        "  row_data.append([\n",
        "    tag, f\"{count:,}\", f\"{percent * 100:3.1f}%\"\n",
        "  ])\n",
        "\n",
        "tabulate_percentage(column_names, row_data )"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------+\n",
            "|       TAGS STATISTICS       |\n",
            "+-------+--------+------------+\n",
            "| Tag   |  Count | Percentage |\n",
            "+-------+--------+------------+\n",
            "| NN    | 26,915 |      13.2% |\n",
            "| IN    | 20,724 |      10.1% |\n",
            "| DT    | 16,817 |       8.2% |\n",
            "| NNP   | 12,449 |       6.1% |\n",
            "| PRP   | 12,193 |       6.0% |\n",
            "| JJ    | 11,591 |       5.7% |\n",
            "| RB    | 10,831 |       5.3% |\n",
            "| .     | 10,317 |       5.0% |\n",
            "| VB    |  9,476 |       4.6% |\n",
            "| NNS   |  8,438 |       4.1% |\n",
            "| ,     |  8,062 |       3.9% |\n",
            "| CC    |  6,706 |       3.3% |\n",
            "| VBD   |  5,402 |       2.6% |\n",
            "| VBP   |  5,374 |       2.6% |\n",
            "| VBZ   |  4,578 |       2.2% |\n",
            "| CD    |  3,998 |       2.0% |\n",
            "| VBN   |  3,967 |       1.9% |\n",
            "| VBG   |  3,330 |       1.6% |\n",
            "| MD    |  3,294 |       1.6% |\n",
            "| TO    |  3,286 |       1.6% |\n",
            "| PRP$  |  3,068 |       1.5% |\n",
            "| -RRB- |  1,008 |       0.5% |\n",
            "| -LRB- |    973 |       0.5% |\n",
            "| WDT   |    948 |       0.5% |\n",
            "| WRB   |    869 |       0.4% |\n",
            "| :     |    866 |       0.4% |\n",
            "| ``    |    813 |       0.4% |\n",
            "| ''    |    785 |       0.4% |\n",
            "| WP    |    760 |       0.4% |\n",
            "| RP    |    755 |       0.4% |\n",
            "| UH    |    689 |       0.3% |\n",
            "| POS   |    684 |       0.3% |\n",
            "| HYPH  |    664 |       0.3% |\n",
            "| JJR   |    503 |       0.2% |\n",
            "| NNPS  |    498 |       0.2% |\n",
            "| JJS   |    383 |       0.2% |\n",
            "| EX    |    359 |       0.2% |\n",
            "| NFP   |    338 |       0.2% |\n",
            "| GW    |    294 |       0.1% |\n",
            "| ADD   |    292 |       0.1% |\n",
            "| RBR   |    276 |       0.1% |\n",
            "| $     |    258 |       0.1% |\n",
            "| PDT   |    175 |       0.1% |\n",
            "| RBS   |    169 |       0.1% |\n",
            "| SYM   |    156 |       0.1% |\n",
            "| LS    |    117 |       0.1% |\n",
            "| FW    |     93 |       0.0% |\n",
            "| AFX   |     48 |       0.0% |\n",
            "| WP$   |     15 |       0.0% |\n",
            "| XX    |      1 |       0.0% |\n",
            "+-------+--------+------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kgkoj6r0LgVG"
      },
      "source": [
        "### Creating an iterator\n",
        "\n",
        "We are going to use the `BucketIterator` to create iterators for our different sets, train, test and validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owBRob2J7iUo"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size = BATCH_SIZE,\n",
        "    device=device\n",
        ")"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xj5-U0T-MHEp"
      },
      "source": [
        "### Model building\n",
        "\n",
        "We are going to build a BiDirectional LSTM (BiLSTM) model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhCmBZhN7iRn"
      },
      "source": [
        "class BiLSTMPOSTagger(nn.Module):\n",
        "  def __init__(self,\n",
        "               input_dim,\n",
        "               embedding_dim,\n",
        "               hidden_dim,\n",
        "               output_dim,\n",
        "               pad_idx,\n",
        "               n_layers =2,\n",
        "               bidirectional=True,\n",
        "               dropout=.5,\n",
        "               ):\n",
        "    super(BiLSTMPOSTagger, self).__init__()\n",
        "    self.embedding = nn.Embedding(input_dim, embedding_dim,\n",
        "                                  padding_idx=pad_idx)\n",
        "    self.lstm = nn.LSTM(embedding_dim,\n",
        "                        hidden_dim,\n",
        "                        num_layers=n_layers,\n",
        "                        bidirectional = bidirectional,\n",
        "                        dropout = dropout if n_layers > 1 else 0\n",
        "                        )\n",
        "    self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim,\n",
        "                        output_dim )\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, text):\n",
        "    # text = [sent len, batch size]\n",
        "    embedded = self.dropout(self.embedding(text))\n",
        "    \n",
        "    # embedded = [sent len, batch size, emb dim]\n",
        "    outputs, (h_0, c_0) = self.lstm(embedded)\n",
        "    \"\"\"\n",
        "    outputs holds the backward and forward hidden states \n",
        "    in the final layer\n",
        "    \n",
        "    hidden and cell are the backward and forward hidden\n",
        "    and cell states at the final time-step\n",
        "    \n",
        "    output = [sent len, batch size, hid dim * n directions]\n",
        "    hidden/cell = [n layers * n directions, batch size, hid dim]\n",
        "    \n",
        "    we use our outputs to make a prediction of what the tag should be\n",
        "    \"\"\"\n",
        "    out = self.fc(self.dropout(outputs))\n",
        "    # out [sent len, batch size, output dim]\n",
        "    return out"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0vJuKlcPSQC"
      },
      "source": [
        "### Model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RV3UVhrW7iOK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31c62c96-c8e2-454a-86a8-1f8be68343ef"
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 128\n",
        "OUTPUT_DIM = len(PTB_TAGS.vocab)\n",
        "\n",
        "N_LAYERS = 2\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT = 0.25\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "model = BiLSTMPOSTagger(\n",
        "    INPUT_DIM, \n",
        "    EMBEDDING_DIM, \n",
        "    HIDDEN_DIM, \n",
        "    OUTPUT_DIM, \n",
        "    PAD_IDX,\n",
        "    n_layers= N_LAYERS, \n",
        "    bidirectional = BIDIRECTIONAL, \n",
        "    dropout= DROPOUT, \n",
        ")\n",
        "model"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BiLSTMPOSTagger(\n",
              "  (embedding): Embedding(8866, 100, padding_idx=1)\n",
              "  (lstm): LSTM(100, 128, num_layers=2, dropout=0.25, bidirectional=True)\n",
              "  (fc): Linear(in_features=256, out_features=51, bias=True)\n",
              "  (dropout): Dropout(p=0.25, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDyKcMGjQKVZ"
      },
      "source": [
        "We are going then to initialize the model weights using Normal distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAgJSinq7iLW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ded3c68a-5087-4af6-bc89-4a5b3a2ef7c5"
      },
      "source": [
        "def init_weights(m):\n",
        "  for name, param in m.named_parameters():\n",
        "    nn.init.normal_(param.data, mean=0, std=0.1)\n",
        "\n",
        "model.apply(init_weights)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BiLSTMPOSTagger(\n",
              "  (embedding): Embedding(8866, 100, padding_idx=1)\n",
              "  (lstm): LSTM(100, 128, num_layers=2, dropout=0.25, bidirectional=True)\n",
              "  (fc): Linear(in_features=256, out_features=51, bias=True)\n",
              "  (dropout): Dropout(p=0.25, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Si6Ar3pQiUz"
      },
      "source": [
        "Next we are going to count moedl parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZAND-q-7iIc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d722613-9395-46da-a64c-c4752f0daf42"
      },
      "source": [
        "def count_trainable_params(model):\n",
        "  return sum(p.numel() for p in model.parameters()), sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "n_params, trainable_params = count_trainable_params(model)\n",
        "print(f\"Total number of paramaters: {n_params:,}\\nTotal tainable parameters: {trainable_params:,}\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of paramaters: 1,530,491\n",
            "Total tainable parameters: 1,530,491\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OI0k3cD2Q_DC"
      },
      "source": [
        "We will then initialize the model embedding layer with pretained word vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NN030Tq7iFb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "592c4f4d-a532-4760-bbcd-b069469aa06d"
      },
      "source": [
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "model.embedding.weight.data.copy_(pretrained_embeddings)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.9269,  1.4873,  0.9007,  ...,  0.1233,  0.3499,  0.6173],\n",
              "        [ 0.7262,  0.0912, -0.3891,  ...,  0.0821,  0.4440, -0.7240],\n",
              "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
              "        ...,\n",
              "        [-0.6808,  0.5419, -1.5231,  ..., -1.1103, -0.5245, -0.9152],\n",
              "        [-0.5972,  0.0471, -0.2406,  ..., -0.9446, -0.1126, -0.2260],\n",
              "        [-1.8684, -1.3026, -0.8013,  ...,  0.2404,  0.4319, -1.3682]])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M46pzlraRhTC"
      },
      "source": [
        "We will then initialize the padding tokens to zeros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcVG7Zpw7iCJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c548f89a-3705-41b0-8d38-b797ce4e35d7"
      },
      "source": [
        "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "model.embedding.weight.data"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.9269,  1.4873,  0.9007,  ...,  0.1233,  0.3499,  0.6173],\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
              "        ...,\n",
              "        [-0.6808,  0.5419, -1.5231,  ..., -1.1103, -0.5245, -0.9152],\n",
              "        [-0.5972,  0.0471, -0.2406,  ..., -0.9446, -0.1126, -0.2260],\n",
              "        [-1.8684, -1.3026, -0.8013,  ...,  0.2404,  0.4319, -1.3682]])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veagGKatRujE"
      },
      "source": [
        "### Optimizer\n",
        "\n",
        "We are going to use the Adam optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRP9HKkQRsit"
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters())"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTjxB87lR61M"
      },
      "source": [
        "### Criterion/Loss Function\n",
        "\n",
        "Next we are going to define our loss function.\n",
        "\n",
        "Even though we have no <unk> tokens within our tag vocab, we still have <pad> tokens. This is because all sentences within a batch need to be the same size. However, we don't want to calculate the loss when the target is a <pad> token as we aren't training our model to recognize padding tokens.\n",
        "\n",
        "We handle this by setting the ignore_index in our loss function to the index of the padding token in our tag vocabulary.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q19NDJqkR6XA"
      },
      "source": [
        "TAG_PAD_IDX = PTB_TAGS.vocab.stoi[PTB_TAGS.pad_token]\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kh0Ep_6USMr1"
      },
      "source": [
        "### Model and criterion to GPU if available"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwXiC4hYSMJT"
      },
      "source": [
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oY9ma1jPTsdY"
      },
      "source": [
        "We dont want to caculate the accuracy over `<pad>` tokens as we are not interested in predicting them. So we are going to create a function that calculate the accuracy of non-padded tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6fgUyi7Tg1W"
      },
      "source": [
        "def categorical_accuracy(preds, y, tag_pad_idx):\n",
        "  max_preds = preds.argmax(dim = 1, keepdim = True)\n",
        "  non_pad_elements = (y != tag_pad_idx).nonzero()\n",
        "  correct = max_preds[non_pad_elements].squeeze(1).eq(y[non_pad_elements])\n",
        "  return correct.sum() / y[non_pad_elements].shape[0]"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2vKmsc_UUxo"
      },
      "source": [
        "### Training and evaluation functions\n",
        "\n",
        "The train function will remail unchanged."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vupm9OdyUUAa"
      },
      "source": [
        "def train(model, iterator,  optimizer, criterion, tag_pad_idx):\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "  model.train()\n",
        "  for batch in iterator:\n",
        "    text = batch.text # text = [sent len, batch size]\n",
        "    tags = batch.ptbtags # tags = [sent len, batch size]\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    predictions = model(text)\n",
        "    # predictions = [sent len, batch size, output dim]\n",
        "    predictions = predictions.view(-1, predictions.shape[-1])\n",
        "    # predictions = [sent len * batch size, output dim]\n",
        "    tags = tags.view(-1) # tags = [sent len * batch size]\n",
        "    loss = criterion(predictions, tags)\n",
        "    acc = categorical_accuracy(predictions, tags, tag_pad_idx)\n",
        "    \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    epoch_loss += loss.item()\n",
        "    epoch_acc += acc.item()\n",
        "\n",
        "  return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "\n",
        "def evaluate(model, iterator, criterion, tag_pad_idx):\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch in iterator:\n",
        "      text = batch.text # text = [sent len, batch size]\n",
        "      tags = batch.ptbtags # tags = [sent len, batch size]\n",
        "      predictions = model(text)\n",
        "      # predictions = [sent len, batch size, output dim]\n",
        "      predictions = predictions.view(-1, predictions.shape[-1])\n",
        "      # predictions = [sent len * batch size, output dim]\n",
        "      tags = tags.view(-1) # tags = [sent len * batch size]\n",
        "      loss = criterion(predictions, tags)\n",
        "      acc = categorical_accuracy(predictions, tags, tag_pad_idx)\n",
        "      \n",
        "      epoch_loss += loss.item()\n",
        "      epoch_acc += acc.item()\n",
        "  return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF-6wpeGXT1t"
      },
      "source": [
        "### Training loop\n",
        "\n",
        "We are going to have helper functions that will helps us to visualizing our trainig epoch\n",
        "\n",
        "1. Time to string function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpUeT8DyW2bS"
      },
      "source": [
        "def hms_string(sec_elapsed):\n",
        "    h = int(sec_elapsed / (60 * 60))\n",
        "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
        "    s = sec_elapsed % 60\n",
        "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00GbYksPXgmz"
      },
      "source": [
        "2. visualize training epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHnAJ34CXgYC"
      },
      "source": [
        "def visualize_training(start, end, train_loss, train_accuracy, val_loss, val_accuracy, title):\n",
        "  data = [\n",
        "       [\"Training\", f'{train_loss:.3f}', f'{train_accuracy:.3f}', f\"{hms_string(end - start)}\" ],\n",
        "       [\"Validation\", f'{val_loss:.3f}', f'{val_accuracy:.3f}', \"\" ],       \n",
        "  ]\n",
        "  table = PrettyTable([\"CATEGORY\", \"LOSS\", \"ACCURACY\", \"ETA\"])\n",
        "  table.align[\"CATEGORY\"] = 'l'\n",
        "  table.align[\"LOSS\"] = 'r'\n",
        "  table.align[\"ACCURACY\"] = 'r'\n",
        "  table.align[\"ETA\"] = 'r'\n",
        "  table.title = title\n",
        "  for row in data:\n",
        "    table.add_row(row)\n",
        "  print(table)\n",
        "  "
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T04toMT7XgWH",
        "outputId": "3f2b1560-c1ba-4f3f-dff1-cf8c490bfa7c"
      },
      "source": [
        "\n",
        "N_EPOCHS = 10\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "  start = time.time()\n",
        "  train_loss, train_acc = train(model, train_iterator, optimizer,\n",
        "                                criterion, TAG_PAD_IDX)\n",
        "  valid_loss, valid_acc = evaluate(model,\n",
        "                                   valid_iterator,\n",
        "                                   criterion, TAG_PAD_IDX)\n",
        "  \n",
        "  title = f\"EPOCH: {epoch+1:02}/{N_EPOCHS:02} {'saving best model...' if valid_loss < best_valid_loss else 'not saving...'}\"\n",
        "  if valid_loss < best_valid_loss:\n",
        "      best_valid_loss = valid_loss\n",
        "      torch.save(model.state_dict(), 'best-model.pt')\n",
        "  end = time.time()\n",
        "  visualize_training(start, end, train_loss, train_acc,\n",
        "                     valid_loss, valid_acc, title)\n",
        "  "
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------------------+\n",
            "|     EPOCH: 01/10 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 1.914 |    0.484 | 0:00:06.89 |\n",
            "| Validation | 1.028 |    0.722 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 02/10 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.666 |    0.814 | 0:00:06.57 |\n",
            "| Validation | 0.675 |    0.811 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 03/10 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.455 |    0.869 | 0:00:06.53 |\n",
            "| Validation | 0.574 |    0.834 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 04/10 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.366 |    0.893 | 0:00:06.56 |\n",
            "| Validation | 0.532 |    0.841 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 05/10 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.316 |    0.907 | 0:00:06.71 |\n",
            "| Validation | 0.503 |    0.849 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 06/10 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.280 |    0.917 | 0:00:06.55 |\n",
            "| Validation | 0.495 |    0.855 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 07/10 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.253 |    0.924 | 0:00:06.61 |\n",
            "| Validation | 0.481 |    0.854 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 08/10 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.234 |    0.929 | 0:00:06.61 |\n",
            "| Validation | 0.471 |    0.860 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 09/10 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.216 |    0.934 | 0:00:06.58 |\n",
            "| Validation | 0.455 |    0.863 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 10/10 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.204 |    0.937 | 0:00:06.60 |\n",
            "| Validation | 0.454 |    0.861 |            |\n",
            "+------------+-------+----------+------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UrjKsZFYXSt"
      },
      "source": [
        "### Evaluating the best model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zujOKg0XgRw",
        "outputId": "18d97925-c8bc-4687-9f40-b6699ad20e5b"
      },
      "source": [
        "def visualize_test(start, end, test_loss,\n",
        "                       test_accuracy, title):\n",
        "  data = [\n",
        "       [\"test\", f'{test_loss:.3f}', f'{test_accuracy:.3f}', f\"{hms_string(end - start)}\" ],       \n",
        "  ]\n",
        "  table = PrettyTable([\"CATEGORY\", \"LOSS\", \"ACCURACY\", \"ETA\"])\n",
        "  table.align[\"CATEGORY\"] = 'l'\n",
        "  table.align[\"LOSS\"] = 'r'\n",
        "  table.align[\"ACCURACY\"] = 'r'\n",
        "  table.align[\"ETA\"] = 'r'\n",
        "  table.title = title\n",
        "  for row in data:\n",
        "    table.add_row(row)\n",
        "  print(table)\n",
        "  \n",
        "\n",
        "model.load_state_dict(torch.load('best-model.pt'))\n",
        "\n",
        "start = time.time()\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion, tag_pad_idx=TAG_PAD_IDX)\n",
        "end = time.time()\n",
        "\n",
        "visualize_test(start, end, test_loss, test_acc, \"MODEL EVALUATION SUMMARY\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------+\n",
            "|         MODEL EVALUATION SUMMARY         |\n",
            "+----------+-------+----------+------------+\n",
            "| CATEGORY |  LOSS | ACCURACY |        ETA |\n",
            "+----------+-------+----------+------------+\n",
            "| test     | 0.484 |    0.862 | 0:00:00.13 |\n",
            "+----------+-------+----------+------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efX2Tmh-f6y1"
      },
      "source": [
        "### Model inference\n",
        "\n",
        "Now we are ready to create out `tag_sentence` function that will:\n",
        "\n",
        "* put the model into evaluation mode\n",
        "* tokenize the sentence with spaCy if it is not a list\n",
        "* lowercase the tokens if the Field did\n",
        "numericalize the tokens using the vocabulary\n",
        "* find out which tokens are not in the vocabulary, i.e. are ``<unk>`` tokens\n",
        "convert the numericalized tokens into a tensor and add a batch dimension\n",
        "* feed the tensor into the model\n",
        "* get the predictions over the sentence\n",
        "* convert the predictions into readable tags\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtO7KhE0f6oU"
      },
      "source": [
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()\n",
        "def tag_sentence(model, device, sentence, text_field, tag_field):\n",
        "  model.eval()\n",
        "\n",
        "  if isinstance(sentence, str):\n",
        "    tokens = [token.text for token in nlp.tokenizer(sentence)]\n",
        "  else:\n",
        "    tokens = [token for token in sentence]\n",
        "\n",
        "  if text_field.lower:\n",
        "    tokens = [t.lower() for t in tokens]\n",
        "\n",
        "  numericalized_tokens = [text_field.vocab.stoi[t] for t in tokens]\n",
        "  unk_idx = text_field.vocab.stoi[text_field.unk_token]\n",
        "  unks = [t for t, n in zip(tokens, numericalized_tokens) if n == unk_idx]\n",
        "  token_tensor = torch.LongTensor(numericalized_tokens)\n",
        "  token_tensor = token_tensor.unsqueeze(-1).to(device)\n",
        "  predictions = model(token_tensor)\n",
        "  top_predictions = predictions.argmax(-1)\n",
        "  predicted_tags = [tag_field.vocab.itos[t.item()] for t in top_predictions]\n",
        "  return tokens, predicted_tags, unks\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngKMHE_Vh2Ll"
      },
      "source": [
        "Taking a single example on our train data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKPnYqBgf6lt",
        "outputId": "511e380f-5870-46e0-abba-39852fa3c1b7"
      },
      "source": [
        "example_index = 1\n",
        "\n",
        "sentence = vars(train_data.examples[example_index])['text']\n",
        "actual_tags = vars(train_data.examples[example_index])['ptbtags']\n",
        "print(sentence)\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[', 'this', 'killing', 'of', 'a', 'respected', 'cleric', 'will', 'be', 'causing', 'us', 'trouble', 'for', 'years', 'to', 'come', '.', ']']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_x1eczyf6jC",
        "outputId": "b5475e8f-b07a-4e46-d3c9-e084060d30f5"
      },
      "source": [
        "tokens, pred_tags, unks = tag_sentence(model, \n",
        "                                       device, \n",
        "                                       sentence, \n",
        "                                       TEXT, \n",
        "                                       PTB_TAGS)\n",
        "\n",
        "print(unks) # 'respected', 'cleric' have unkown tags"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['respected', 'cleric']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQWdK7UpimU9"
      },
      "source": [
        "We can check how correct the nodel is right now, the model missed \"respected\" because it is an unknown word in the cu=orpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9Az4sRQf6fc",
        "outputId": "a94f1917-7161-46fe-bc86-07c2042eabb0"
      },
      "source": [
        "print(\"Pred. Tag\\tActual Tag\\tCorrect?\\tToken\\n\")\n",
        "\n",
        "for token, pred_tag, actual_tag in zip(tokens, pred_tags, actual_tags):\n",
        "    correct = '' if pred_tag == actual_tag else ''\n",
        "    print(f\"{pred_tag}\\t\\t{actual_tag}\\t\\t{correct}\\t\\t{token}\")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pred. Tag\tActual Tag\tCorrect?\tToken\n",
            "\n",
            "-LRB-\t\t-LRB-\t\t\t\t[\n",
            "DT\t\tDT\t\t\t\tthis\n",
            "NN\t\tNN\t\t\t\tkilling\n",
            "IN\t\tIN\t\t\t\tof\n",
            "DT\t\tDT\t\t\t\ta\n",
            "NN\t\tJJ\t\t\t\trespected\n",
            "NN\t\tNN\t\t\t\tcleric\n",
            "MD\t\tMD\t\t\t\twill\n",
            "VB\t\tVB\t\t\t\tbe\n",
            "VBG\t\tVBG\t\t\t\tcausing\n",
            "PRP\t\tPRP\t\t\t\tus\n",
            "NN\t\tNN\t\t\t\ttrouble\n",
            "IN\t\tIN\t\t\t\tfor\n",
            "NNS\t\tNNS\t\t\t\tyears\n",
            "TO\t\tTO\t\t\t\tto\n",
            "VB\t\tVB\t\t\t\tcome\n",
            ".\t\t.\t\t\t\t.\n",
            "-RRB-\t\t-RRB-\t\t\t\t]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxCFTP0vi9lk"
      },
      "source": [
        "Next we will make our own sentence and test this out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5TkzXAuf6c8",
        "outputId": "85ae4542-46e3-4973-8824-37b1694d0f27"
      },
      "source": [
        "sentence = 'The Queen will deliver a speech about the conflict in North Korea at 1pm tomorrow.'\n",
        "\n",
        "tokens, tags, unks = tag_sentence(model, \n",
        "                                  device, \n",
        "                                  sentence, \n",
        "                                  TEXT, \n",
        "                                  PTB_TAGS)\n",
        "\n",
        "print(unks) # we cont have unknowns here."
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01D4EBF7f6Zq",
        "outputId": "3c89d184-a3ad-4715-e0d1-0eafca12d9cd"
      },
      "source": [
        "print(\"Pred. Tag\\tToken\\n\")\n",
        "for token, tag in zip(tokens, tags):\n",
        "    print(f\"{tag}\\t\\t{token}\")"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pred. Tag\tToken\n",
            "\n",
            "DT\t\tthe\n",
            "NN\t\tqueen\n",
            "MD\t\twill\n",
            "VB\t\tdeliver\n",
            "DT\t\ta\n",
            "NN\t\tspeech\n",
            "IN\t\tabout\n",
            "DT\t\tthe\n",
            "NN\t\tconflict\n",
            "IN\t\tin\n",
            "NNP\t\tnorth\n",
            "NNP\t\tkorea\n",
            "IN\t\tat\n",
            "CD\t\t1\n",
            "NN\t\tpm\n",
            "NN\t\ttomorrow\n",
            ".\t\t.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFiD--CwjRwd"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "Next we will be looking at PoS tagging using fine tuned transformers.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXM-sH7GjvG2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}