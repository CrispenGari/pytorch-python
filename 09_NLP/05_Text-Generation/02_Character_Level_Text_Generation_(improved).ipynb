{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02_Character_Level_Text_Generation_(improved).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNucv6yiW6vJ"
      },
      "source": [
        "### character Level text generator (modified)\n",
        "\n",
        "This notebook will be based on the previous notebook. The goal is to improve the accuracy of the text generation model as compared to the previously created model.\n",
        "\n",
        "Code cells from the previous notebook will remain the same. We are going to make few changes.\n",
        "\n",
        "\n",
        "_\"The model will be fed with a word and will predict what the next character in the sentence will be. This process will repeat itself until we generate a sentence of our desired length\"._\n",
        "\n",
        "### Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Ftc0QphfW6kq",
        "outputId": "029bf367-0f6c-4a19-974d-fc912d5732d8"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import os, time, pickle, string, random\n",
        "import numpy as np\n",
        "\n",
        "torch.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.9.0+cu102'"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mq3jWa3UXsvY"
      },
      "source": [
        "### Device"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0Nt2gYpW6ho",
        "outputId": "413111b0-7719-4438-f238-1af0913141d4"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yB05im5QX8C3"
      },
      "source": [
        "### Data\n",
        "We are going to use the dataset that I've downloaded [here](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt) we are then going to load it from google drive and the steps will be as follows:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUkux6zRW6ea",
        "outputId": "acd90594-7df2-44ed-8f37-5473831f6fa2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2p7xG1SW6ba",
        "outputId": "a79c054a-a9c4-4d95-eb9f-1d81888926b2"
      },
      "source": [
        "file_path = \"/content/drive/My Drive/NLP Data/text-gen/input.txt\"\n",
        "os.path.exists(file_path)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "re4DQWwwY6nw"
      },
      "source": [
        "### Loading the dataset.\n",
        "\n",
        "First, we'll define the sentences that we want our model to output the first few characters. Our dataset is a text file containing Shakespeare's plays or books that we will extract sequence of chars to use as input to our model. Then our model will learn how to complete sentences like \"Shakespeare would do\".\n",
        "\n",
        "### SEEDS\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJG0ThpFW6Yi"
      },
      "source": [
        "SEED = 42\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf4RIfW2Z5g_"
      },
      "source": [
        "### Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dg4noe2aW6Vr"
      },
      "source": [
        "def load_text_data(filename, init_dialog=False):\n",
        "  \"\"\"\n",
        "  Setting init_dialog = True will remove lines where the character who is going to speak is indicate\n",
        "  \"\"\"\n",
        "  sentences = []\n",
        "  with open(filename, 'r') as reader:\n",
        "    for line in reader:\n",
        "      if init_dialog or \":\" not in line:\n",
        "        sentences.append(line[:-1])\n",
        "\n",
        "  return sentences\n",
        "  "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-TOFo2GW6St",
        "outputId": "b534a738-5595-4016-8475-c515f27f1231"
      },
      "source": [
        "sentences = load_text_data(file_path)\n",
        "print('Number of sentences: ', len(sentences))\n",
        "print(sentences[:20])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sentences:  29723\n",
            "['Before we proceed any further, hear me speak.', '', 'Speak, speak.', '', 'You are all resolved rather to die than to famish?', '', 'Resolved. resolved.', '', 'First, you know Caius Marcius is chief enemy to the people.', '', \"We know't, we know't.\", '', \"Let us kill him, and we'll have corn at our own price.\", \"Is't a verdict?\", '', '', 'One word, good citizens.', '', 'We are accounted poor citizens, the patricians good.', 'would yield us but the superfluity, while it were']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPCxCtj6bEe7"
      },
      "source": [
        "### Data Cleaning\n",
        "We will convert to lowercase the text and remove non alphanumeric chracters (a parameter configuration)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tppWbqp-W6Pt"
      },
      "source": [
        "def clean_text(sentences, alpha=False):\n",
        "  if alpha:\n",
        "    # Remove non alphabetic character\n",
        "    cleaned_text = [''.join([t.lower() for t in text if t.isalpha() or t.isspace()])\n",
        "      for text in sentences]\n",
        "  else:\n",
        "    cleaned_text = [t.lower() for t in sentences]\n",
        "  \n",
        "  return [t for t in cleaned_text if t!='']\n",
        "  "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1w17i4llW6Mr",
        "outputId": "4d6d48f3-f846-4cec-c5b8-499aadf4e4bc"
      },
      "source": [
        "# Clean the sentences\n",
        "sentences = clean_text(sentences)\n",
        "# Join all the sentences in a one long string\n",
        "sentences = ' '.join(sentences)\n",
        "print('Number of characters: ', len(sentences))\n",
        "print(sentences[:100])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of characters:  894876\n",
            "before we proceed any further, hear me speak. speak, speak. you are all resolved rather to die than \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcYXSQLjcumC"
      },
      "source": [
        "### Creating the dictionary\n",
        "\n",
        "Now we'll create a dictionary out of all the characters that we have in the sentences and map them to an integer. This will allow us to convert our input characters to their respective integers (char2int) and viceversa (int2char)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShJeB5f4W6Jw"
      },
      "source": [
        "class CharVocab:\n",
        "  def __init__(self, type_vocab,\n",
        "               pad_token = \"<pad>\",\n",
        "               eos_token = \"<eos>\",\n",
        "               unk_token = \"<unk>\"\n",
        "               ):\n",
        "    self.type = type_vocab\n",
        "    self.int2char = []\n",
        "    if pad_token !=None:\n",
        "      self.int2char += [pad_token]\n",
        "    if eos_token !=None:\n",
        "      self.int2char += [eos_token]\n",
        "    if unk_token !=None: \n",
        "      self.int2char += [unk_token]\n",
        "\n",
        "    self.char2int = {}\n",
        "  \n",
        "  def __call__(self, text):\n",
        "    chars = set(''.join(text))\n",
        "    self.int2char += list(chars)\n",
        "    self.char2int = {char: ind for ind, char in enumerate(self.int2char)}"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBbRyY7wW6Gz",
        "outputId": "5cb78026-c4b7-4272-9413-120ad1c72f80"
      },
      "source": [
        "\n",
        "vocab = CharVocab('char', None, None,'<unk>')\n",
        "vocab(sentences)\n",
        "print('Length of vocabulary: ', len(vocab.int2char))\n",
        "print('Int to Char: ', vocab.int2char)\n",
        "print('Char to Int: ', vocab.char2int)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of vocabulary:  38\n",
            "Int to Char:  ['<unk>', 'u', 'z', ';', 'q', 'e', 'b', 'f', ',', '.', 'r', 'h', 's', \"'\", '$', 'y', 'm', 'a', 't', '-', '&', 'x', 'n', 'p', 'w', '?', 'd', 'k', 'i', ' ', 'o', 'c', 'g', 'j', 'v', '!', 'l', '3']\n",
            "Char to Int:  {'<unk>': 0, 'u': 1, 'z': 2, ';': 3, 'q': 4, 'e': 5, 'b': 6, 'f': 7, ',': 8, '.': 9, 'r': 10, 'h': 11, 's': 12, \"'\": 13, '$': 14, 'y': 15, 'm': 16, 'a': 17, 't': 18, '-': 19, '&': 20, 'x': 21, 'n': 22, 'p': 23, 'w': 24, '?': 25, 'd': 26, 'k': 27, 'i': 28, ' ': 29, 'o': 30, 'c': 31, 'g': 32, 'j': 33, 'v': 34, '!': 35, 'l': 36, '3': 37}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Np4ReBM7eftJ"
      },
      "source": [
        "We are then going to save the dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Id9Y6Mb-W6EC",
        "outputId": "450ede59-f667-40c9-9be3-1b9df9d8dfd6"
      },
      "source": [
        "with open( \"/content/drive/My Drive/NLP Data/text-gen/char_dict.plk\", \"wb\") as f:\n",
        "  pickle.dump(vocab.char2int, f)\n",
        "\n",
        "with open( \"/content/drive/My Drive/NLP Data/text-gen/int_dict.plk\", \"wb\") as f:\n",
        "  pickle.dump(vocab.int2char, f)\n",
        "\n",
        "print(\"Done\")\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMqR-PQqfA_w"
      },
      "source": [
        "### Creating the input data and labels for training.\n",
        "\n",
        "As we're going to predict the next character in the sequence at each time step, we'll have to divide each sentence into\n",
        "\n",
        "* **Input data:** The last input character should be excluded as it does not need to be fed into the model\n",
        "* **Target/Ground Truth Label:** One time-step ahead of the Input data as this will be the \"correct answer\" for the model at each time step corresponding to the input data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRNTPk9wW6BK"
      },
      "source": [
        "def one_hot_encode(indices, dict_size):\n",
        "  features = np.eye(dict_size, dtype=\"float32\")[indices.flatten()]\n",
        "  features = features.reshape((*indices.shape, dict_size))\n",
        "  return features\n",
        "\n",
        "def encode_text(input_text, vocab, one_hot=False):\n",
        "  ''' Encode the input_text replacing the char by its integer number based on the dictionary vocab'''\n",
        "  output = [vocab.char2int.get(character,0) for character in input_text]\n",
        "  if one_hot:\n",
        "    dict_size = len(vocab.char2int)\n",
        "    return one_hot_encode(output, dict_size)\n",
        "  else:\n",
        "    return np.array(output)\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85J3dCcv1Z3p"
      },
      "source": [
        "Now, we can encode our text, replacing every character by the integer value in the dictionary. When we have our dataset unified and prepared, we should do a quick check and see an example of the data our model will be trained on. This is generally a good idea as it allows you to see how each of the further processing steps affects the reviews and it also ensures that the data has been loaded correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SCrdAyzW57f",
        "outputId": "cc298f8d-5d70-4e53-96df-938fce44cc61"
      },
      "source": [
        "train_data = encode_text(sentences, vocab, one_hot=False)\n",
        "\n",
        "# Create the input sequence, from 0 to len-1\n",
        "input_seq =train_data[:-1]\n",
        "\n",
        "# Create the target sequence, from 1 to len. It is right-shifted one place\n",
        "\n",
        "target_seq=train_data[1:]\n",
        "print('\\nOriginal text:')\n",
        "print(sentences[:100])\n",
        "print('\\nEncoded text:')\n",
        "print(train_data[:100])\n",
        "print('\\nInput sequence:')\n",
        "print(input_seq[:100])\n",
        "print('\\nTarget sequence:')\n",
        "print(target_seq[:100])\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Original text:\n",
            "before we proceed any further, hear me speak. speak, speak. you are all resolved rather to die than \n",
            "\n",
            "Encoded text:\n",
            "[ 6  5  7 30 10  5 29 24  5 29 23 10 30 31  5  5 26 29 17 22 15 29  7  1\n",
            " 10 18 11  5 10  8 29 11  5 17 10 29 16  5 29 12 23  5 17 27  9 29 12 23\n",
            "  5 17 27  8 29 12 23  5 17 27  9 29 15 30  1 29 17 10  5 29 17 36 36 29\n",
            " 10  5 12 30 36 34  5 26 29 10 17 18 11  5 10 29 18 30 29 26 28  5 29 18\n",
            " 11 17 22 29]\n",
            "\n",
            "Input sequence:\n",
            "[ 6  5  7 30 10  5 29 24  5 29 23 10 30 31  5  5 26 29 17 22 15 29  7  1\n",
            " 10 18 11  5 10  8 29 11  5 17 10 29 16  5 29 12 23  5 17 27  9 29 12 23\n",
            "  5 17 27  8 29 12 23  5 17 27  9 29 15 30  1 29 17 10  5 29 17 36 36 29\n",
            " 10  5 12 30 36 34  5 26 29 10 17 18 11  5 10 29 18 30 29 26 28  5 29 18\n",
            " 11 17 22 29]\n",
            "\n",
            "Target sequence:\n",
            "[ 5  7 30 10  5 29 24  5 29 23 10 30 31  5  5 26 29 17 22 15 29  7  1 10\n",
            " 18 11  5 10  8 29 11  5 17 10 29 16  5 29 12 23  5 17 27  9 29 12 23  5\n",
            " 17 27  8 29 12 23  5 17 27  9 29 15 30  1 29 17 10  5 29 17 36 36 29 10\n",
            "  5 12 30 36 34  5 26 29 10 17 18 11  5 10 29 18 30 29 26 28  5 29 18 11\n",
            " 17 22 29 18]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9rgA3kLW54z"
      },
      "source": [
        "Now we can save our encoded dataset to a file, so we can restore it whenever it is necessary. It is important to note the format of the data that we are saving as we will need to know it when we write the training code. In our case, we will save the dataset as a pickle object, it is the array containing the whole dataset encoded as an integer value for every character."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnj3WdiEWvdV",
        "outputId": "398d38ec-1679-4bad-9261-5be80082eeb3"
      },
      "source": [
        "with open( \"/content/drive/My Drive/NLP Data/text-gen/input_data.plk\", \"wb\") as f:\n",
        "  pickle.dump(train_data, f)\n",
        "\n",
        "print(\"Done\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AnCATVZ9f7e"
      },
      "source": [
        "Lets check our one-hot-encode function that we will use later during the training phase:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_R_O0xW9eCv",
        "outputId": "fb71ca3b-d8fb-4194-f358-34aa50d05332"
      },
      "source": [
        "print('Encoded characters: ',train_data[100:107])\n",
        "print('One-hot-encoded characters: ',one_hot_encode(train_data[100:107], 38))\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded characters:  [18 30 29  7 17 16 28]\n",
            "One-hot-encoded characters:  [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSib-veX9ydJ"
      },
      "source": [
        "### Creating a batch data generator\n",
        "\n",
        "When training on the dataset, we need to extract a batch size examples from the inputs and targets, forward and backward the RNN and then repite the iteration with another batch size examples. A batch generator will help us to extract a batch size examples from our datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKEiWg1d9h0U"
      },
      "source": [
        "def batch_generator_sequence(\n",
        "    features_seq, label_seq, batch_size, seq_len\n",
        "  ):\n",
        "  \"\"\"Generator function that yields batches of data (input and target)\n",
        "\n",
        "  Args:\n",
        "      features_seq: sequence of chracters, feature of our model.\n",
        "      label_seq: sequence of chracters, the target label of our model\n",
        "      batch_size (int): number of examples (in this case, sentences) per batch.\n",
        "      seq_len (int): maximum length of the output tensor.\n",
        "\n",
        "  Yields:\n",
        "      x_epoch: sequence of features for the epoch\n",
        "      y_epoch: sequence of labels for the epoch\n",
        "  \"\"\"\n",
        "  num_batches = len(features_seq) //(batch_size * seq_len)\n",
        "  if num_batches == 0:\n",
        "    raise ValueError(\"No batches created. Use smaller batch size or sequence length.\")\n",
        "\n",
        "  # calculate effective length of text to use\n",
        "  rounded_len = num_batches * batch_size * seq_len\n",
        "  # Reshape the features matrix in batch size x num_batches * seq_len\n",
        "  x = np.reshape(features_seq[: rounded_len], [batch_size, num_batches * seq_len])\n",
        "  # Reshape the target matrix in batch size x num_batches * seq_len\n",
        "  y = np.reshape(label_seq[: rounded_len], [batch_size, num_batches * seq_len])\n",
        "\n",
        "  epoch = 0\n",
        "  while True:\n",
        "    # roll so that no need to reset rnn states over epochs\n",
        "    x_epoch = np.split(np.roll(x, -epoch, axis=0), num_batches, axis=1)\n",
        "    y_epoch = np.split(np.roll(y, -epoch, axis=0), num_batches, axis=1)\n",
        "    for batch in range(num_batches):\n",
        "        yield x_epoch[batch], y_epoch[batch]\n",
        "    epoch += 1\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fW-DXdkb_h0r"
      },
      "source": [
        "### Define RNN model\n",
        "\n",
        "The model is very simple:\n",
        "* An LSTM layer to encode the input (there is no need for an embedding layer because the data is one-hot-encoded)\n",
        "* A dropout layer to reduce overfitting\n",
        "* The decoder, a fully connected layer mapping to a vocabulary size outputs\n",
        "\n",
        "The output provides the probability of every item in the vocabulary to be the next char.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4pXmSgc_yTY"
      },
      "source": [
        "class RNNModel(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_size,\n",
        "               hidden_dim, n_layers, dropout=.2):\n",
        "    super(RNNModel, self).__init__()\n",
        "\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.embedding_size = embedding_size\n",
        "    self.n_layers = n_layers\n",
        "    self.vocab_size = vocab_size\n",
        "    self.dropout = dropout\n",
        "    self.char2int = None\n",
        "    self.int2char = None\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.rnn = nn.LSTM(embedding_size, hidden_dim, \n",
        "                        n_layers, dropout=dropout, batch_first = True)\n",
        "    self.decoder = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "  def forward(self, x, state):\n",
        "    # input shape: [batch_size, seq_len, embedding_size]\n",
        "    out, state = self.rnn(x, state)\n",
        "    # out shape: [batch_size, seq_len, rnn_size]\n",
        "    # hidden shape: [2, num_layers, batch_size, rnn_size]\n",
        "    out = self.dropout(out)\n",
        "    out = out.contiguous().view(-1, self.hidden_dim)\n",
        "    logits = self.decoder(out)\n",
        "    # output shape: [seq_len * batch_size, vocab_size]\n",
        "    return logits, state\n",
        "\n",
        "  def init_state(self, device, batch_size=1):\n",
        "    \"\"\"\n",
        "    initialises rnn states.\n",
        "    \"\"\"\n",
        "    return (torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device),\n",
        "                torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device))\n",
        "    \n",
        "  def predict(self, input):\n",
        "    # input shape: [seq_len, batch_size]\n",
        "    logits, hidden = self.forward(input)\n",
        "    # logits shape: [seq_len * batch_size, vocab_size]\n",
        "    # hidden shape: [2, num_layers, batch_size, rnn_size]\n",
        "    probs = F.softmax(logits)\n",
        "    # shape: [seq_len * batch_size, vocab_size]\n",
        "    probs = probs.view(input.size(0), input.size(1), probs.size(1))\n",
        "    # output shape: [seq_len, batch_size, vocab_size]\n",
        "    return probs, hidden\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCniV2XDBcHp"
      },
      "source": [
        "### Training the model\n",
        "During training:\n",
        "* In every epoch get the next batch data, move the tensors to the device, call the model (Forward pass), calculate the loss function, get the gradients and update the weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRmbuKBbBah3"
      },
      "source": [
        "def train_main(model, optimizer, loss_fn,\n",
        "               batch_data, num_batches, val_batches, \n",
        "               batch_size, seq_len, n_epochs,\n",
        "               clip_norm, device\n",
        "  ):\n",
        "  # Training Run\n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "    # Store the loss in every batch iteration\n",
        "    #epoch_losses = torch.Tensor(num_batches)\n",
        "    epoch_losses = []\n",
        "    # Init the hidden state\n",
        "    hidden = model.init_state(device, batch_size)\n",
        "    # Train all the batches in every epoch\n",
        "    print(\"Epoch {}/{}\".format(epoch, n_epochs+1))\n",
        "    for i in range(num_batches-val_batches):\n",
        "      # Get the next batch data for input and target\n",
        "      input_batch, target_batch = next(batch_data)\n",
        "      # Onr hot encode the input data\n",
        "      input_batch = one_hot_encode(input_batch, model.vocab_size)\n",
        "      # Tranform to tensor\n",
        "      input_data = torch.from_numpy(input_batch)\n",
        "      target_data = torch.from_numpy(target_batch)\n",
        "      # Create a new variable for the hidden state, necessary to calculate the gradients\n",
        "      hidden = tuple(([Variable(var.data) for var in hidden]))\n",
        "      # Move the input data to the device\n",
        "      input_data = input_data.to(device)\n",
        "      #print('Input shape: ', input_data.shape)\n",
        "      #print('Hidden shape: ', hidden[0].shape, hidden[1].shape)\n",
        "      # Set the model to train and prepare the gradients\n",
        "      model.train()\n",
        "      optimizer.zero_grad() # Clears existing gradients from previous epoch\n",
        "      # Pass Fordward the RNN\n",
        "      output, hidden = model(input_data, hidden)\n",
        "      #print('Output shape: ', output.shape)\n",
        "      output = output.to(device)\n",
        "      #print('Output shape: ', output.shape)\n",
        "      #print('Target shape; ', target_data.shape)\n",
        "      # Move the target data to the device\n",
        "      target_data = target_data.to(device)\n",
        "      #print('Target shape; ', target_data.shape)\n",
        "      target_data = torch.reshape(target_data, (batch_size*seq_len,))\n",
        "      #print('Target shape; ', target_data.shape)\n",
        "      loss = loss_fn(output, target_data.view(batch_size*seq_len))\n",
        "      #print(loss)\n",
        "      # Save the loss\n",
        "      #epoch_losses[i] = loss.item() #data[0]\n",
        "      epoch_losses.append(loss.item()) #data[0]\n",
        "  \n",
        "      loss.backward() # Does backpropagation and calculates gradients\n",
        "      # clip gradient norm\n",
        "      nn.utils.clip_grad_norm_(model.parameters(), clip_norm)\n",
        "      \n",
        "      optimizer.step() # Updates the weights accordingly\n",
        "\n",
        "    # Now, when epoch is finished, evaluate the model on validation data\n",
        "    model.eval()\n",
        "    val_hidden = model.init_state(device, batch_size)\n",
        "    val_losses = []\n",
        "    print(\"Val Epoch {}/{}\".format(epoch, n_epochs+1))\n",
        "    for i in range(val_batches):\n",
        "      # Get the next batch data for input and target\n",
        "      input_batch, target_batch = next(batch_data)\n",
        "      # Onr hot encode the input data\n",
        "      input_batch = one_hot_encode(input_batch, model.vocab_size)\n",
        "      # Tranform to tensor\n",
        "      input_data = torch.from_numpy(input_batch)\n",
        "      target_data = torch.from_numpy(target_batch)\n",
        "      # Create a new variable for the hidden state, necessary to calculate the gradients\n",
        "      hidden = tuple(([Variable(var.data) for var in val_hidden]))\n",
        "      # Move the input data to the device\n",
        "      input_data = input_data.to(device)\n",
        "      # Pass Fordward the RNN\n",
        "      output, hidden = model(input_data, hidden)\n",
        "      #print('Output shape: ', output.shape)\n",
        "      output = output.to(device)\n",
        "      #print('Output shape: ', output.shape)\n",
        "      #print('Target shape; ', target_data.shape)\n",
        "      # Move the target data to the device\n",
        "      target_data = target_data.to(device)\n",
        "      #print('Target shape; ', target_data.shape)\n",
        "      target_data = torch.reshape(target_data, (batch_size*seq_len,))\n",
        "      #print('Target shape; ', target_data.shape)\n",
        "      loss = loss_fn(output, target_data.view(batch_size*seq_len))\n",
        "      #print(loss)\n",
        "      # Save the loss\n",
        "      val_losses.append(loss.item()) #data[0]\n",
        "\n",
        "    model.train()                  \n",
        "    print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
        "    print(\"Train Loss: {:.4f}\".format(np.mean(epoch_losses)), end=' ')\n",
        "    print(\"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n",
        "    print()\n",
        "\n",
        "    if epoch != n_epochs:\n",
        "      print(\"> Next epoch\")\n",
        "  return epoch_losses\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8kBR9LdCu8C"
      },
      "source": [
        "After defining the model above, we'll have to instantiate the model with the relevant parameters and define our hyperparamters as well. The hyperparameters we're defining below are:\n",
        "\n",
        "* n_epochs: Number of Epochs --> This refers to the number of times our model will go through the entire training dataset\n",
        "* lr: Learning Rate --> This affects the rate at which our model updates the weights in the cells each time backpropogation is done\n",
        "  * A smaller learning rate means that the model changes the values of the weight with a smaller magnitude\n",
        "  * A larger learning rate means that the weights are updated to a larger extent for each time step\n",
        "* batch_size: Number of examples to train on every train step\n",
        "maxlen: Length of the input sequence of char\n",
        "* embedding_size: the vocab size because the input feature is one-hot-encoded\n",
        "* hidden_dim: the number of hidden units in our LSTM module\n",
        "* n_layers: number of layers of our LSTM module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WWJdPIqC7U0"
      },
      "source": [
        "# Define hyperparameters for training\n",
        "n_epochs = 20\n",
        "# lr=0.001\n",
        "batch_size=16\n",
        "maxlen=64\n",
        "clip_norm=5\n",
        "val_fraction = 0.1\n",
        "\n",
        "# Define hypeparameters of the model\n",
        "hidden_dim = 512 #64\n",
        "n_layers = 1\n",
        "embedding_size=len(vocab.char2int)\n",
        "dict_size = len(vocab.char2int)\n",
        "drop_rate = 0.5"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hmAgnH2DDyi"
      },
      "source": [
        "Optimizer and loss functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXTry7D5DB8Y",
        "outputId": "bee5704d-a50f-448e-acc6-084c46efb5a6"
      },
      "source": [
        "model = RNNModel(dict_size,embedding_size, hidden_dim, n_layers)\n",
        "model"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNNModel(\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              "  (rnn): LSTM(38, 512, batch_first=True, dropout=0.2)\n",
              "  (decoder): Linear(in_features=512, out_features=38, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A92hN22bDMqy"
      },
      "source": [
        "Counting model parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvRM3LObDJwM",
        "outputId": "7bb4b597-2605-407a-db57-b860c88985ac"
      },
      "source": [
        "def count_trainable_params(model):\n",
        "  n_t_params =sum(p.numel() for p in model.parameters() if p.requires_grad == True)\n",
        "  return n_t_params\n",
        "\n",
        "print(f'The model has {count_trainable_params(model):,} trainable parameters')\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 1,149,990 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8DVZ-oVDT7_",
        "outputId": "ed7539af-230d-49b7-9408-30f80c44fa0b"
      },
      "source": [
        "model = model.to(device)\n",
        "print(model)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNNModel(\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (rnn): LSTM(38, 512, batch_first=True, dropout=0.2)\n",
            "  (decoder): Linear(in_features=512, out_features=38, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKJ2hBVhDaHX"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters())"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63B4UidrDdUi",
        "outputId": "c329bcaf-2573-419e-dbe5-dc1e3b716c65"
      },
      "source": [
        "\n",
        "# Calculate the number of batches to train\n",
        "num_batches = len(input_seq) // (batch_size*maxlen)\n",
        "val_batches = int(num_batches*val_fraction)\n",
        "\n",
        "# Create the batch data generator\n",
        "batch_data = batch_generator_sequence(input_seq, target_seq, batch_size, maxlen)\n",
        "losses = train_main(model, optimizer, criterion, batch_data, num_batches, val_batches, batch_size, \n",
        "                    maxlen, n_epochs, clip_norm, device)\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/21\n",
            "Val Epoch 1/21\n",
            "Epoch: 1/20............. Train Loss: 2.2613 Val Loss: 1.9667\n",
            "\n",
            "> Next epoch\n",
            "Epoch 2/21\n",
            "Val Epoch 2/21\n",
            "Epoch: 2/20............. Train Loss: 1.8178 Val Loss: 1.7450\n",
            "\n",
            "> Next epoch\n",
            "Epoch 3/21\n",
            "Val Epoch 3/21\n",
            "Epoch: 3/20............. Train Loss: 1.6543 Val Loss: 1.6321\n",
            "\n",
            "> Next epoch\n",
            "Epoch 4/21\n",
            "Val Epoch 4/21\n",
            "Epoch: 4/20............. Train Loss: 1.5520 Val Loss: 1.5707\n",
            "\n",
            "> Next epoch\n",
            "Epoch 5/21\n",
            "Val Epoch 5/21\n",
            "Epoch: 5/20............. Train Loss: 1.4809 Val Loss: 1.5309\n",
            "\n",
            "> Next epoch\n",
            "Epoch 6/21\n",
            "Val Epoch 6/21\n",
            "Epoch: 6/20............. Train Loss: 1.4280 Val Loss: 1.5112\n",
            "\n",
            "> Next epoch\n",
            "Epoch 7/21\n",
            "Val Epoch 7/21\n",
            "Epoch: 7/20............. Train Loss: 1.3856 Val Loss: 1.5005\n",
            "\n",
            "> Next epoch\n",
            "Epoch 8/21\n",
            "Val Epoch 8/21\n",
            "Epoch: 8/20............. Train Loss: 1.3482 Val Loss: 1.4983\n",
            "\n",
            "> Next epoch\n",
            "Epoch 9/21\n",
            "Val Epoch 9/21\n",
            "Epoch: 9/20............. Train Loss: 1.3155 Val Loss: 1.5026\n",
            "\n",
            "> Next epoch\n",
            "Epoch 10/21\n",
            "Val Epoch 10/21\n",
            "Epoch: 10/20............. Train Loss: 1.2849 Val Loss: 1.5150\n",
            "\n",
            "> Next epoch\n",
            "Epoch 11/21\n",
            "Val Epoch 11/21\n",
            "Epoch: 11/20............. Train Loss: 1.2569 Val Loss: 1.5295\n",
            "\n",
            "> Next epoch\n",
            "Epoch 12/21\n",
            "Val Epoch 12/21\n",
            "Epoch: 12/20............. Train Loss: 1.2323 Val Loss: 1.5413\n",
            "\n",
            "> Next epoch\n",
            "Epoch 13/21\n",
            "Val Epoch 13/21\n",
            "Epoch: 13/20............. Train Loss: 1.2116 Val Loss: 1.5555\n",
            "\n",
            "> Next epoch\n",
            "Epoch 14/21\n",
            "Val Epoch 14/21\n",
            "Epoch: 14/20............. Train Loss: 1.1939 Val Loss: 1.5604\n",
            "\n",
            "> Next epoch\n",
            "Epoch 15/21\n",
            "Val Epoch 15/21\n",
            "Epoch: 15/20............. Train Loss: 1.1781 Val Loss: 1.5676\n",
            "\n",
            "> Next epoch\n",
            "Epoch 16/21\n",
            "Val Epoch 16/21\n",
            "Epoch: 16/20............. Train Loss: 1.1640 Val Loss: 1.5782\n",
            "\n",
            "> Next epoch\n",
            "Epoch 17/21\n",
            "Val Epoch 17/21\n",
            "Epoch: 17/20............. Train Loss: 1.1512 Val Loss: 1.5865\n",
            "\n",
            "> Next epoch\n",
            "Epoch 18/21\n",
            "Val Epoch 18/21\n",
            "Epoch: 18/20............. Train Loss: 1.1400 Val Loss: 1.5985\n",
            "\n",
            "> Next epoch\n",
            "Epoch 19/21\n",
            "Val Epoch 19/21\n",
            "Epoch: 19/20............. Train Loss: 1.1313 Val Loss: 1.6016\n",
            "\n",
            "> Next epoch\n",
            "Epoch 20/21\n",
            "Val Epoch 20/21\n",
            "Epoch: 20/20............. Train Loss: 1.1201 Val Loss: 1.6130\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q63o5BtfF206"
      },
      "source": [
        "### Predicting the input sequences\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qC4n208pFY1i"
      },
      "source": [
        "def sample_from_probs(probs, top_n=10):\n",
        "    \"\"\"\n",
        "    truncated weighted random choice.\n",
        "    \"\"\"\n",
        "    _, indices = torch.sort(probs)\n",
        "    # set probabilities after top_n to 0\n",
        "    probs[indices.data[:-top_n]] = 0\n",
        "    #print(probs.shape)\n",
        "    sampled_index = torch.multinomial(probs, 1)\n",
        "    return sampled_index\n",
        "\n",
        "def predict_probs(model, hidden, character, vocab):\n",
        "    # One-hot encoding our input to fit into the model\n",
        "    character = np.array([[vocab.char2int[c] for c in character]])\n",
        "    #character = one_hot_encode(character, len(vocab.char2int), character.shape[1], 1)\n",
        "    character = one_hot_encode(character, model.vocab_size)\n",
        "    character = torch.from_numpy(character)\n",
        "    character = character.to(device)\n",
        "    \n",
        "    out, hidden = model(character, hidden)\n",
        "\n",
        "    prob = nn.functional.softmax(out[-1], dim=0).data\n",
        "\n",
        "    return prob, hidden"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4hRRXzAGAJb"
      },
      "source": [
        "Let’s test our model now and see what kind of output we will get."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJ9ac190F-bP"
      },
      "source": [
        "def generate_from_text(model, out_len, vocab, top_n=1, start='hey'):\n",
        "    model.eval() # eval mode\n",
        "    start = start.lower()\n",
        "    # First off, run through the starting characters\n",
        "    chars = [ch for ch in start]\n",
        "    size = out_len - len(chars)\n",
        "    # Generate the initial hidden state\n",
        "    state = model.init_state(device, 1)\n",
        "    \n",
        "    # Warm up the initial state, predicting on the initial string\n",
        "    for ch in chars:\n",
        "        #char, state = predict(model, ch, state, top_n=top_k)\n",
        "        probs, state = predict_probs(model, state, ch, vocab)\n",
        "        next_index = sample_from_probs(probs, top_n)\n",
        "\n",
        "    # Now pass in the previous characters and get a new one\n",
        "    for ii in range(size):\n",
        "        #char, h = predict_char(model, chars, vocab)\n",
        "        probs, state = predict_probs(model, state, chars, vocab)\n",
        "        next_index = sample_from_probs(probs, top_n)\n",
        "        # append to sequence\n",
        "        chars.append(vocab.int2char[next_index.data[0]])\n",
        "\n",
        "    return ''.join(chars)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZHKf91kGDpH",
        "outputId": "9b73b89d-fa68-42f1-cc7e-8f33860fe2fe"
      },
      "source": [
        "text_predicted = generate_from_text(model, 1000, vocab, 3, 'we want ')\n",
        "print(text_predicted)\n",
        "print(len(text_predicted))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "we want our dear lord. that, i am confession. and shall we hear thou wint, and friends are might. go, but thou speak'st true, i am not some offices of this amorous to your father as they than fellow. this is my loving love and leave of this. this is a blows of blood and heart's lord. why should i death? would not be satisfied; and now i stay, i stay; for i will stir a son, they have befull'd by himself to-day. why, that's a finger than the sine all through the duke of norfolk and hereford, as thy bed with me, and therefore his son whiles i have to day. i'll tell upon their paward. to change but the missearen deeds may speak to me. i will not bear my power with him. but she will be revolged to this subject and this soul so shrink and father you. if you will nay, but but to lien so noble. there is no blook, she shall be thus. therefore, make the like, and there begins against the duke of gloucester and their fathers. and they are framed in her. this, when have been togation that homours \n",
            "1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWauFJSmGH4A"
      },
      "source": [
        "\n",
        "def generate_from_char(model, out_len, vocab, top_n=1, start='hey'):\n",
        "    model.eval() # eval mode\n",
        "    start = start.lower()\n",
        "    # First off, run through the starting characters\n",
        "    chars = [ch for ch in start]\n",
        "    size = out_len - len(chars)\n",
        "    # Generate the initial hidden state\n",
        "    state = model.init_state(device, 1)\n",
        "    # Warm up the initial state, predicting on the initial string\n",
        "    for ch in chars:\n",
        "        #char, state = predict(model, ch, state, top_n=top_k)\n",
        "        probs, state = predict_probs(model, state, ch, vocab)\n",
        "        next_index = sample_from_probs(probs, top_n)\n",
        "        \n",
        "    # Include the last char predicted to the predicted output\n",
        "    chars.append(vocab.int2char[next_index.data[0]])   \n",
        "    \n",
        "    # Now pass in the previous characters and get a new one\n",
        "    for ii in range(size-1):\n",
        "        #char, h = predict_char(model, chars, vocab)\n",
        "        probs, state = predict_probs(model, state, chars[-1], vocab)\n",
        "        next_index = sample_from_probs(probs, top_n)\n",
        "        # append to sequence\n",
        "        chars.append(vocab.int2char[next_index.data[0]])\n",
        "\n",
        "    return ''.join(chars)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRUbQw2wGRtL",
        "outputId": "25067e29-800c-480f-d697-2326eb357e28"
      },
      "source": [
        "text_predicted = generate_from_char(model, 1000, vocab, 3, 'we want ')\n",
        "print(text_predicted)\n",
        "print(len(text_predicted))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "we want and darknest that himself, i here better it was a poison, thou hast a swoed with me, and true and takest me affect thy death. why, have you done, for this six indeed. and in the bitter craves of this strangerns not the believe that the staff, the lass i holl, her brother chamber at. i chide thy helb, so meed thee, i am contrar. we are confess the stroke, and to be so forbids the tome. what's the defect should have their souls, to do thee, for i have been beating endsers than the such and they have desperatedent would build him with a purse and the botherood of thy father so let his hard, and will i live to take an humbly in the bold of incuse more men. i'll take your hearts to see you this; and, for this capital cousin of as said i loare of secret beauty, and not take it in his former father, by the heavens that sending of the bourness of his cousen of my country's foul in hollion, to right whiles in this regur throwe to bear a stirrung and an issue, shalt thou tell my strange \n",
            "1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMXJT4qsGZIL"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "The model is now better as compared to the previous model after changing some model parameters though it end up training for long."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbYUF4JnGTHk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}