{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "00_NER-BiLSTM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aE4o0aOazcFg"
      },
      "source": [
        "---\n",
        "Author: **`Crispen Gari`**\n",
        "\n",
        "Year: **`2021`**\n",
        "\n",
        "Date: **`2021-09-15`**\n",
        "\n",
        "Language: **`Python`**\n",
        "\n",
        "Libray: **`Pytorch`**\n",
        "\n",
        "Topic: **`Named Entity Recognition (NER)`**\n",
        "\n",
        "Main: **`Natural Language Processing (NLP)`**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fv1XmS5E0ICM"
      },
      "source": [
        "### Named Entity Recognition (NER) using Bi-Directional LSTM (Bi-LSTM)\n",
        "\n",
        "In this series of notebookswe are going to have a look at an intresting topic in Natural Language Processing (NLP) known as Named Entity Recognition (NER).\n",
        "\n",
        "### Some of the uses\n",
        "Named Entity Recognition can automatically scan entire articles and reveal which are the major people, organizations, and places discussed in them. Knowing the relevant tags for each article help in automatically categorizing the articles in defined hierarchies and enable smooth content discover.\n",
        "\n",
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2qMKDufx0H5d",
        "outputId": "8d8e8a8f-994f-405a-baba-4279607965ed"
      },
      "source": [
        "import time, os, torch, random, json\n",
        "\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from torchtext.legacy import data, datasets\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "torch.__version__\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.9.0+cu102'"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JVcK4JL10V8"
      },
      "source": [
        "### Seeds and Device"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HJ06MWy2CVl",
        "outputId": "a014f7ce-a826-4d53-9575-1cb22e08c7f2"
      },
      "source": [
        "SEED = 42\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0PsQB171aww"
      },
      "source": [
        "### Data.\n",
        "\n",
        "The data that we will be working with was found [here](https://github.com/yoseflaw/nerindo/tree/master/input). What i've done is to download the the three files and load uploaded them on my google drive so that it can be eaisly loaded in google colab. \n",
        "\n",
        "### Explaining the data.\n",
        "The data files are tab deliminated, `tsv` where columns are separated by a `\\t`.\n",
        "\n",
        "To accommodate multi-word entities, the tags follow what is called **`BILOU`** format: **B**eginning, **I**nside, **L**ast, **O**uter, **U**nit. This is indicated by the character preceding the dash symbol. The type of the named entity is shown by the remaining part. For instance, *Universtas Gadjah Mada* is an `ORGANIZATION` and *Arie Sudjito* is a `PERSON`. \n",
        "\n",
        "Here is what a single file may look like:\n",
        "```\n",
        "Pengamat\tO\n",
        "politik\tO\n",
        "dari\tO\n",
        "Universitas\tB-ORGANIZATION\n",
        "Gadjah\tI-ORGANIZATION\n",
        "Mada\tL-ORGANIZATION\n",
        ",\tO\n",
        "Arie\tB-PERSON\n",
        "Sudjito\tL-PERSON\n",
        ",\tO\n",
        "menilai\tO\n",
        ",\tO\n",
        "keinginan\tO\n",
        "Ketua\tO\n",
        "Umum\tO\n",
        "Partai\tB-ORGANIZATION\n",
        "Golkar\tL-ORGANIZATION\n",
        "Aburizal\tB-PERSON\n",
        "Bakrie\tL-PERSON\n",
        "untuk\tO\n",
        "maju\tO\n",
        "kembali\tO\n",
        "sebagai\tO\n",
        "ketua\tO\n",
        "umum\tO\n",
        "merupakan\tO\n",
        "pemaksaan\tO\n",
        "kehendak\tO\n",
        ".\tO\n",
        "\n",
        "Menurut\tO\n",
        "dia\tO\n",
        ",\tO\n",
        "ada\tO\n",
        "kesan\tO\n",
        "bahwa\tO\n",
        "Aburizal\tU-PERSON\n",
        "menggunakan\tO\n",
        "segala\tO\n",
        "cara\tO\n",
        "untuk\tO\n",
        "memuluskan\tO\n",
        "jalannya\tO\n",
        "kembali\tO\n",
        "menduduki\tO\n",
        "Golkar\tU-ORGANIZATION\n",
        "1\tO\n",
        ".\tO\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSkSmHpm46tY",
        "outputId": "3b6348a4-15ea-4ec6-9b8e-4b335d5260dc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwaHCbYt_P9q"
      },
      "source": [
        "### Root Path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8dM-4BT4r-f",
        "outputId": "2d0dab17-b365-48ee-adab-909fb78e68bd"
      },
      "source": [
        "root = '/content/drive/My Drive/NLP Data/ner/data'\n",
        "os.path.exists(root)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2C8dEkkj-7UM"
      },
      "source": [
        "### Fields"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdT24RRa0H2U"
      },
      "source": [
        "WORD = data.Field(lower=True)\n",
        "TAG = data.Field(unk_token=None)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64-oZILQ0Hzu"
      },
      "source": [
        "fields = (\n",
        " (\"word\", WORD),\n",
        " (\"tag\", TAG)\n",
        ")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDzIWQW00Hwk"
      },
      "source": [
        "train_dataset, valid_dataset, test_dataset = datasets.SequenceTaggingDataset.splits(\n",
        "    path=root,\n",
        "    train=\"train.tsv\",\n",
        "    validation=\"val.tsv\",\n",
        "    test= \"test.tsv\",\n",
        "    fields=fields\n",
        ")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhKm61P_0Ht8",
        "outputId": "216bb51f-dcca-4d31-9114-d53278e7ee71"
      },
      "source": [
        "print(vars(train_dataset.examples[67]))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'word': ['\"', 'benar', ',', 'jaksa', 'agung', 'prasetyo', '.', 'tadi', 'pagi', '(', 'keputusannya', ')', ',', '\"', 'kata', 'andi', 'kepada', 'tempo', 'di', 'jakarta', ',', 'kamis', ',', '20', 'november', '2014', '.'], 'tag': ['O', 'O', 'O', 'O', 'O', 'U-PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'U-PERSON', 'O', 'U-ORGANIZATION', 'O', 'U-LOCATION', 'O', 'B-TIME', 'I-TIME', 'I-TIME', 'I-TIME', 'L-TIME', 'O']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlDA2-X2FCDT"
      },
      "source": [
        "### Building vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-WtxIFm0Hqm"
      },
      "source": [
        "WORD.build_vocab(train_dataset, min_freq=3)\n",
        "TAG.build_vocab(train_dataset)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPbfukgdNfsF",
        "outputId": "cd077bd6-abd4-4256-e860-a6dbc9c75c53"
      },
      "source": [
        "TAG.vocab.stoi"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(None,\n",
              "            {'<pad>': 0,\n",
              "             'B-LOCATION': 10,\n",
              "             'B-ORGANIZATION': 7,\n",
              "             'B-PERSON': 5,\n",
              "             'B-QUANTITY': 13,\n",
              "             'B-TIME': 18,\n",
              "             'I-LOCATION': 17,\n",
              "             'I-ORGANIZATION': 9,\n",
              "             'I-PERSON': 15,\n",
              "             'I-QUANTITY': 16,\n",
              "             'I-TIME': 12,\n",
              "             'L-LOCATION': 11,\n",
              "             'L-ORGANIZATION': 8,\n",
              "             'L-PERSON': 6,\n",
              "             'L-QUANTITY': 14,\n",
              "             'L-TIME': 19,\n",
              "             'O': 1,\n",
              "             'U-LOCATION': 4,\n",
              "             'U-ORGANIZATION': 2,\n",
              "             'U-PERSON': 3,\n",
              "             'U-QUANTITY': 21,\n",
              "             'U-TIME': 20})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CFXDgO2FEiE"
      },
      "source": [
        "### Creating iterators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2Xb59QI0Hn7"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "train_iter, valid_iter, test_iter = data.BucketIterator.splits(\n",
        "    (train_dataset,valid_dataset, test_dataset),\n",
        "    device = device,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    sort_key = lambda x: x.word,\n",
        ")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lk1GuVZ8NyqD"
      },
      "source": [
        "### Building the vocabulary\n",
        "\n",
        "Every word that apears less that 2 times in the corpus will be converted to unknown token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ezNs0apMxO1"
      },
      "source": [
        "WORD.build_vocab(train_dataset, min_freq=2)\n",
        "TAG.build_vocab(train_dataset)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAPbsNJmFeki"
      },
      "source": [
        "### Counting examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oogcp7Ny0HlE",
        "outputId": "4c591f41-9973-4d54-cf83-ba2ad113ec0e"
      },
      "source": [
        "print(f\"Train set: {len(train_dataset)} sentences\")\n",
        "print(f\"Val set: {len(valid_dataset)} sentences\")\n",
        "print(f\"Test set: {len(test_dataset)} sentences\")\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set: 3535 sentences\n",
            "Val set: 470 sentences\n",
            "Test set: 468 sentences\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtqOIJeRNvQj"
      },
      "source": [
        "### Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1W6UKqtOOa6"
      },
      "source": [
        "class BiLSTM(nn.Module):\n",
        "  def __init__(self, input_dim,\n",
        "               embedding_dim,\n",
        "               hidden_dim,\n",
        "               output_dim,\n",
        "               word_pad_idx,\n",
        "               dropout=.5,\n",
        "               n_layers=2,\n",
        "               bidirectional = True\n",
        "               ):\n",
        "    super(BiLSTM, self).__init__()\n",
        "    self.embedding = nn.Embedding(input_dim, embedding_dim,)\n",
        "    self.lstm = nn.LSTM(\n",
        "        input_size=embedding_dim,\n",
        "        hidden_size=hidden_dim,\n",
        "        num_layers=n_layers,\n",
        "        bidirectional=bidirectional,\n",
        "        dropout=dropout if n_layers > 1 else 0\n",
        "    )\n",
        "    self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim,\n",
        "                        output_dim)\n",
        "    self.dropout= nn.Dropout(.5)\n",
        "\n",
        "  def forward(self, sentence):\n",
        "    # sentence = [sentence length, batch size]\n",
        "    embedded= self.dropout(self.embedding(sentence)) # [sentence length, batch size, embedding dim]\n",
        "    out, _ = self.lstm(embedded) # out = [sentence length, batch size, hidden dim * 2]\n",
        "    out = self.fc(self.dropout(out)) # [sentence length, batch size, output dim]\n",
        "    return out\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bT56eZLyQEGs",
        "outputId": "fdd6d456-e841-4880-aed2-70c05bf35107"
      },
      "source": [
        "INPUT_DIM=len(WORD.vocab)\n",
        "EMBEDDING_DIM=300\n",
        "HIDDEN_DIM=256\n",
        "OUTPUT_DIM=len(TAG.vocab)\n",
        "N_LAYERS=2\n",
        "DROPOUT=0.1\n",
        "WORD_PAD_IDX = WORD.vocab.stoi[WORD.pad_token]\n",
        "TAG_PAD_IDX = TAG.vocab.stoi[TAG.pad_token]\n",
        "BIDIRECTIONAL = True\n",
        "\n",
        "\n",
        "model = BiLSTM(\n",
        "    INPUT_DIM,\n",
        "    EMBEDDING_DIM,\n",
        "    HIDDEN_DIM,\n",
        "    OUTPUT_DIM,\n",
        "    WORD_PAD_IDX,\n",
        "    DROPOUT,\n",
        "    N_LAYERS,\n",
        "    BIDIRECTIONAL\n",
        ").to(device)\n",
        "model"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BiLSTM(\n",
              "  (embedding): Embedding(5271, 300)\n",
              "  (lstm): LSTM(300, 256, num_layers=2, dropout=0.1, bidirectional=True)\n",
              "  (fc): Linear(in_features=512, out_features=22, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jt-MYuIAXZJR"
      },
      "source": [
        "### Initializing model weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNafr7nIXlDY",
        "outputId": "9db7c802-f966-42f4-e5a7-a157e078329e"
      },
      "source": [
        "def init_weights(m):\n",
        "  for name, param in m.named_parameters():\n",
        "    nn.init.normal_(param.data, mean=0, std=0.1)\n",
        "\n",
        "model.apply(init_weights)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BiLSTM(\n",
              "  (embedding): Embedding(5271, 300)\n",
              "  (lstm): LSTM(300, 256, num_layers=2, dropout=0.1, bidirectional=True)\n",
              "  (fc): Linear(in_features=512, out_features=22, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GXC_SeQX2b1"
      },
      "source": [
        "### initialize embedding for padding as zero"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3AsggGuX6X2",
        "outputId": "e12af364-d502-4335-d1c8-eb2504e1d1ab"
      },
      "source": [
        "model.embedding.weight.data[WORD_PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "model.embedding.weight.data"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.2319,  0.0509, -0.2044,  ...,  0.0600, -0.1761,  0.0536],\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0905,  0.1235, -0.0220,  ...,  0.0997,  0.0513, -0.1714],\n",
              "        ...,\n",
              "        [ 0.1098,  0.0072, -0.1398,  ...,  0.0879, -0.1017,  0.0143],\n",
              "        [ 0.0674, -0.0810,  0.2721,  ...,  0.0041, -0.0147,  0.0828],\n",
              "        [-0.0022, -0.0720, -0.0810,  ..., -0.0563,  0.0377,  0.0070]],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGZIhMCqYNNm"
      },
      "source": [
        "### Counting model parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cmk3nwuk0HfU",
        "outputId": "71cfcbcd-ffb1-4a52-af81-609c45474104"
      },
      "source": [
        "def count_trainable_params(model):\n",
        "  return sum(p.numel() for p in model.parameters()), sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "n_params, trainable_params = count_trainable_params(model)\n",
        "print(f\"Total number of paramaters: {n_params:,}\\nTotal tainable parameters: {trainable_params:,}\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of paramaters: 4,312,330\n",
            "Total tainable parameters: 4,312,330\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7gqiihTYtbt"
      },
      "source": [
        "### Training the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oo1cpbd2GoCa"
      },
      "source": [
        "def accuracy(preds, y):\n",
        "  max_preds = preds.argmax(dim=1, keepdim=True).to(device)  # get the index of the max probability\n",
        "  non_pad_elements = (y != TAG_PAD_IDX).nonzero().to(device)   # prepare masking for paddings\n",
        "  correct = max_preds[non_pad_elements].squeeze(1).eq(y[non_pad_elements])\n",
        "  return correct.sum() / torch.FloatTensor([y[non_pad_elements].shape[0]]).to(device) \n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qX5v0EGJGsd9"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8VDeJCoZhow"
      },
      "source": [
        "### Optimizer and Criterion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpGCWQDdHPFa"
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss().to(device)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4_zzdE-Zmk0"
      },
      "source": [
        "### Train and evalutation functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8psaVdIUHBva"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "  model.train()\n",
        "  for batch in iterator:\n",
        "    # text = [sent len, batch size]\n",
        "    text = batch.word\n",
        "    # tags = [sent len, batch size]\n",
        "    true_tags = batch.tag\n",
        "    optimizer.zero_grad()\n",
        "    pred_tags = model(text)\n",
        "    # to calculate the loss and accuracy, we flatten both prediction and true tags\n",
        "    # flatten pred_tags to [sent len, batch size, output dim]\n",
        "    pred_tags = pred_tags.view(-1, pred_tags.shape[-1])\n",
        "    # flatten true_tags to [sent len * batch size]\n",
        "    true_tags = true_tags.view(-1)\n",
        "    batch_loss = criterion(pred_tags, true_tags)\n",
        "    batch_acc = accuracy(pred_tags, true_tags)\n",
        "    batch_loss.backward()\n",
        "    optimizer.step()\n",
        "    epoch_loss += batch_loss.item()\n",
        "    epoch_acc += batch_acc.item()\n",
        "  return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # similar to epoch() but model is in evaluation mode and no backprop\n",
        "        for batch in iterator:\n",
        "            text = batch.word\n",
        "            true_tags = batch.tag\n",
        "            pred_tags = model(text)\n",
        "            pred_tags = pred_tags.view(-1, pred_tags.shape[-1])\n",
        "            true_tags = true_tags.view(-1)\n",
        "            batch_loss = criterion(pred_tags, true_tags)\n",
        "            batch_acc = accuracy(pred_tags, true_tags)\n",
        "            epoch_loss += batch_loss.item()\n",
        "            epoch_acc += batch_acc.item()\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qECbmsYaZvcT"
      },
      "source": [
        "### Training Loop\n",
        "\n",
        "We are going to create helper functions that will help us to visualize our training.\n",
        "\n",
        "\n",
        "1. Time to string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5J__Uc6ZzcJ"
      },
      "source": [
        "def hms_string(sec_elapsed):\n",
        "  h = int(sec_elapsed / (60 * 60))\n",
        "  m = int((sec_elapsed % (60 * 60)) / 60)\n",
        "  s = sec_elapsed % 60\n",
        "  return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8KLeTjKZ1Ta"
      },
      "source": [
        "2. tabulate training epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbuJE5x_Z7f4"
      },
      "source": [
        "from prettytable import PrettyTable\n",
        "\n",
        "def visualize_training(start, end, train_loss, train_accuracy, val_loss, val_accuracy, title):\n",
        "  data = [\n",
        "       [\"Training\", f'{train_loss:.3f}', f'{train_accuracy:.3f}', f\"{hms_string(end - start)}\" ],\n",
        "       [\"Validation\", f'{val_loss:.3f}', f'{val_accuracy:.3f}', \"\" ],       \n",
        "  ]\n",
        "  table = PrettyTable([\"CATEGORY\", \"LOSS\", \"ACCURACY\", \"ETA\"])\n",
        "  table.align[\"CATEGORY\"] = 'l'\n",
        "  table.align[\"LOSS\"] = 'r'\n",
        "  table.align[\"ACCURACY\"] = 'r'\n",
        "  table.align[\"ETA\"] = 'r'\n",
        "  table.title = title\n",
        "  for row in data:\n",
        "    table.add_row(row)\n",
        "  print(table)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6Ht54F-GsbZ",
        "outputId": "697eab7c-8bbb-43cf-9f86-ee4c648caac8"
      },
      "source": [
        "N_EPOCHS = 10\n",
        "best_valid_loss = float('inf')\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start = time.time()\n",
        "    train_loss, train_acc = train(model, train_iter, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iter, criterion)\n",
        "    title = f\"EPOCH: {epoch+1:02}/{N_EPOCHS:02} {'saving best model...' if valid_loss < best_valid_loss else 'not saving...'}\"\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'best-model.pt')\n",
        "    end = time.time()\n",
        "    visualize_training(start, end, train_loss, train_acc, valid_loss, valid_acc, title)\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------------------+\n",
            "|     EPOCH: 01/10 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.631 |    0.714 | 0:00:03.53 |\n",
            "| Validation | 0.350 |    0.844 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 02/10 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.339 |    0.829 | 0:00:03.47 |\n",
            "| Validation | 0.300 |    0.847 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 03/10 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.304 |    0.832 | 0:00:03.40 |\n",
            "| Validation | 0.248 |    0.851 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 04/10 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.244 |    0.840 | 0:00:03.43 |\n",
            "| Validation | 0.199 |    0.865 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 05/10 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.187 |    0.864 | 0:00:03.44 |\n",
            "| Validation | 0.170 |    0.882 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 06/10 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.149 |    0.890 | 0:00:03.43 |\n",
            "| Validation | 0.154 |    0.894 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 07/10 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.120 |    0.910 | 0:00:03.38 |\n",
            "| Validation | 0.139 |    0.901 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 08/10 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.099 |    0.924 | 0:00:03.38 |\n",
            "| Validation | 0.131 |    0.907 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 09/10 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.082 |    0.935 | 0:00:03.40 |\n",
            "| Validation | 0.131 |    0.908 |            |\n",
            "+------------+-------+----------+------------+\n",
            "+--------------------------------------------+\n",
            "|     EPOCH: 10/10 saving best model...      |\n",
            "+------------+-------+----------+------------+\n",
            "| CATEGORY   |  LOSS | ACCURACY |        ETA |\n",
            "+------------+-------+----------+------------+\n",
            "| Training   | 0.070 |    0.945 | 0:00:03.47 |\n",
            "| Validation | 0.127 |    0.911 |            |\n",
            "+------------+-------+----------+------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gtH2gySa-_P"
      },
      "source": [
        "### Evaluating the best model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYnP8R5HGsYu",
        "outputId": "ff2bdc02-3f43-4a2e-b653-c59edad35e8f"
      },
      "source": [
        "model.load_state_dict(torch.load('best-model.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_iter, criterion)\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.139 | Test Acc: 90.66%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyIIW748bKD_"
      },
      "source": [
        "### Model Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4KPkW0PbX6Z"
      },
      "source": [
        "from spacy.lang.id import Indonesian\n",
        "nlp = Indonesian()"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAmYKbb7bIOs",
        "outputId": "6dc27f11-283b-4f90-8f61-9f1c3278e495"
      },
      "source": [
        "def infer(model, sentence, true_tags=None):\n",
        "    model.eval()\n",
        "    # tokenize sentence\n",
        "    tokens = [token.text.lower() for token in nlp(sentence)]\n",
        "    # transform to indices based on corpus vocab\n",
        "    numericalized_tokens = [WORD.vocab.stoi[t] for t in tokens]\n",
        "    # find unknown words\n",
        "    unk_idx = WORD.vocab.stoi[WORD.unk_token]\n",
        "    unks = [t for t, n in zip(tokens, numericalized_tokens) if n == unk_idx]\n",
        "    # begin prediction\n",
        "    token_tensor = torch.LongTensor(numericalized_tokens)\n",
        "    token_tensor = token_tensor.unsqueeze(-1).to(device)\n",
        "    predictions = model(token_tensor)\n",
        "    # convert results to tags\n",
        "    top_predictions = predictions.argmax(-1)\n",
        "    predicted_tags = [TAG.vocab.itos[t.item()] for t in top_predictions]\n",
        "    # print inferred tags\n",
        "    max_len_token = max([len(token) for token in tokens] + [len(\"word\")])\n",
        "    max_len_tag = max([len(tag) for tag in predicted_tags] + [len(\"pred\")])\n",
        "    print(\n",
        "        f\"{'word'.ljust(max_len_token)}\\t{'unk'.ljust(max_len_token)}\\t{'pred tag'.ljust(max_len_tag)}\" \n",
        "        + (\"\\ttrue tag\" if true_tags else \"\")\n",
        "        )\n",
        "    for i, token in enumerate(tokens):\n",
        "      is_unk = \"✓\" if token in unks else \"\"\n",
        "      print(\n",
        "          f\"{token.ljust(max_len_token)}\\t{is_unk.ljust(max_len_token)}\\t{predicted_tags[i].ljust(max_len_tag)}\" \n",
        "          + (f\"\\t{true_tags[i]}\" if true_tags else \"\")\n",
        "          )\n",
        "    return tokens, predicted_tags, unks\n",
        "\n",
        "sentence = \"Sementara itu, Kepala Pelaksana BPBD Luwu Utara Muslim Muchtar mengatakan, terdapat 15.000 jiwa mengungsi akibat banjir bandang.\"\n",
        "tags = [\"O\", \"O\", \"O\", \"O\", \"O\", \"B-ORGANIZATION\", \"I-ORGANIZATION\", \"L-ORGANIZATION\", \"B-PERSON\", \"L-PERSON\", \"O\", \"O\", \"O\", \"U-QUANTITY\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"]\n",
        "words, infer_tags, unknown_tokens = infer(model, sentence=sentence, true_tags=tags)\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word      \tunk       \tpred tag  \ttrue tag\n",
            "sementara \t          \tO         \tO\n",
            "itu       \t          \tO         \tO\n",
            ",         \t          \tO         \tO\n",
            "kepala    \t          \tO         \tO\n",
            "pelaksana \t          \tO         \tO\n",
            "bpbd      \t✓         \tO         \tB-ORGANIZATION\n",
            "luwu      \t✓         \tO         \tI-ORGANIZATION\n",
            "utara     \t          \tO         \tL-ORGANIZATION\n",
            "muslim    \t          \tO         \tB-PERSON\n",
            "muchtar   \t✓         \tO         \tL-PERSON\n",
            "mengatakan\t          \tO         \tO\n",
            ",         \t          \tO         \tO\n",
            "terdapat  \t          \tO         \tO\n",
            "15.000    \t✓         \tB-QUANTITY\tU-QUANTITY\n",
            "jiwa      \t          \tL-QUANTITY\tO\n",
            "mengungsi \t          \tO         \tO\n",
            "akibat    \t          \tO         \tO\n",
            "banjir    \t          \tO         \tO\n",
            "bandang   \t          \tO         \tO\n",
            ".         \t          \tO         \tO\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-As2_DNEb5Rh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
