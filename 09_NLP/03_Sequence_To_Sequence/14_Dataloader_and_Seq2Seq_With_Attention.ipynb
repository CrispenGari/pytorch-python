{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "14_Dataloader_and_Seq2Seq_With_Attention.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwwyR820eJLI"
      },
      "source": [
        "### Seq2Seq With Attention\n",
        "In this notebook we are going to learn new things. WE are going to learn the following:\n",
        "\n",
        "1. Using the Dataset Class to load our own data\n",
        "2. Creating a model that will be able to translate from French to English.\n",
        "\n",
        "\n",
        "### Basic Imports\n",
        "WE are going to import modules we need as we go. These are the basic moodules that we may need to import first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "syKVBC2Zdv5P",
        "outputId": "071d64ad-53a1-4574-a77a-ec53bb8b86ec"
      },
      "source": [
        "import torch\n",
        "import time, os, math, random\n",
        "\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
        "\n",
        "import numpy as np \n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "torch.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.9.0+cu102'"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lh4tdLa4fOt_"
      },
      "source": [
        "### Seeds "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dfHPuKAfSHG"
      },
      "source": [
        "SEED = 42\n",
        "\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "random.seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deteministic = True"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OGuAJnEgFE_"
      },
      "source": [
        "### Device"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCpOQVo_gG6A",
        "outputId": "09aa8773-c321-4132-b5ee-105a16ea4d88"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWVMKFL9ewdm"
      },
      "source": [
        "### Data\n",
        "We are going to load the data from my google drive. I've uploaded a txt file that we can load and make some damage on it. First we need to mount the drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSJko1MIeCxb",
        "outputId": "6ede1c73-4f11-4b80-f17b-53df8e6807fb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIF6IsVwgX-v"
      },
      "source": [
        "### Path to data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UI7prgGGeCuG",
        "outputId": "cb91a021-e26a-4907-fedd-189f251a16b6"
      },
      "source": [
        "base_path = '/content/drive/MyDrive/NLP Data/seq2seq/en-fr'\n",
        "os.path.exists(base_path)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3HesvGehBPP"
      },
      "source": [
        "### Data processing\n",
        "\n",
        "We are going to create a new file called `en-fr.txt` that will contain english to french sentence pairs. We are going this because our giant file `fra.txt` contain some unnecessary or extra pair on each line. We are going to trim that and come up with a fresh text file in a couple of code cells that follows:\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZLlSUUAj4JF",
        "outputId": "41b52c96-94c1-4dac-96c4-77fa4b36c0a5"
      },
      "source": [
        "lines = open(os.path.join(base_path, 'fra.txt')).read().split('\\n')\n",
        "lines[:2]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Go.\\tVa !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #1158250 (Wittydev)',\n",
              " 'Go.\\tMarche.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8090732 (Micsmithel)']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzT63ao8j3-3",
        "outputId": "25016632-d021-4c6f-a324-05e0fa29ae8c"
      },
      "source": [
        "clean_lines = []\n",
        "for line in lines:\n",
        "  try:\n",
        "    en, fr, _ = line.split('\\t')\n",
        "    clean_lines.append(f'{en}\\t{fr}')\n",
        "  except:\n",
        "    pass\n",
        "len(clean_lines)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "190206"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESV_-ur7ljuX",
        "outputId": "fbbdd4ed-1f88-4b5a-e72b-9456ba5300c4"
      },
      "source": [
        "clean_lines[:2]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Go.\\tVa !', 'Go.\\tMarche.']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2RKF9E4lZIw"
      },
      "source": [
        "Now that we have line that are paired. We are now ready to create our file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvO7XG_Ij35j",
        "outputId": "7ecbfdbe-35fa-4319-873f-be75b7eaacfd"
      },
      "source": [
        "with open(os.path.join(base_path, 'en-fr.txt' ), 'w') as f:\n",
        "  f.write(\"\\n\".join(clean_lines))\n",
        "print(\"Created file en-fr.txt\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Created file en-fr.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxLyMGyZhliY"
      },
      "source": [
        "### Tokenizers for each language.\n",
        "We are going to use the spacy tokenizer to create two tokenizers. One will be for `french` and the other one will be for `english`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7nDkzDVeCrP"
      },
      "source": [
        "from torchtext.data.utils import get_tokenizer"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJs4mfx7iILw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd9084b9-20a6-42b7-c00d-0d9c97e8d97c"
      },
      "source": [
        "!python -m spacy download en\n",
        "!python -m spacy download fr"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0 MB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.62.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.6.4)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "Collecting fr_core_news_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-2.2.5/fr_core_news_sm-2.2.5.tar.gz (14.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.7 MB 5.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from fr_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (4.62.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_sm==2.2.5) (4.6.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.0.4)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('fr_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/fr_core_news_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/fr\n",
            "You can now load the model via spacy.load('fr')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fdp1NOdqeCoV"
      },
      "source": [
        "fr_tokenizer = get_tokenizer('spacy', language=\"fr\")\n",
        "en_tokenizer = get_tokenizer('spacy', language=\"en\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujgU4MxZiPXP",
        "outputId": "c1c5741f-76e1-4240-9894-b20325aaa4b3"
      },
      "source": [
        "en_tokenizer(\"This is a boy?\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This', 'is', 'a', 'boy', '?']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwFZ-dsOmySg"
      },
      "source": [
        "Now we are ready to create our Dataset class. We are going to call this dataset class `EnglishFrenchDataset`. If you want to understand more about how to inherit from the pytorch `Dataset` class and the magic i recommend [this](https://github.com/CrispenGari/pytorch-python/blob/main/01_PyTorch_Basics/08_DataLoaders/DataLoader.ipynb)\n",
        "\n",
        "\n",
        "### `EnglishFrenchDataset`:\n",
        "\n",
        "Args:\n",
        "```\n",
        "  path:        path to a text file\n",
        "  transform:   list transformation\n",
        "  max_vocab:   maximum number for words\n",
        "  shuffle:     shuffling the data\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xq_IVzpwoG9M"
      },
      "source": [
        "PATH = os.path.join(base_path, 'en-fr.txt')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afmrgjnGeClQ"
      },
      "source": [
        "class EnglishFrenchDataset(Dataset):\n",
        "  def __init__(self, path:str, transform=None,\n",
        "               max_vocab = None):\n",
        "    self.max_vocab = max_vocab\n",
        "    self.transform = transform\n",
        "\n",
        "    # Special tokens\n",
        "    self.pad = '<pad>'\n",
        "    self.sos = '<sos>'\n",
        "    self.eos = '<eos>'\n",
        "    self.unk = '<unk>'\n",
        "\n",
        "    # Helper functions that flattens lists\n",
        "    self.flatten = lambda x: [sublist for lst in x for sublist in lst]\n",
        "    \n",
        "    # Loading the dataset\n",
        "    df = pd.read_csv(path, names=[\"en\", \"fr\"], sep=\"\\t\")\n",
        "    self.len = len(df)\n",
        "\n",
        "    # Tokenize src (en -> fr)\n",
        "    self.tokenize_df(df)\n",
        "   \n",
        "    # Replacing rare occuring words in the corpus to be <unk>\n",
        "    # self.replace_rare_tokens(df)\n",
        "\n",
        "    # tokens to index maping (stoi -> string to intenger)\n",
        "    self.stoi(df)\n",
        "    # Remove sequences with mostly <UNK>\n",
        "    if self.max_vocab is not None:\n",
        "      df = self.remove_mostly_unk(df)\n",
        "      \n",
        "    # Convert tokens to indices\n",
        "    self.tokens_to_indices(df)\n",
        "    self.df = df\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.len\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.tokens_pairs[index][0], self.tokens_pairs[index][1]\n",
        "  # Every sequence (input and target) should start with <sos>  and end with <eos>\n",
        "  def add_start_and_end_to_tokens(self, x):\n",
        "    return  [self.sos] + x + [self.eos]\n",
        "\n",
        "  def tokenize_df(self, df):\n",
        "    \"\"\"Turn src/trg into tokens\"\"\"\n",
        "    df['src'] = df.fr.apply(lambda x: x.lower()).apply(fr_tokenizer).apply(self.add_start_and_end_to_tokens)\n",
        "    df['trg'] = df.en.apply(lambda x: x.lower()).apply(en_tokenizer).apply(self.add_start_and_end_to_tokens)\n",
        "\n",
        "  def replace_rare_tokens(self, df):\n",
        "    \"\"\"replacing rare tokens with unk token\"\"\"\n",
        "    common_tokens_src = self.get_most_common_tokens(\n",
        "        df.src.tolist()\n",
        "    )\n",
        "    common_tokens_trg = self.get_most_common_tokens(\n",
        "        df.src.tolist()\n",
        "    )\n",
        "    df.loc[:, 'src'] = df.src.apply(\n",
        "          lambda tokens: [token if token in common_tokens_src \n",
        "                          else self.unk for token in tokens]\n",
        "      )\n",
        "    df.loc[:, 'trg'] = df.trg.apply(\n",
        "          lambda tokens: [token if token in common_tokens_trg \n",
        "                          else self.unk for token in tokens]\n",
        "    )\n",
        "\n",
        "  def get_most_common_tokens(self, tokens):\n",
        "    \"\"\"Return the max_vocab most common tokens.\"\"\"\n",
        "    all_tokens = self.flatten(tokens)\n",
        "    # Substract 4 for <pad>, <sos>, <eos>, and <unk>\n",
        "    common_tokens = set(list(zip(*Counter(all_tokens).most_common(\n",
        "            self.max_vocab - 4)))[0])\n",
        "    return common_tokens\n",
        "\n",
        "  def remove_mostly_unk(self, df, threshold=0.99):\n",
        "      \"\"\"Remove sequences with mostly <UNK>.\"\"\"\n",
        "      calculate_ratio = (\n",
        "          lambda tokens: sum(1 for token in tokens if token != '<UNK>')\n",
        "          / len(tokens) > threshold\n",
        "      )\n",
        "      df = df[df.src.apply(calculate_ratio)]\n",
        "      df = df[df.trg.apply(calculate_ratio)]\n",
        "      return df\n",
        "  def stoi(self, df):\n",
        "    unique_tokens_src = set(self.flatten(df.src))\n",
        "    unique_tokens_trg = set(self.flatten(df.trg))\n",
        "    for token in reversed([self.pad, self.sos, self.eos, self.unk]):\n",
        "      if token in unique_tokens_src:\n",
        "        unique_tokens_src.remove(token)\n",
        "      if token in unique_tokens_trg:\n",
        "        unique_tokens_trg.remove(token)\n",
        "            \n",
        "    unique_tokens_src = sorted(list(unique_tokens_src))\n",
        "    unique_tokens_trg = sorted(list(unique_tokens_trg))\n",
        "\n",
        "    # Add <pad>, <sos>, <eos>, and <unk> tokens\n",
        "    for token in reversed([self.pad, self.sos, self.eos, self.unk]):\n",
        "      unique_tokens_src = [token] + unique_tokens_src\n",
        "      unique_tokens_trg = [token] + unique_tokens_trg\n",
        "            \n",
        "      self.stoi_src = {token: idx for idx, token\n",
        "                                 in enumerate(unique_tokens_src)}\n",
        "      self.itos_src = {idx: token for token, idx\n",
        "                                 in self.stoi_src.items()}\n",
        "      self.stoi_trg = {token: idx for idx, token\n",
        "                                  in enumerate(unique_tokens_trg)}\n",
        "      self.itos_trg = {idx: token for token, idx\n",
        "                                  in self.stoi_trg.items()}\n",
        "    \n",
        "  def tokens_to_indices(self, df):\n",
        "    \"\"\"Convert tokens to indices.\"\"\"\n",
        "    df['src_tokens'] = df.src.apply(\n",
        "        lambda tokens: [self.stoi_src[token] for token in tokens])\n",
        "   \n",
        "    df['trg_tokens'] = df.trg.apply(\n",
        "        lambda tokens: [self.stoi_trg[token] for token in tokens])\n",
        "    self.tokens_pairs = list(zip(df.src_tokens, df.trg_tokens))\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDEtDa5p2Rjk"
      },
      "source": [
        "### Custom transformation\n",
        "We are going to create a custom transformation class that will convert list to tensors. We are going to call the `ToTensor` class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jqLQbWgeCim"
      },
      "source": [
        "class ToTensor:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def __call__(self, sample):\n",
        "    _, __ = sample\n",
        "    return torch.tensor(_), torch.tensor(__)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wz08Dc6n3JN3"
      },
      "source": [
        "### Creating the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGwa0jt4eCff",
        "outputId": "5952d2aa-fbc0-453c-d190-7933cca65893"
      },
      "source": [
        "%%time\n",
        "dataset = EnglishFrenchDataset(\n",
        "    os.path.join(base_path, 'en-fr.txt'),\n",
        "    transform = ToTensor()\n",
        ")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 23.7 s, sys: 456 ms, total: 24.2 s\n",
            "Wall time: 24.2 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VT4kH0HFb4Wt",
        "outputId": "62c42435-bb2c-4e7b-f07a-9f9a58004a25"
      },
      "source": [
        "len(dataset)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "190206"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "iM4dAXBYdl9N",
        "outputId": "b1782065-d3de-4059-f410-2a357eee6daa"
      },
      "source": [
        "dataset.df.head(10).iloc[:, 2:]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>src</th>\n",
              "      <th>trg</th>\n",
              "      <th>src_tokens</th>\n",
              "      <th>trg_tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[&lt;sos&gt;, va, !, &lt;eos&gt;]</td>\n",
              "      <td>[&lt;sos&gt;, go, ., &lt;eos&gt;]</td>\n",
              "      <td>[1, 24169, 4, 2]</td>\n",
              "      <td>[1, 5989, 20, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[&lt;sos&gt;, marche, ., &lt;eos&gt;]</td>\n",
              "      <td>[&lt;sos&gt;, go, ., &lt;eos&gt;]</td>\n",
              "      <td>[1, 14284, 18, 2]</td>\n",
              "      <td>[1, 5989, 20, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[&lt;sos&gt;, bouge, !, &lt;eos&gt;]</td>\n",
              "      <td>[&lt;sos&gt;, go, ., &lt;eos&gt;]</td>\n",
              "      <td>[1, 3175, 4, 2]</td>\n",
              "      <td>[1, 5989, 20, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[&lt;sos&gt;, salut, !, &lt;eos&gt;]</td>\n",
              "      <td>[&lt;sos&gt;, hi, ., &lt;eos&gt;]</td>\n",
              "      <td>[1, 21175, 4, 2]</td>\n",
              "      <td>[1, 6532, 20, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[&lt;sos&gt;, salut, ., &lt;eos&gt;]</td>\n",
              "      <td>[&lt;sos&gt;, hi, ., &lt;eos&gt;]</td>\n",
              "      <td>[1, 21175, 18, 2]</td>\n",
              "      <td>[1, 6532, 20, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[&lt;sos&gt;, cours,  , !, &lt;eos&gt;]</td>\n",
              "      <td>[&lt;sos&gt;, run, !, &lt;eos&gt;]</td>\n",
              "      <td>[1, 5923, 25871, 4, 2]</td>\n",
              "      <td>[1, 11437, 4, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>[&lt;sos&gt;, courez,  , !, &lt;eos&gt;]</td>\n",
              "      <td>[&lt;sos&gt;, run, !, &lt;eos&gt;]</td>\n",
              "      <td>[1, 5904, 25871, 4, 2]</td>\n",
              "      <td>[1, 11437, 4, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>[&lt;sos&gt;, prenez, vos, jambes, à, vos, cous, !, ...</td>\n",
              "      <td>[&lt;sos&gt;, run, !, &lt;eos&gt;]</td>\n",
              "      <td>[1, 17825, 24735, 13126, 24988, 24735, 5946, 4...</td>\n",
              "      <td>[1, 11437, 4, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>[&lt;sos&gt;, file, !, &lt;eos&gt;]</td>\n",
              "      <td>[&lt;sos&gt;, run, !, &lt;eos&gt;]</td>\n",
              "      <td>[1, 10134, 4, 2]</td>\n",
              "      <td>[1, 11437, 4, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>[&lt;sos&gt;, filez, !, &lt;eos&gt;]</td>\n",
              "      <td>[&lt;sos&gt;, run, !, &lt;eos&gt;]</td>\n",
              "      <td>[1, 10138, 4, 2]</td>\n",
              "      <td>[1, 11437, 4, 2]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 src  ...        trg_tokens\n",
              "0                              [<sos>, va, !, <eos>]  ...  [1, 5989, 20, 2]\n",
              "1                          [<sos>, marche, ., <eos>]  ...  [1, 5989, 20, 2]\n",
              "2                           [<sos>, bouge, !, <eos>]  ...  [1, 5989, 20, 2]\n",
              "3                           [<sos>, salut, !, <eos>]  ...  [1, 6532, 20, 2]\n",
              "4                           [<sos>, salut, ., <eos>]  ...  [1, 6532, 20, 2]\n",
              "5                        [<sos>, cours,  , !, <eos>]  ...  [1, 11437, 4, 2]\n",
              "6                       [<sos>, courez,  , !, <eos>]  ...  [1, 11437, 4, 2]\n",
              "7  [<sos>, prenez, vos, jambes, à, vos, cous, !, ...  ...  [1, 11437, 4, 2]\n",
              "8                            [<sos>, file, !, <eos>]  ...  [1, 11437, 4, 2]\n",
              "9                           [<sos>, filez, !, <eos>]  ...  [1, 11437, 4, 2]\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AUFBygMNP6d"
      },
      "source": [
        "### Splitting Sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LhYKrzdNSzm",
        "outputId": "07283b08-f20a-4c91-835f-27e7f6eaf040"
      },
      "source": [
        "train_size = int(len(dataset) * .9)\n",
        "valid_size = int(len(dataset) * .08)\n",
        "test_size = len(dataset) - train_size - valid_size\n",
        "train_dataset,valid_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size, test_size])\n",
        "\n",
        "train_size, test_size, valid_size"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(171185, 3805, 15216)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8j1PCzl_VuD"
      },
      "source": [
        "### Creating generators using DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHZHCrIseCXY"
      },
      "source": [
        "def collate(batch):\n",
        "  src = [torch.LongTensor(item[0]) for item in batch]\n",
        "  trg = [torch.LongTensor(item[1])  for item in batch]\n",
        "\n",
        "  padded_src = pad_sequence(src,\n",
        "                               padding_value=dataset.stoi_src[dataset.pad],\n",
        "                               batch_first=True)\n",
        "  padded_trg = pad_sequence(trg,\n",
        "                            padding_value=dataset.stoi_src[dataset.pad],\n",
        "                            batch_first=True)\n",
        "\n",
        "  # Sort by length for CUDA optimizations\n",
        "  lengths = torch.LongTensor([len(x) for x in src])\n",
        "  lengths, permutation = lengths.sort(dim=0, descending=True)\n",
        "\n",
        "  return padded_src[permutation].to(device), padded_trg[permutation].to(device), lengths.to(device)\n",
        " \n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNx8MnKreCUm"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=collate)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, collate_fn=collate)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=collate)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJBDqs3TNbbK"
      },
      "source": [
        "### Seq2Seq with Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuEQX_BROARq"
      },
      "source": [
        "### Encoder model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4Gin_fSeCRX"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, vocab_size,\n",
        "               embedding_dim,\n",
        "               hidden_size, batch_size):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_size = batch_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
        "    self.gru = nn.GRU(\n",
        "        self.embedding_dim,\n",
        "        self.hidden_size,\n",
        "        batch_first=True,\n",
        "    )\n",
        "    \n",
        "  def forward(self, src, lengths):\n",
        "    self.batch_size = src.size(0)\n",
        "    # Turn input indices into distributed embeddings\n",
        "    x = self.embedding(src)\n",
        "    # Remove padding for more efficient RNN application\n",
        "    x = pack_padded_sequence(x, lengths.to('cpu'), batch_first=True)\n",
        "\n",
        "    # Apply RNN to get hidden state at all timesteps (output)\n",
        "    # and hidden state of last output (self.hidden)\n",
        "    output, self.hidden = self.gru(x, self.init_hidden())\n",
        "    # Pad the sequences like they were before\n",
        "    output, _ = pad_packed_sequence(output)\n",
        "    return output, self.hidden\n",
        "  def init_hidden(self):\n",
        "    # Randomly initialize the weights of the RNN\n",
        "    return torch.randn(1, self.batch_size, self.hidden_size).to(device)\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCUXxT_nODfq"
      },
      "source": [
        "### Decoder Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b80mR7pOOFaH"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(\n",
        "      self, \n",
        "      vocab_size,\n",
        "      embedding_dim, \n",
        "      decoder_hidden_size,\n",
        "      encoder_hidden_size, \n",
        "      batch_size):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_size = batch_size\n",
        "    self.encoder_hidden_size = encoder_hidden_size\n",
        "    self.decoder_hidden_size = decoder_hidden_size\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
        "    self.gru = nn.GRU(\n",
        "        self.embedding_dim + self.encoder_hidden_size, \n",
        "        self.decoder_hidden_size,\n",
        "        batch_first=True,\n",
        "    )\n",
        "    self.fc = nn.Linear(self.encoder_hidden_size, self.vocab_size)\n",
        "    \n",
        "    # Attention weights\n",
        "    self.W1 = nn.Linear(self.encoder_hidden_size, self.decoder_hidden_size)\n",
        "    self.W2 = nn.Linear(self.encoder_hidden_size, self.decoder_hidden_size)\n",
        "    self.V = nn.Linear(self.encoder_hidden_size, 1)\n",
        "\n",
        "  def forward(self, trg, hidden, encoder_output):\n",
        "    self.batch_size = trg.size(0)\n",
        "    # Switch the dimensions of sequence_length and batch_size\n",
        "    encoder_output = encoder_output.permute(1, 0, 2)\n",
        "\n",
        "    # Add an extra axis for a time dimension\n",
        "    hidden_with_time_axis = hidden.permute(1, 0, 2)\n",
        "    \n",
        "    # Attention score (Bahdanaus)\n",
        "    score = torch.tanh(self.W1(encoder_output) + self.W2(hidden_with_time_axis))\n",
        "\n",
        "    # Attention weights\n",
        "    attention_weights = torch.softmax(self.V(score), dim=1)\n",
        "    \n",
        "    # Find the context vectors\n",
        "    context_vector = attention_weights * encoder_output\n",
        "    context_vector = torch.sum(context_vector, dim=1)\n",
        "    \n",
        "    # Turn target indices into distributed embeddings\n",
        "    x = self.embedding(trg)\n",
        "    \n",
        "    # Add the context representation to the target embeddings\n",
        "    x = torch.cat((context_vector.unsqueeze(1), x), -1)\n",
        "    \n",
        "    # Apply the RNN\n",
        "    output, state = self.gru(x, self.init_hidden())\n",
        "    \n",
        "    # Reshape the hidden states (output)\n",
        "    output = output.view(-1, output.size(2))\n",
        "    \n",
        "    # Apply a linear layer\n",
        "    x = self.fc(output)\n",
        "    \n",
        "    return x, state, attention_weights\n",
        "  \n",
        "  def init_hidden(self):\n",
        "    # Randomly initialize the weights of the RNN\n",
        "    return torch.randn(1, self.batch_size, self.decoder_hidden_size).to(device)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AouTsm1OKT5"
      },
      "source": [
        "### Encoder Decoder Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEwcKyjNONCk"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=dataset.stoi_src[dataset.pad])\n",
        "def loss_function(real, pred):\n",
        "  # Use mask to only consider non-zero inputs in the loss\n",
        "  mask = real.ge(1).float().to(device)\n",
        "  loss_ = criterion(pred, real) * mask \n",
        "  return torch.mean(loss_)\n",
        "\n",
        "\n",
        "class EncoderDecoder(nn.Module):\n",
        "\n",
        "  def __init__(self, inputs_vocab_size,\n",
        "                targets_vocab_size, hidden_size,\n",
        "                embedding_dim, batch_size, \n",
        "                targets_start_idx, targets_stop_idx):\n",
        "      super(EncoderDecoder, self).__init__()\n",
        "      self.batch_size = batch_size\n",
        "      self.targets_start_idx = targets_start_idx\n",
        "      self.targets_stop_idx = targets_stop_idx\n",
        "      \n",
        "      self.encoder = Encoder(inputs_vocab_size, embedding_dim,\n",
        "                              hidden_size, batch_size).to(device)\n",
        "      \n",
        "      self.decoder = Decoder(targets_vocab_size, embedding_dim,\n",
        "                              hidden_size, hidden_size, batch_size).to(device)\n",
        "      \n",
        "  def predict(self, inputs, lengths):\n",
        "    self.batch_size = inputs.size(0)\n",
        "    \n",
        "    encoder_output, encoder_hidden = self.encoder(\n",
        "        inputs.to(device),\n",
        "        lengths,\n",
        "    )\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    # Initialize the input of the decoder to be <SOS>\n",
        "    decoder_input = torch.LongTensor(\n",
        "        [[self.targets_start_idx]] * self.batch_size,\n",
        "    )\n",
        "    \n",
        "    # Output predictions instead of loss\n",
        "    output = []\n",
        "    for _ in range(20):\n",
        "      predictions, decoder_hidden, _ = self.decoder(\n",
        "          decoder_input.to(device), \n",
        "          decoder_hidden.to(device),\n",
        "          encoder_output.to(device),\n",
        "      )\n",
        "      prediction = torch.multinomial(F.softmax(predictions, dim=1), 1)\n",
        "      decoder_input = prediction\n",
        "      \n",
        "      prediction = prediction[0].item()\n",
        "      output.append(prediction)\n",
        "\n",
        "      if prediction == self.targets_stop_idx:\n",
        "          return output\n",
        "    return output\n",
        "\n",
        "  def forward(self, inputs, targets, lengths):\n",
        "    self.batch_size = inputs.size(0)\n",
        "    \n",
        "    encoder_output, encoder_hidden = self.encoder(\n",
        "        inputs.to(device),\n",
        "        lengths,\n",
        "    )\n",
        "    decoder_hidden = encoder_hidden\n",
        "    \n",
        "    # Initialize the input of the decoder to be <SOS>\n",
        "    decoder_input = torch.LongTensor(\n",
        "        [[self.targets_start_idx]] * self.batch_size,\n",
        "    )\n",
        "            \n",
        "    # Use teacher forcing to train the model. Instead of feeding the model's\n",
        "    # own predictions to itself, feed the target token at every timestep.\n",
        "    # This leads to faster convergence\n",
        "    loss = 0\n",
        "    for timestep in range(1, targets.size(1)):\n",
        "      predictions, decoder_hidden, _ = self.decoder(\n",
        "          decoder_input.to(device), \n",
        "          decoder_hidden.to(device),\n",
        "          encoder_output.to(device),\n",
        "      )\n",
        "      decoder_input = targets[:, timestep].unsqueeze(1)\n",
        "      \n",
        "      loss += loss_function(targets[:, timestep], predictions)\n",
        "    return loss / targets.size(1)"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_XvgRkVW9o2"
      },
      "source": [
        "### Model Instance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVjDe_fmORrX",
        "outputId": "3605daae-e142-4cfa-a196-f0ccf1caa815"
      },
      "source": [
        "model = EncoderDecoder(\n",
        "    inputs_vocab_size=len(dataset.stoi_src),\n",
        "    targets_vocab_size=len(dataset.stoi_trg),\n",
        "    hidden_size=256,\n",
        "    embedding_dim=100, \n",
        "    batch_size=BATCH_SIZE, \n",
        "    targets_start_idx=dataset.stoi_trg[dataset.sos],\n",
        "    targets_stop_idx=dataset.stoi_trg[dataset.eos],\n",
        ").to(device)\n",
        "\n",
        "model"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EncoderDecoder(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(25876, 100)\n",
              "    (gru): GRU(100, 256, batch_first=True)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(15139, 100)\n",
              "    (gru): GRU(356, 256, batch_first=True)\n",
              "    (fc): Linear(in_features=256, out_features=15139, bias=True)\n",
              "    (W1): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (W2): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (V): Linear(in_features=256, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24uhwyPyRBUT"
      },
      "source": [
        "### Counting model parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ix1DQaWCREbv",
        "outputId": "5e4a9e6c-817e-4590-d4b2-ee45105f1fb1"
      },
      "source": [
        "def count_trainable_params(model):\n",
        "  return sum(p.numel() for p in model.parameters()), sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "n_params, trainable_params = count_trainable_params(model)\n",
        "print(f\"Total number of paramaters: {n_params:,}\\nTotal tainable parameters: {trainable_params:,}\")"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of paramaters: 8,870,560\n",
            "Total tainable parameters: 8,870,560\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFkql7fVRMGL"
      },
      "source": [
        "### Initializing model weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhLsIwgERSUj"
      },
      "source": [
        "def init_weights(m: nn.Module):\n",
        "  for name, param in m.named_parameters():\n",
        "    if 'weight' in name:\n",
        "      nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "    else:\n",
        "      nn.init.constant_(param.data, 0)"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8x49EgCRUG6",
        "outputId": "9f6c0b57-3537-457e-a3d9-191ef55bcc12"
      },
      "source": [
        "model.apply(init_weights)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EncoderDecoder(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(25876, 100)\n",
              "    (gru): GRU(100, 256, batch_first=True)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(15139, 100)\n",
              "    (gru): GRU(356, 256, batch_first=True)\n",
              "    (fc): Linear(in_features=256, out_features=15139, bias=True)\n",
              "    (W1): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (W2): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (V): Linear(in_features=256, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o69cESmSRaGj"
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters())"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APiwrOyNRjF_"
      },
      "source": [
        "### Training and evaluate functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9H0QEQmRmTF"
      },
      "source": [
        "def train(model, iterator, optimizer):\n",
        "  model.train()\n",
        "  total_loss = total = 0\n",
        "  for index, (inputs, targets, lengths) in enumerate(iterator):\n",
        "    optimizer.zero_grad()\n",
        "    loss = model(inputs, targets, lengths)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    total_loss += loss.item()\n",
        "    total += targets.size(1)\n",
        "  train_loss = total_loss / total\n",
        "  return train_loss\n",
        "\n",
        "def evaluate(model, iterator):\n",
        "  model.eval()\n",
        "  total_loss = total = 0\n",
        "  with torch.no_grad():\n",
        "      for index, (inputs, targets, lengths) in enumerate(iterator):\n",
        "        loss = model(inputs, targets, lengths)\n",
        "        total_loss += loss.item()\n",
        "        total += targets.size(1)\n",
        "  test_loss = total_loss / total\n",
        "  return test_loss\n"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjgPJynMTWFc"
      },
      "source": [
        "### Helper functions\n",
        "\n",
        "1. time to string"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6srn5n6TV3p"
      },
      "source": [
        "def hms_string(sec_elapsed):\n",
        "    h = int(sec_elapsed / (60 * 60))\n",
        "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
        "    s = sec_elapsed % 60\n",
        "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlODnp4ZTZcM"
      },
      "source": [
        "2. Tabulate training epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iin5TgXBQr-u"
      },
      "source": [
        "from prettytable import PrettyTable\n",
        "def tabulate_training(column_names, data, title):\n",
        "  table = PrettyTable(column_names)\n",
        "  table.title= title\n",
        "  table.align[column_names[0]] = 'l'\n",
        "  table.align[column_names[1]] = 'r'\n",
        "  table.align[column_names[2]] = 'r'\n",
        "  table.align[column_names[3]] = 'r'\n",
        "  for row in data:\n",
        "    table.add_row(row)\n",
        "  print(table)"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zd66oJgCTSb7"
      },
      "source": [
        "### Train Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paJVJvBKTpXh",
        "outputId": "0599ed0d-e67c-410b-a933-10c97d4e43ae"
      },
      "source": [
        "N_EPOCHS = 10\n",
        "best_valid_loss = float('inf')\n",
        "column_names = [\"SET\", \"LOSS\", \"PPL\", \"ETA\"]\n",
        "print(\"TRAINING STARTS....\")\n",
        "for epoch in range(N_EPOCHS):\n",
        "  start = time.time()\n",
        "  train_loss = train(model, train_loader, optimizer )\n",
        "  valid_loss = evaluate(model, valid_loader)\n",
        "  end = time.time()\n",
        "  title = f\"EPOCH: {epoch+1:02}/{N_EPOCHS:02} | {'saving model...' if valid_loss < best_valid_loss else 'not saving...'}\" \n",
        "  if valid_loss < best_valid_loss:\n",
        "      best_valid_loss = valid_loss\n",
        "      torch.save(model.state_dict(), 'best-model.pt')\n",
        "  rows_data =[\n",
        "        [\"train\", f\"{train_loss:.3f}\", f\"{math.exp(train_loss):7.3f}\", hms_string(end - start) ],\n",
        "        [\"val\", f\"{valid_loss:.3f}\", f\"{math.exp(train_loss):7.3f}\", '' ]\n",
        "  ]\n",
        "  tabulate_training(column_names, rows_data, title)\n",
        "\n",
        "print(\"TRAINING ENDS....\")\n"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAINING STARTS....\n",
            "+--------------------------------------+\n",
            "|    EPOCH: 01/10 | saving model...    |\n",
            "+-------+-------+---------+------------+\n",
            "| SET   |  LOSS |     PPL |        ETA |\n",
            "+-------+-------+---------+------------+\n",
            "| train | 0.094 |   1.099 | 0:03:48.32 |\n",
            "| val   | 0.086 |   1.099 |            |\n",
            "+-------+-------+---------+------------+\n",
            "+--------------------------------------+\n",
            "|    EPOCH: 02/10 | saving model...    |\n",
            "+-------+-------+---------+------------+\n",
            "| SET   |  LOSS |     PPL |        ETA |\n",
            "+-------+-------+---------+------------+\n",
            "| train | 0.071 |   1.074 | 0:03:48.55 |\n",
            "| val   | 0.065 |   1.074 |            |\n",
            "+-------+-------+---------+------------+\n",
            "+--------------------------------------+\n",
            "|    EPOCH: 03/10 | saving model...    |\n",
            "+-------+-------+---------+------------+\n",
            "| SET   |  LOSS |     PPL |        ETA |\n",
            "+-------+-------+---------+------------+\n",
            "| train | 0.055 |   1.056 | 0:03:48.87 |\n",
            "| val   | 0.051 |   1.056 |            |\n",
            "+-------+-------+---------+------------+\n",
            "+--------------------------------------+\n",
            "|    EPOCH: 04/10 | saving model...    |\n",
            "+-------+-------+---------+------------+\n",
            "| SET   |  LOSS |     PPL |        ETA |\n",
            "+-------+-------+---------+------------+\n",
            "| train | 0.043 |   1.044 | 0:03:49.71 |\n",
            "| val   | 0.041 |   1.044 |            |\n",
            "+-------+-------+---------+------------+\n",
            "+--------------------------------------+\n",
            "|    EPOCH: 05/10 | saving model...    |\n",
            "+-------+-------+---------+------------+\n",
            "| SET   |  LOSS |     PPL |        ETA |\n",
            "+-------+-------+---------+------------+\n",
            "| train | 0.034 |   1.034 | 0:03:49.45 |\n",
            "| val   | 0.034 |   1.034 |            |\n",
            "+-------+-------+---------+------------+\n",
            "+--------------------------------------+\n",
            "|    EPOCH: 06/10 | saving model...    |\n",
            "+-------+-------+---------+------------+\n",
            "| SET   |  LOSS |     PPL |        ETA |\n",
            "+-------+-------+---------+------------+\n",
            "| train | 0.028 |   1.028 | 0:03:49.50 |\n",
            "| val   | 0.031 |   1.028 |            |\n",
            "+-------+-------+---------+------------+\n",
            "+--------------------------------------+\n",
            "|    EPOCH: 07/10 | saving model...    |\n",
            "+-------+-------+---------+------------+\n",
            "| SET   |  LOSS |     PPL |        ETA |\n",
            "+-------+-------+---------+------------+\n",
            "| train | 0.024 |   1.024 | 0:03:50.06 |\n",
            "| val   | 0.029 |   1.024 |            |\n",
            "+-------+-------+---------+------------+\n",
            "+--------------------------------------+\n",
            "|    EPOCH: 08/10 | saving model...    |\n",
            "+-------+-------+---------+------------+\n",
            "| SET   |  LOSS |     PPL |        ETA |\n",
            "+-------+-------+---------+------------+\n",
            "| train | 0.021 |   1.021 | 0:03:50.50 |\n",
            "| val   | 0.027 |   1.021 |            |\n",
            "+-------+-------+---------+------------+\n",
            "+--------------------------------------+\n",
            "|    EPOCH: 09/10 | saving model...    |\n",
            "+-------+-------+---------+------------+\n",
            "| SET   |  LOSS |     PPL |        ETA |\n",
            "+-------+-------+---------+------------+\n",
            "| train | 0.019 |   1.019 | 0:03:50.52 |\n",
            "| val   | 0.026 |   1.019 |            |\n",
            "+-------+-------+---------+------------+\n",
            "+--------------------------------------+\n",
            "|    EPOCH: 10/10 | saving model...    |\n",
            "+-------+-------+---------+------------+\n",
            "| SET   |  LOSS |     PPL |        ETA |\n",
            "+-------+-------+---------+------------+\n",
            "| train | 0.017 |   1.017 | 0:03:50.89 |\n",
            "| val   | 0.025 |   1.017 |            |\n",
            "+-------+-------+---------+------------+\n",
            "TRAINING ENDS....\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUnab-iTVO28"
      },
      "source": [
        "### Evaluating the best model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KoDf1LyVSGb",
        "outputId": "7a6244e9-bce5-446d-972d-77dbc46bb96a"
      },
      "source": [
        "model.load_state_dict(torch.load('best-model.pt'))\n",
        "\n",
        "test_loss = evaluate(model, test_loader)\n",
        "title = \"Model Evaluation Summary\"\n",
        "data_rows = [[\"Test\", f'{test_loss:.3f}', f'{math.exp(test_loss):7.3f}', \"\"]]\n",
        "\n",
        "tabulate_training([\"SET\", \"LOSS\", \"PPL\", \"ETA\"], data_rows, title)\n"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------------------+\n",
            "|   Model Evaluation Summary   |\n",
            "+------+-------+---------+-----+\n",
            "| SET  |  LOSS |     PPL | ETA |\n",
            "+------+-------+---------+-----+\n",
            "| Test | 0.025 |   1.025 |     |\n",
            "+------+-------+---------+-----+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKaVDDf1OUiO"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYVQ-0qtOVup",
        "outputId": "ea87d26c-71d3-4e75-9a12-b761bb1ad31e"
      },
      "source": [
        "model.eval()\n",
        "total_loss = total = 0\n",
        "with torch.no_grad():\n",
        "  for inputs, targets, lengths in test_loader:\n",
        "    print('input| >', ' '.join([\n",
        "        dataset.itos_src[idx]\n",
        "        for idx in inputs.cpu()[0].numpy()[1:-1]\n",
        "    ]))\n",
        "    print('target| >', ' '.join([\n",
        "        dataset.itos_trg[idx]\n",
        "        for idx in targets.cpu()[0].numpy()[1:-1]\n",
        "    ]))\n",
        "    # Forwards pass\n",
        "    outputs = model.predict(inputs, lengths)\n",
        "    prediction = ' '.join([\n",
        "        dataset.itos_trg[idx]\n",
        "        for idx in outputs[:-1]\n",
        "    ])\n",
        "    print()\n",
        "    print(\"predicted| =\", prediction)\n",
        "    print(\"*\" * 100)\n",
        "    print()"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input| > après avoir réfléchi sur ma vie jusqu' à présent , j' ai décidé que j' avais besoin de changer mes objectifs .\n",
            "target| > after reflecting on my life up to now , i decided that i needed to change my goals .\n",
            "\n",
            "predicted| = after your soul in my english to him , i was decided i change .\n",
            "****************************************************************************************************\n",
            "\n",
            "input| > pourquoi ne restes - tu pas un moment après que tout le monde soit parti de manière à ce que nous puissions discuter   ?\n",
            "target| > why do n't you hang around a while after everyone else leaves so we can talk ?\n",
            "\n",
            "predicted| = why do n't you stay a while after everybody else that happen , what we can play himself about\n",
            "****************************************************************************************************\n",
            "\n",
            "input| > je me fiche que tu y ailles ou pas . j' y vais de toutes façons .\n",
            "target| > i do n't care if you go or not . i 'm going anyway .\n",
            "\n",
            "predicted| = i do n't care if you 're there or not by myself .\n",
            "****************************************************************************************************\n",
            "\n",
            "input| > je n' arrive pas à croire que tu ne sois pas disposé à au moins envisager la possibilité qu' il y ait une autre explication .\n",
            "target| > i ca n't believe that you are n't at least willing to consider the possibility that there 's another explanation .\n",
            "\n",
            "predicted| = i ca n't believe you 're not interested in least accompanies another explanation .\n",
            "****************************************************************************************************\n",
            "\n",
            "input| > c' est une théorie intéressante , mais je ne vois pas comment elle peut être testée .\n",
            "target| > it 's an intriguing theory , but i do n't see how it can be tested .\n",
            "\n",
            "predicted| = it 's an different theory , but i ca n't see how she can fit .\n",
            "****************************************************************************************************\n",
            "\n",
            "input| > je ne peux pas vous dire qui est sur la liste . mon patron m' a dit de ne pas le dire .\n",
            "target| > i ca n't tell you who 's on the list . my boss told me not to .\n",
            "\n",
            "predicted| = i ca n't tell you who 's about the list . i would do that i could n't tell\n",
            "****************************************************************************************************\n",
            "\n",
            "input| > qu' y a -t -il comme bons aliments à manger avec des pommes de terre ?\n",
            "target| > what are some good foods to eat with potatoes ? <eos> <pad> <pad> <pad> <pad>\n",
            "\n",
            "predicted| = what 's like good food to eat with apples ?\n",
            "****************************************************************************************************\n",
            "\n",
            "input| > il est très aisé d' avoir l' air naturel dans votre langue natale et très aisé d' avoir l' air de manquer de naturel dans une autre langue .\n",
            "target| > it 's very easy to sound natural in your own native language , and very easy to sound unnatural in your non - native language .\n",
            "\n",
            "predicted| = it 's very easy to get provided in your native speaker now seem now seem very difficult - esteem\n",
            "****************************************************************************************************\n",
            "\n",
            "input| > sa voiture est peut - être plus agréable à conduire , mais elle lui coûte aussi plus cher en frais d' entretien .\n",
            "target| > her car might be more pleasant to drive , but it also costs her more for maintenance .\n",
            "\n",
            "predicted| = her car may be pretty good than driving , but when the most of the further bullets .\n",
            "****************************************************************************************************\n",
            "\n",
            "input| > je n' ai jamais imaginé un seul instant que je serais toujours en train de faire ce genre de chose à mon âge .\n",
            "target| > i never for a moment imagined that i would still be doing this kind of thing at my age .\n",
            "\n",
            "predicted| = i never dreamed about this moment imagined would have to go through this store .\n",
            "****************************************************************************************************\n",
            "\n",
            "input| > « pourquoi tom reste - t - il à la maison ? » « il reste à la maison parce qu' il est malade . »\n",
            "target| > why is tom staying home ? \" he 's staying home because he 's sick . \"\n",
            "\n",
            "predicted| = why are tom tom home ? lately ? he is sick behind it . it satisfied . \"\n",
            "****************************************************************************************************\n",
            "\n",
            "input| > la couleur verte est souvent associée à l' argent et au monde de la finance , parce que les billets de banque sont verts dans beaucoup de pays .\n",
            "target| > the color green is often associated with money and the world of finance , because banknotes in many countries are green .\n",
            "\n",
            "predicted| = the yellow eyes is often completed from the rest of the town does the bank in st .\n",
            "****************************************************************************************************\n",
            "\n",
            "input| > il est revenu non-pas parce qu' il avait le mal du pays , mais parce qu' il n' avait plus assez d' argent .\n",
            "target| > he came back not because he was homesick , but because he was running short of money .\n",
            "\n",
            "predicted| = it came back because he 'd homesick .\n",
            "****************************************************************************************************\n",
            "\n",
            "input| > je sais que tu ne fais qu' essayer de donner un coup de main et je t' en suis reconnaissant .\n",
            "target| > i know you 're just trying to help , and i appreciate that . <eos> <pad> <pad> <pad>\n",
            "\n",
            "predicted| = i know you 're just trying to lose my supper .\n",
            "****************************************************************************************************\n",
            "\n",
            "input| > à vrai dire , j' ai conduit la voiture de mon père sans sa permission .\n",
            "target| > to tell the truth , i drove my father 's car without his permission . <eos> <pad>\n",
            "\n",
            "predicted| = to think , i went busier my father without her permission .\n",
            "****************************************************************************************************\n",
            "\n",
            "input| > je n' arrive toujours pas à croire que tu n' as pas été aussi impressionné que je l' ai été .\n",
            "target| > i ca n't believe you were n't as impressed as i was . <eos> <pad>\n",
            "\n",
            "predicted| = i just ca n't believe you were n't it impressed before i was in too .\n",
            "****************************************************************************************************\n",
            "\n",
            "input| > j' ai raté le dernier train , donc j' ai dû marcher tout le chemin jusqu' à chez moi .\n",
            "target| > i missed the last train , so i had to walk all the way home .\n",
            "\n",
            "predicted| = i failed the last train , so i should walk walking by myself .\n",
            "****************************************************************************************************\n",
            "\n",
            "input| > un homme avec une montre sait quelle heure il est , un homme avec deux montres n' en est jamais sûr .\n",
            "target| > a man with a watch knows what time it is , a man with two watches is never sure .\n",
            "\n",
            "predicted| = a man with a notebook , which one is for three zees is my other book and let 's\n",
            "****************************************************************************************************\n",
            "\n",
            "input| > vous ne pouvez tout simplement pas ne pas payer quelqu' un pour le travail pour lequel vous l' avez engagé .\n",
            "target| > you ca n't just not pay someone for work you hired them to do . <eos> <pad> <pad> <pad>\n",
            "\n",
            "predicted| = you ca n't just not to someone someone who 's pain .\n",
            "****************************************************************************************************\n",
            "\n",
            "input| > est -ce que ça te dérange si j' ouvre la fenêtre et laisse aller la fumée dehors   ?\n",
            "target| > do you mind if i open the window and let the smoke out ?\n",
            "\n",
            "predicted| = will this i feel my window and check on smoke ?\n",
            "****************************************************************************************************\n",
            "\n",
            "input| > pour ta propre sécurité , ne monte jamais à bord d' une voiture avec un conducteur en état d' ébriété .\n",
            "target| > for your own safety , never ride in a car with a drunk driver . <eos> <pad>\n",
            "\n",
            "predicted| = for your way ok , i 've never working from an auditorium about a driver 's getting a walk\n",
            "****************************************************************************************************\n",
            "\n",
            "input| > je ne peux pas te dire tout ce qu' on m' a dit parce qu' on m' a dit de ne pas le faire .\n",
            "target| > i ca n't tell you everything i 've been told because i 've been told not to . <eos> <pad> <pad> <pad>\n",
            "\n",
            "predicted| = i ca n't tell you everything i 'm told to you said do n't absolutely what to do !\n",
            "****************************************************************************************************\n",
            "\n",
            "input| > tout le monde respectait la limite de vitesse , alors je savais qu' il y avait probablement un radar devant moi .\n",
            "target| > everybody was obeying the speed limit , so i knew there was likely a speed trap ahead . <eos> <pad>\n",
            "\n",
            "predicted| = everybody makes the fingerprints up speaking , so i knew you had a treatment mad .\n",
            "****************************************************************************************************\n",
            "\n",
            "input| > je ne peux pas assister à la fête , mais merci en tout cas de m' avoir invité .\n",
            "target| > i can not go to the party , but thank you for inviting me all the same .\n",
            "\n",
            "predicted| = i ca n't attend the party , but left people it a pain .\n",
            "****************************************************************************************************\n",
            "\n",
            "input| > pourriez - vous , s' il vous plait , me dire quel est votre numéro de téléphone mobile   ?\n",
            "target| > could you please tell me what your cell phone number is ? <eos> <pad> <pad>\n",
            "\n",
            "predicted| = could you please tell me this is 3202 - phone number ?\n",
            "****************************************************************************************************\n",
            "\n",
            "input| > bien sûr que vous pouvez vous fier à moi . vous ai - je déjà refilé un mauvais tuyau ?\n",
            "target| > of course you can trust me . have i ever given you a bum steer before ? <eos> <pad>\n",
            "\n",
            "predicted| = although i can trust me . you find you a bum steer before ?\n",
            "****************************************************************************************************\n",
            "\n",
            "input| > mon père pensait que celui qui ne pouvait gagner sa vie au japon était un paresseux .\n",
            "target| > my father believed that anyone who could not make a living in japan was lazy . <eos>\n",
            "\n",
            "predicted| = my father thought that tom could not able to win his life in japan .\n",
            "****************************************************************************************************\n",
            "\n",
            "input| > la bibliothèque près de chez moi est maintenant seulement ouverte trois jours par semaine en raison de coupes budgétaires .\n",
            "target| > the library near my house is now open only three days a week due to budget cutbacks . <eos>\n",
            "\n",
            "predicted| = the library is near my house and just go over ten days at morning .\n",
            "****************************************************************************************************\n",
            "\n",
            "input| > je pensais qu' une poignée de gens serait venue faire du ski - nautique avec nous , mais absolument personne ne se manifesta .\n",
            "target| > i thought a bunch of people would go water skiing with us , but absolutely no one else showed up .\n",
            "\n",
            "predicted| = i thought that was a lot of being died at what have said that .\n",
            "****************************************************************************************************\n",
            "\n",
            "input| > je préfère ne pas perdre mon temps à essayer d' apprendre une langue que je n' ai pas besoin de connaître .\n",
            "target| > i 'd rather not waste my time trying to learn a language that i do n't need to know .\n",
            "\n",
            "predicted| = i prefer not to lose my time to try to learn a much more disappointed about to learn credit\n",
            "****************************************************************************************************\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSxFe2gXZqrd"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "In this notebook we have learned ho to create and load our dataset from local files and prepare it step by step to make it ready for training.\n",
        "\n",
        "### Ref \n",
        "\n",
        "* [this github repo](https://github.com/scoutbee/pytorch-nlp-notebooks)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_u23j7rNke8U"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}