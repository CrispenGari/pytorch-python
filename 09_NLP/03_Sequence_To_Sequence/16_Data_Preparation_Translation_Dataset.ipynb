{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "16_Data_Preparation_Translation_Dataset.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VN9-7O4JwPBi",
        "outputId": "de2797b3-2bb9-4f92-ee27-5a4d853c9688"
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LoIV8c5XwTFe",
        "outputId": "45b52896-2a44-45b2-99ce-b612a538eb6a"
      },
      "source": [
        "base_path = '/content/drive/My Drive/NLP Data/seq2seq/manythings'\n",
        "os.path.exists(base_path)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxeVyyqoxoRm"
      },
      "source": [
        "We are going to create 3 files:\n",
        "\n",
        "> train, test and valid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "la02b-IDxoDp"
      },
      "source": [
        "de_path = 'German - English/deu.txt'"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWzEaW2XwTCQ"
      },
      "source": [
        "lines = open(os.path.join(base_path, de_path)).read().split('\\n')\n",
        "\n",
        "eng, deu = [], []\n",
        "\n",
        "for line in lines:\n",
        "  try:\n",
        "    en, de, _  = line.split('\\t')\n",
        "    eng.append(en)\n",
        "    deu.append(de)\n",
        "  except:\n",
        "    continue\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0Hi-dKywS--",
        "outputId": "fc89891e-6689-46c0-c65f-4fd677364f8a"
      },
      "source": [
        "eng[:3], deu[:3]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['Go.', 'Hi.', 'Hi.'], ['Geh.', 'Hallo!', 'Grüß Gott!'])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRgZDv6Ey--B"
      },
      "source": [
        "Split eng and deu into three sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-bhoMbZwS72",
        "outputId": "d910a25d-3849-4116-d4b7-47708f1161a1"
      },
      "source": [
        "train_eng, test_eng , train_deu, test_deu = train_test_split(\n",
        "    eng, deu, random_state=42,\n",
        "    test_size = .005\n",
        ")\n",
        "train_eng, val_eng , train_deu, val_deu = train_test_split(\n",
        "    train_eng, train_deu, random_state=42,\n",
        "    test_size = .005\n",
        ")\n",
        "\n",
        "len(train_eng), len(test_eng), len(val_eng)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(240166, 1213, 1207)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTnTCAunz2hE"
      },
      "source": [
        "Now we are going to create 6 files from these list of data which are:\n",
        "\n",
        "```\n",
        "1. train.de\n",
        "2. train.en\n",
        "3. test.de\n",
        "4. test.en\n",
        "5. val.de\n",
        "6. val.en\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsVG997FwS5J",
        "outputId": "4030fdfe-8a36-489a-f138-ac5cdbd6840c"
      },
      "source": [
        "writter = open(os.path.join(base_path, \"German - English/train.en\"), \"w\")\n",
        "for line in train_eng:\n",
        "  writter.write(line+'\\n')\n",
        "writter.close()\n",
        "print(\"done\")\n",
        "\n",
        "writter = open(os.path.join(base_path, \"German - English/test.en\"), \"w\")\n",
        "for line in test_eng:\n",
        "  writter.write(line+'\\n')\n",
        "writter.close()\n",
        "print(\"done\")\n",
        "\n",
        "writter = open(os.path.join(base_path, \"German - English/valid.en\"), \"w\")\n",
        "for line in val_eng:\n",
        "  writter.write(line+'\\n')\n",
        "writter.close()\n",
        "print(\"done\")\n",
        "\n",
        "\n",
        "writter = open(os.path.join(base_path, \"German - English/train.de\"), \"w\")\n",
        "for line in train_deu:\n",
        "  writter.write(line+'\\n')\n",
        "writter.close()\n",
        "print(\"done\")\n",
        "\n",
        "writter = open(os.path.join(base_path, \"German - English/test.de\"), \"w\")\n",
        "for line in test_deu:\n",
        "  writter.write(line+'\\n')\n",
        "writter.close()\n",
        "print(\"done\")\n",
        "\n",
        "writter = open(os.path.join(base_path, \"German - English/valid.de\"), \"w\")\n",
        "for line in val_deu:\n",
        "  writter.write(line+'\\n')\n",
        "writter.close()\n",
        "print(\"done\")\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done\n",
            "done\n",
            "done\n",
            "done\n",
            "done\n",
            "done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ir9Fjql42opR"
      },
      "source": [
        "Loading the dataset..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdwI-G99wS2A"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn  import functional as F\n",
        "import spacy, math, random\n",
        "import numpy as np\n",
        "from torchtext.legacy import datasets, data"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYNpN5-x2vei",
        "outputId": "5cc9a2b4-3778-4716-e99c-fe9bcbeceebc"
      },
      "source": [
        "import spacy\n",
        "import spacy.cli\n",
        "spacy.cli.download('de_core_news_sm')\n",
        "\n",
        "import de_core_news_sm, en_core_web_sm\n",
        "\n",
        "spacy_de = spacy.load('de_core_news_sm')\n",
        "spacy_en = spacy.load('en_core_web_sm')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('de_core_news_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPn97NMM2zZa"
      },
      "source": [
        "def tokenize_de(sent):\n",
        "  return [tok.text for tok in spacy_de.tokenizer(sent)]\n",
        "\n",
        "def tokenize_en(sent):\n",
        "  return [tok.text for tok in spacy_en.tokenizer(sent)]"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53-tYPUo260H"
      },
      "source": [
        "\n",
        "SRC = data.Field(\n",
        "    tokenize = tokenize_de,\n",
        "    lower= True,\n",
        "    init_token = \"<sos>\",\n",
        "     eos_token = \"<eos>\",\n",
        "     include_lengths =True\n",
        ")\n",
        "TRG = data.Field(\n",
        "    tokenize = tokenize_en,\n",
        "    lower= True,\n",
        "    init_token = \"<sos>\",\n",
        "     eos_token = \"<eos>\"\n",
        ")"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFU2Q-z54hXA"
      },
      "source": [
        "We are making use of the following class to create a dataset from our custom file. The code bellow was taken from [this](https://pytorch.org/text/_modules/torchtext/datasets/translation.html) site."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Djt22fU14iQn"
      },
      "source": [
        "class TranslationDataset(data.Dataset):\n",
        "    \"\"\"Defines a dataset for machine translation.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def sort_key(ex):\n",
        "        return data.interleave_keys(len(ex.src), len(ex.trg))\n",
        "\n",
        "    def __init__(self, path, exts, fields, **kwargs):\n",
        "        \"\"\"Create a TranslationDataset given paths and fields.\n",
        "\n",
        "        Arguments:\n",
        "            path: Common prefix of paths to the data files for both languages.\n",
        "            exts: A tuple containing the extension to path for each language.\n",
        "            fields: A tuple containing the fields that will be used for data\n",
        "                in each language.\n",
        "            Remaining keyword arguments: Passed to the constructor of\n",
        "                data.Dataset.\n",
        "        \"\"\"\n",
        "        if not isinstance(fields[0], (tuple, list)):\n",
        "            fields = [('src', fields[0]), ('trg', fields[1])]\n",
        "\n",
        "        src_path, trg_path = tuple(os.path.expanduser(path + x) for x in exts)\n",
        "\n",
        "        examples = []\n",
        "        with io.open(src_path, mode='r', encoding='utf-8') as src_file, \\\n",
        "                io.open(trg_path, mode='r', encoding='utf-8') as trg_file:\n",
        "            for src_line, trg_line in zip(src_file, trg_file):\n",
        "                src_line, trg_line = src_line.strip(), trg_line.strip()\n",
        "                if src_line != '' and trg_line != '':\n",
        "                    examples.append(data.Example.fromlist(\n",
        "                        [src_line, trg_line], fields))\n",
        "\n",
        "        super(TranslationDataset, self).__init__(examples, fields, **kwargs)\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def splits(cls, exts, fields, path=None, root='.data',\n",
        "               train='train', validation='val', test='test', **kwargs):\n",
        "        \"\"\"Create dataset objects for splits of a TranslationDataset.\n",
        "\n",
        "        Arguments:\n",
        "            exts: A tuple containing the extension to path for each language.\n",
        "            fields: A tuple containing the fields that will be used for data\n",
        "                in each language.\n",
        "            path (str): Common prefix of the splits' file paths, or None to use\n",
        "                the result of cls.download(root).\n",
        "            root: Root dataset storage directory. Default is '.data'.\n",
        "            train: The prefix of the train data. Default: 'train'.\n",
        "            validation: The prefix of the validation data. Default: 'val'.\n",
        "            test: The prefix of the test data. Default: 'test'.\n",
        "            Remaining keyword arguments: Passed to the splits method of\n",
        "                Dataset.\n",
        "        \"\"\"\n",
        "        if path is None:\n",
        "            path = cls.download(root)\n",
        "\n",
        "        train_data = None if train is None else cls(\n",
        "            os.path.join(path, train), exts, fields, **kwargs)\n",
        "        val_data = None if validation is None else cls(\n",
        "            os.path.join(path, validation), exts, fields, **kwargs)\n",
        "        test_data = None if test is None else cls(\n",
        "            os.path.join(path, test), exts, fields, **kwargs)\n",
        "        return tuple(d for d in (train_data, val_data, test_data)\n",
        "                     if d is not None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdVDSTrc3VDJ",
        "outputId": "06571009-9bad-4c0d-a397-1b0f52ea168f"
      },
      "source": [
        "root_path = os.path.join(base_path, \"German - English\")\n",
        "os.listdir(root_path)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['deu.txt',\n",
              " 'train.en',\n",
              " 'test.en',\n",
              " 'valid.en',\n",
              " 'train.de',\n",
              " 'test.de',\n",
              " 'valid.de',\n",
              " 'multi30k']"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHqpqssQ2-lo"
      },
      "source": [
        "train_data, valid_data, test_data = datasets.TranslationDataset.splits(\n",
        "    exts=('.de', '.en'),\n",
        "    path=root_path,\n",
        "    train='train', validation='valid', test='test',\n",
        "    fields = (SRC, TRG)\n",
        ")"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3ghK7JM4rdc",
        "outputId": "5d68c7d8-5f67-4ff0-eb2a-7be564739701"
      },
      "source": [
        "print(vars(train_data[0]))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'src': ['du', 'solltest', 'es', 'tun', '.'], 'trg': ['you', 'should', 'do', 'it', '.']}\n"
          ]
        }
      ]
    }
  ]
}