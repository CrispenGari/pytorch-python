{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "08_French_To_English.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxyfmlIwhSzj"
      },
      "source": [
        "### French to English Translation `Multi30k` dataset.\n",
        "\n",
        "There are a lot of datasets that comes with the [torchtext.datasets](https://pytorch.org/text/stable/datasets.html#multi30k) for machine translation. We are going to do French to english translation and latter on we will do english to french translation. In this notebook we are going to use `RNNs` but as we move we will be using Conv Nets and Transformers + Attention.\n",
        "\n",
        "### Datasets that are available for machine translation in torchtext.\n",
        "\n",
        "* Multi30k \n",
        "* IWSLT2016\n",
        "* IWSLT2017\n",
        "\n",
        "### Data preparation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNQXuoRdgyvq"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn  import functional as F\n",
        "import spacy, math, random\n",
        "import numpy as np\n",
        "from torchtext.legacy import datasets, data\n",
        "import time\n",
        "from prettytable import PrettyTable\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lN4YBldGioU0"
      },
      "source": [
        "**Note:** This notebook is based on this [notebook](https://github.com/CrispenGari/PyTorch-Python/blob/main/09_TorchText/03_Sequence_To_Sequence/04_Packed_Padded_Sequences%2C_Masking%2C_Inference_and_BLEU.ipynb)\n",
        "\n",
        "### SEEDS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AA2z1LTBinXh"
      },
      "source": [
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "random.seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deteministic = True"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MMJh87jjlf6"
      },
      "source": [
        "### Loading tokenizer models.\n",
        "We are going to load two models, the english and the french model for tokenization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJwBdAkTjZN6",
        "outputId": "405fbb3c-5a50-466e-c579-b00dfa769143"
      },
      "source": [
        "import spacy\n",
        "spacy.cli.download('fr_core_news_sm')\n",
        "\n",
        "spacy_fr = spacy.load('fr_core_news_sm')\n",
        "spacy_en = spacy.load('en_core_web_sm')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('fr_core_news_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ek4LhYGykE5q"
      },
      "source": [
        "### Tokenization functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtQv5Up-jZKM"
      },
      "source": [
        "def tokenize_fr(sent):\n",
        "  return [tok.text for tok in spacy_fr.tokenizer(sent)]\n",
        "\n",
        "def tokenize_en(sent):\n",
        "  return [tok.text for tok in spacy_en.tokenizer(sent)]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZY13x82kZLb"
      },
      "source": [
        "#### Creating the Fields.\n",
        "Since we are using packed padded sequences we need to tell pytorch how long the actual (non-padded) sequences are. Luckly pytorch does this for us we just have to pass the argument `include_lengths=True`.\n",
        "\n",
        "**Note:** Again we only pass `include_lengths` arg to True for the `SRC` (source)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-0XVt1LjZFa"
      },
      "source": [
        "SRC = data.Field(\n",
        "    tokenize= tokenize_fr,\n",
        "    lower= True,\n",
        "    init_token = \"<sos>\",\n",
        "    eos_token = \"<eos>\",\n",
        "    include_lengths =True\n",
        ")\n",
        "\n",
        "TRG = data.Field(\n",
        "    tokenize = tokenize_en,\n",
        "    lower= True,\n",
        "    init_token = \"<sos>\",\n",
        "     eos_token = \"<eos>\"\n",
        ")\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdtDPjYzliYK"
      },
      "source": [
        "### Loading the `Multi30k` dataset.\n",
        "This time around we are using french instead of german so our extention will change for the `src` from `.de` to `.fr` in the `exts` tupple.\n",
        "\n",
        "**Note:** Again note that we are going to pass the `SRC` extention first in the `exts` tupple."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "id": "_C6BZX-KjZCS",
        "outputId": "ad7d15ce-16b7-4b3d-cb61-489ef95d5f4c"
      },
      "source": [
        "train_data, valid_data, test_data = datasets.Multi30k.splits(\n",
        "    root=\".data\",\n",
        "    exts=('.fr', '.en'),\n",
        "    fields = (SRC, TRG),\n",
        ")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading training.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training.tar.gz: 100%|██████████| 1.21M/1.21M [00:00<00:00, 1.60MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading validation.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "validation.tar.gz: 100%|██████████| 46.3k/46.3k [00:00<00:00, 242kB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading mmt_task1_test2016.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "mmt_task1_test2016.tar.gz: 100%|██████████| 66.2k/66.2k [00:00<00:00, 236kB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-a6ff33b5ec23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\".data\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.fr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mfields\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSRC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/legacy/datasets/translation.py\u001b[0m in \u001b[0;36msplits\u001b[0;34m(cls, exts, fields, root, train, validation, test, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         return super(Multi30k, cls).splits(\n\u001b[0;32m--> 117\u001b[0;31m             exts, fields, path, root, train, validation, test, **kwargs)\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/legacy/datasets/translation.py\u001b[0m in \u001b[0;36msplits\u001b[0;34m(cls, exts, fields, path, root, train, validation, test, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         train_data = None if train is None else cls(\n\u001b[0;32m---> 69\u001b[0;31m             os.path.join(path, train), exts, fields, **kwargs)\n\u001b[0m\u001b[1;32m     70\u001b[0m         val_data = None if validation is None else cls(\n\u001b[1;32m     71\u001b[0m             os.path.join(path, validation), exts, fields, **kwargs)\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/legacy/datasets/translation.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, exts, fields, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mexamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msrc_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m                 \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrg_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0msrc_line\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_line\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '.data/multi30k/train.fr'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uC39o2Y-yTEy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QghA4BkySuy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chtqMFpNySgb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmVRrFhmi-XG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ED7PytmmjWMq"
      },
      "source": [
        ""
      ]
    }
  ]
}