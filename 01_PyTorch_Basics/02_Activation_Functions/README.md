## Activation functions.

Activation functions apply a non-linear transformation and decide weather a neron must activated or no. The reason why we use activation function is because without activation functions our model will be basically satcked as a linear regression model.
The introduction on non-liniearity in our model helps it to learn better and perfom complex task.

### Moset Popular Activation functions

1. **Step function**
    > Not used most
 
2. **Sigmoid**
    > For **binary Classification** if the output is either **true** of **false**
3. **TanH**
    > For **Hiden layers**
4. **ReLU**
     > For **Hiden layers**
5. **Leaky ReLU**
    > Improved version of `RelU` for vanishng gradient problems
6. **Softmax**
    > For **Multiclass Classification Problems**