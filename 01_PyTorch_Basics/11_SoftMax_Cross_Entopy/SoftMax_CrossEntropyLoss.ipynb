{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "promotional-money",
   "metadata": {},
   "source": [
    "### SoftMax_CrossEntropyLoss - Theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "funded-medium",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "critical-colon",
   "metadata": {},
   "source": [
    "```\n",
    "                |-> 2.0 |                 |-> .65 |                   \n",
    "    Linear----> |-> 1.0 |---> Softmax --->|-> .25 |----> CrossEntopy(y, y_pred)\n",
    "                |-> 0.1 |                 |-> .10 |\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "christian-division",
   "metadata": {},
   "source": [
    "### The softmax\n",
    "* The sum of the probabilities of each outcome should be **`1`**\n",
    "* Softmax applies the exponential function to each element and normalizes by dividing by the sum of all these exponents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "worthy-porcelain",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "upper-interest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6590011 , 0.24243295, 0.09856589], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([2., 1., 0.1], dtype='float32')\n",
    "output = softmax(x)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endless-russian",
   "metadata": {},
   "source": [
    "> The sum of these probabilities adds up to one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "center-allocation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99999994"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(output, axis=0) # which is close to one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attached-buffalo",
   "metadata": {},
   "source": [
    "### Softmax using torch buildin function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "prostate-mixer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.0000, 1.0000, 0.1000])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.from_numpy(x)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "north-hungary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6590, 0.2424, 0.0986])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = torch.softmax(a, dim=0)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "through-heating",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(output) # this is exactly 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessible-daniel",
   "metadata": {},
   "source": [
    "### The Cross entropy\n",
    "`Cross-entropy` loss, or ``log`` loss, measures the performance of a classification model whose output is a probability value between 0 and 1. \n",
    "- loss increases as the predicted probability diverges from the actual label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "judicial-portal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(actual, predicted):\n",
    "    EPS = 1e-15\n",
    "    predicted = np.clip(predicted, EPS, 1 - EPS)\n",
    "    loss = -np.sum(actual * np.log(predicted))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-august",
   "metadata": {},
   "source": [
    "> Labels must be ``one hot encoded``\n",
    "\n",
    "```\n",
    "       class_0: [1 0 0]\n",
    "       class_1: [0 1 0]\n",
    "       class_2: [0 0 1]\n",
    "```\n",
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "alert-stewart",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([1, 0, 0])\n",
    "y_pred_good = np.array([0.8, 0.1, 0.1])\n",
    "y_pred_bad = np.array([0.4, 0.5, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "optimum-savings",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2231435513142097 0.916290731874155\n"
     ]
    }
   ],
   "source": [
    "loss_1 = cross_entropy(y, y_pred_good)\n",
    "loss_2 = cross_entropy(y, y_pred_bad)\n",
    "\n",
    "print(loss_1, loss_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sophisticated-macintosh",
   "metadata": {},
   "source": [
    "#### Pytorch CrossEntropyLoss\n",
    "* CrossEntropyLoss in PyTorch (applies Softmax)\n",
    "* ``nn.LogSoftmax + nn.NLLLoss``\n",
    "* **NLLLoss** -> **N**egative **L**og **L**ikelihood **L**oss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "oriental-spirituality",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "amber-sacrifice",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target is of size nSamples = 1\n",
    "# each element has class label: 0, 1, or 2\n",
    "# Y (=target) contains class labels, not one-hot\n",
    "\n",
    "Y = torch.tensor([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "judicial-maple",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4170) tensor(1.8406)\n"
     ]
    }
   ],
   "source": [
    "Y_pred_good = torch.tensor([[2.0, 1.0, 0.1]])\n",
    "Y_pred_bad = torch.tensor([[0.5, 2.0, 0.3]])\n",
    "l1 = loss(Y_pred_good, Y)\n",
    "l2 = loss(Y_pred_bad, Y)\n",
    "\n",
    "print(l1, l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepted-witch",
   "metadata": {},
   "source": [
    "### ``Binary classification`` vrs ``Multiclass problem``\n",
    "\n",
    "* **Binary classification** - apply ``sigmoid`` as an activation function to the output layer of the ``NN``\n",
    "    * ``loss = nn.BCELoss()``\n",
    "\n",
    "* **Multiclass problem** - do not apply softmax or  at the last layer because `CrossEntropyLoss()` applies softmax.\n",
    "    * ``loss = nn.CrossEntropyLoss()``"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
