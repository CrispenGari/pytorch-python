{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "middle-dance",
   "metadata": {},
   "source": [
    "### Logistic Regression - scikit-learn-datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indirect-milton",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acute-jewelry",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alike-passion",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "parallel-evans",
   "metadata": {},
   "outputs": [],
   "source": [
    "breast_cancer = datasets.load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "french-shade",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(breast_cancer.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "independent-elite",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['malignant', 'benign'], dtype='<U9')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = breast_cancer.data\n",
    "y = breast_cancer.target\n",
    "types = breast_cancer.target_names\n",
    "types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescription-chart",
   "metadata": {},
   "source": [
    "> **Splitting the data** - into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "tender-tradition",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test= train_test_split(X, y, random_state=42, test_size=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authentic-uncertainty",
   "metadata": {},
   "source": [
    "> **Scaling the data** - using the `StandardScaler`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "agreed-bruce",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "directed-earthquake",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "otherwise-monster",
   "metadata": {},
   "source": [
    "> **Converting data** - Convering the data into ``pytorch`` tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "obvious-surfing",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensors =torch.from_numpy(X_train.astype('float32'))\n",
    "X_test_tensors =torch.from_numpy(X_test.astype('float32'))\n",
    "\n",
    "y_train_tensors =torch.from_numpy(y_train.astype('float32'))\n",
    "y_test_tensors =torch.from_numpy(y_test.astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "through-honey",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([455, 30]),\n",
       " torch.Size([455]),\n",
       " torch.Size([114, 30]),\n",
       " torch.Size([114]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tensors.shape, y_train_tensors.shape, X_test_tensors.shape, y_test_tensors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bright-priority",
   "metadata": {},
   "source": [
    "### Changing the y_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "changing-parameter",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_tensors = y_test_tensors.view(y_test_tensors.shape[0], 1)\n",
    "y_train_tensors = y_train_tensors.view(y_train_tensors.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "searching-winter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.],\n",
       "         [0.]]),\n",
       " tensor([[1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_tensors[:2], y_train_tensors[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-latin",
   "metadata": {},
   "source": [
    "### Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "explicit-blank",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = X_test_tensors.shape[-1]\n",
    "input_shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "willing-craps",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(\n",
       "  (linn): Linear(in_features=30, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linn = nn.Linear(input_shape, 1)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return torch.sigmoid(self.linn(X)) ## good for binary classifiction problems\n",
    "    \n",
    "    \n",
    "model = LogisticRegression()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amino-enough",
   "metadata": {},
   "source": [
    "### Loss and Optimizer\n",
    "> For the `loss` we are going to use `Binary Cross Entropy Loss Function (BCELoss)` and `SGD()` optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "attended-preparation",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extreme-crossing",
   "metadata": {},
   "source": [
    "### Trainning the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "corporate-support",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1/200, loss: 0.1607\n",
      "Epochs: 21/200, loss: 0.1569\n",
      "Epochs: 41/200, loss: 0.1534\n",
      "Epochs: 61/200, loss: 0.1501\n",
      "Epochs: 81/200, loss: 0.1471\n",
      "Epochs: 101/200, loss: 0.1444\n",
      "Epochs: 121/200, loss: 0.1418\n",
      "Epochs: 141/200, loss: 0.1394\n",
      "Epochs: 161/200, loss: 0.1372\n",
      "Epochs: 181/200, loss: 0.1351\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 200\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # forward pass \n",
    "    y_pred = model(X_train_tensors)\n",
    "    # loss\n",
    "    loss = loss_function(y_pred, y_train_tensors) ## Pass the y_pred first\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    ## update weights\n",
    "    optimizer.step()\n",
    "    ## zero grad\n",
    "    optimizer.zero_grad()\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epochs: {epoch+1}/{EPOCHS}, loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "robust-madrid",
   "metadata": {},
   "source": [
    "### Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "lyric-channels",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "total, correct = 0, 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = torch.round(model(X_test_tensors))\n",
    "    \n",
    "    for i, j in zip(y_pred, y_test_tensors):\n",
    "        if i == j:\n",
    "            correct+=1\n",
    "        total+=1\n",
    "    print(f\"Accuracy: {correct/total:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biblical-finish",
   "metadata": {},
   "source": [
    "### Making prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "sealed-mambo",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [0.]], grad_fn=<RoundBackward>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.round(model(X_test_tensors[:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-gasoline",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
